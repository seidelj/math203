\documentclass{tufte-book}

\usepackage{amsmath, amsthm}
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

\title{Empirical Analysis}
\author{Alec Brandon}
\date{\today}

\usepackage{booktabs}
\usepackage{units}
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}
\usepackage{multicol}
\usepackage{lipsum}
\usepackage{pdfpages}
\usepackage{pgfplots}
\usepackage{tikz}

\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name


\newtheoremstyle{mytheoremstyle} % name
	{\topsep}		% Space above
	{\topsep}		% Space below
	{\itshape}		% Body font
	{}			% Indent amount
	{\bfseries}	% Theorem head font
	{\textnormal{:}}	% Punctuation after theorem head
	{.5em}		% Space after theorem head
	{}			%Theorem headspec 
\theoremstyle{mytheoremstyle}
\newtheorem*{thm}{Thm.}

\newtheoremstyle{mylemstyle} % name
	{\topsep}		% Space above
	{\topsep}		% Space below
	{\itshape}		% Body font
	{}			% Indent amount
	{\bfseries}	% Theorem head font
	{\textnormal{:}}	% Punctuation after theorem head
	{.5em}		% Space after theorem head
	{}			%Theorem headspec 
\theoremstyle{mylemstyle}
\newtheorem*{lem}{Lem.}


\newtheoremstyle{mydefstyle} % name
	{\topsep}		% Space above
	{\topsep}		% Space below
	{\normalfont}	% Body font
	{}			% Indent amount
	{\bfseries}	% Theorem head font
	{\textnormal{:}}	% Punctuation after theorem head
	{.5em}		% Space after theorem head
	{}			%Theorem headspec 
\theoremstyle{mydefstyle}
\newtheorem*{mydef}{Def.}
\newtheorem*{ex}{E.g.}

\begin{document}

\maketitle
\part{Empirical Analysis 31000---Azeem Shaikh}
\chapter{Probability preliminaries}
\section{Set theory}
\newthought{Sets,} they're just collections of things. Some sets have special features that'll be useful to us, but before we define those, it's probably useful to point out the following definitions/features/notation used for sets:
	\begin{enumerate}
		\item The set \(A\) can be defined as: \(A = \{x \in \mathbb{R} : P(x)\ \text{ is true}\ \}\).\footnote{Where \(P(x)\) is whatever you want it to be. E.g., \(x \in [0, 1]\).}
		\item If \(A \subset B\) then if \(x \in A\) then \(x \in B\).
		\item If \(A \subset B\) and \(B \subset A\) then \(A = B\).
		\item \(A \cup B = \{x: x \in A\ \text{ or } x \in B\} \).
		\item \(A \cap B = \{x: x \in A\ \text{ and } x \in B\} \).
		\item \(A^c = \{x: x \not \in A\} \).
		\item \(A \cap (B \cup C) = (A \cap B) \cup (A \cap C)\). And \(A \cup (B \cap C) = (A \cup B) \cap (A \cup C)\). 
		\item \emph{DeMorgan's Law}: \((A \cup C)^c = A^c \cap C^c\) and \((A \cap C)^c = A^c \cup C^c\).
	\end{enumerate}
OK. Now time for a few definitions that'll be useful for the material in this class:
	\begin{mydef}[Disjoint]
		\(A, B\) are disjoint iff \(A \cap B = \emptyset\). E.g., \(P\{A \cup B) = P\{A) + P\{B)\) if \(A, B\) are disjoint.
	\end{mydef}
	
	\begin{mydef}[Pairwise Disjoint]
		\(A_1, A_2, \dots\) are pairwise disjoint iff \(A_i \cap A_j = \emptyset\ \forall\ i \ne j\).
	\end{mydef}
	
	\begin{mydef}[Partition]
		If \(A_1, A_2, \dots\) are pairwise disjoint and \(\cup_{i = 1}^\infty A_i = \mathbf{S}\) then they form a partition of \(\mathbf{S}\).
	\end{mydef}

\section{Probability theory}
\newthought{The} basic idea of probability theory is that there is some Sample Space, \(\mathbf{S}\), that's the set of all possible outcomes of an experiment. Then there are Events, \(A\), which are just any collection of possible outcomes of an experiment. Consider an example to see what these things might mean. Say that we're interested in coin flips. Then \(\mathbf{S} = \{H, T\}\) and an event would be any subset of possible outcomes of flipping a coin, like, \(A = \{H\}\). 

More mathematically, an Event is any subset of \(\mathbf{S}\), including \(\mathbf{S}\). The third ingredient for probability theory is the probability function, \(P\), which is defined by the following axioms:
		\begin{marginfigure}
			\includegraphics{kolmogorov.jpg}			\caption{Andrey Nikolaevich Kolmogorov. Respect.}		\end{marginfigure}
\begin{mydef}[Kolmogorov's Axioms]
	A probability function, \(P\), is a function \(P\) that maps from the sample-space, \(\mathbf{S}\), to the real numbers and satisfies:
		\begin{enumerate}
			\item \(P\{A\} \ge 0\) for all events, \(A\).
			\item \(P\{S\} = 1\).
			\item If events \(A_1, A_2, \dots\) are pairwise disjoint, then \(P\{\cup^{\infty}_{i = 1} A_i\} = \sum_{i = 1}^\infty P\{A_i\}\).
		\end{enumerate}
\end{mydef}
With those established the following theorems follow:
\begin{thm}[Handful of Probability Theorems]
	Presented below without proof. See Chapter 1 of Casella and Berger.
	\begin{description}
		\item \(P\{\emptyset\} = 0\)
		\item \(P\{A\} \le 1\)
		\item \(P\{A^c\} = 1 - P\{A\}\)
		\item \(P\{A \cap B^c\} = P\{B\} - P\{A \cap B\}\)
		\item \(P\{A \cup B\} = P\{A\} + P\{B\} - P\{A \cap B\} \)
		\item If \(A \subset B \implies P\{A\} \le P\{B\}\)
		\item \(P\{A\} = \sum_{i = 1}^\infty P\{A \cup C_i\}\) for any partition \(C_1, C_2, \dots\)
		\item \emph{Boole's Inequality} \(P\{\cup_{i = 1}^\infty A_i\} \le \sum_{i = 1}^\infty P\{A_i\}\) for any sets \(A_1, A_2, \dots\)
		\item \emph{Bonferonni's Inequality'} \(P\{A \cap B\} \ge P\{A\} + P\{B\} - 1\).\footnote{Note: This follows directly from Boole's Inequality. All you need is Boole's Inequality, the fact that \(P\{A^c\} = 1 - P\{A\}\), and Captain DeMorgan's Law.}
	\end{description}
\end{thm}

\section{Inequalities}
\newthought{In} the spirit of the inequalities above, another useful inequality is the Cauchy-Schwarz inequality. It doesn't really have anything to do with probability theory, per se, so we'll do a quick section on inequalities here. In that spirit:
\begin{thm}[Cauchy-Schwarz]
	\[|a + b|^2 \le |a|^2 + |b|^2\]
\end{thm}
And a closely related inequality is the triangle inequality.
\begin{thm}[Triangle Inequality]\marginnote{\(\bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup\)}
	\[|x+y| \le |x| + |y|\ \text{ and }\ |x - y| \ge \left| |x| - |y| \right|\]
\end{thm}
As we will come to see later, inequalities are pretty handy when we're interested in proving something converges to zero. 

\section{Conditional probability and independence} 
\newthought{Given} the name of the subsection, I think we can dispense with a clever intro and just get into it:
\begin{mydef}[Conditional Probability]
	\[P\{A|B\} = \frac{P\{A \cap B\}}{P\{B\}}\]
\end{mydef}

\begin{thm}[Bayes's Rule]\footnote{Of Thomas Bayes fame. We call \(P\{A_i | B\}\) the posterior, \(P\{A_i\}\) the prior, and \(P\{B|A_i\}\) is just the likelihood for the conditional probability.}
	Let \(A_i\) be a partition of the sample-space and let \(B\) be any set. Then:
		\[P\{A_i | B\} = \frac{P\{B|A_i\}P\{A_i\}}{\sum_{j = 1}^\infty P\{B|A_j\}P\{A_j\}} \text{.}\]
\end{thm}

\begin{mydef}[Independence]
	Two events, \(A, B\), are statistically independent i/f/f \(P\{A \cap B\} = P\{A\}P\{B\}\).
\end{mydef}

\begin{thm}
	If \(A, B\) are independent events then:
		\begin{description}
			\item \(A, B^c\) are independent
			\item \(A^c, B\) are independent
			\item \(A^c, B^c\) are independent
		\end{description}
\end{thm}

\begin{mydef}[Mutual Independence]
	A collection of events, \(A_1, \dots, A_n\) are called mutually independent i/f/f for any subcollection \(A_{i_1}, \dots, A_{i_k}\) we have:
		\[P\left\{\cap_{j = 1}^k A_{i_j}\right\} = \prod_{j=1}^k P\{A_{i_j}\}\]
\end{mydef}

\section{Random variables and distribution functions}
\newthought{Random} variables are crazy useful because we can think about combinations of events as opposed to just events.\footnote{I.e., you don't have to have a seperate piece of notation for every event, but can think about just summing up outcomes of events into one number, \(X\).} Anyways, here's the key details:
\begin{mydef}[Cumulative Distribution Function] The CDF of a random variable, \(X\), \(F_X(x)\) is:\footnote{Often we'll suppress the \(X\) in \(F_X(x)\) and just write \(F(x)\) when it's clear from the context.}
		\[F_X(x) = P\{X \le x\}, \forall x\]
\end{mydef}

\begin{mydef}[Identically distributed] The random variables \(X, Y\) are identically distributed iff for every set \(A\), \(P\{X \in A\} = P\{Y \in A\}\). Equivalently we can say that \(F_X(x) = F_Y(x)\). 
\end{mydef}

\begin{marginfigure}
\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}

\begin{tikzpicture}
\begin{axis}[
  no markers, domain=0:6, samples=100,
  axis lines*=left, xlabel=\(x\), ylabel=\(f_X(x)\),
  every axis y label/.style={at=(current axis.above origin),anchor=south},
  every axis x label/.style={at=(current axis.right of origin),anchor=west},
  height=4cm, width=6cm,
  xtick={3,6.5}, ytick=\empty,
  enlargelimits=false, clip=false, axis on top,
  grid = major
  ]

  \addplot [very thick,cyan!50!black] {gauss(3,1)};
\end{axis}
\end{tikzpicture}
\caption{PDF of \(X \sim P = \mathcal{N}(3, 1)\).}
\end{marginfigure}

\begin{mydef}[Probability Mass/Density Function] The PMF of a discrete random variable is:
		\[f_X(x) = P\{X = x\}\ \forall x\]
	The PDF of a continuous random variable, \(f_X(x)\) satisfies:
		\[F_X(x) = \int_{-\infty}^x f_X(\tilde{x}) d\tilde{x}\ \forall x\]
\end{mydef}

\section{Expectations and moments}
\newthought{Expectations} are crazy important for econometrics because just about everything is a conditional expectation, so it's worth going over some definitions of this stuff here:
\begin{mydef}[Expectations] The expected value of a random variable, \(g(X)\), denoted by \(Eg(X)\) or \(E[g(X)]\) is:
	\[Eg(X) = \left\{ 
			  \begin{array}{l l}
			    \int_{-\infty}^\infty g(x)f_{X}(x)dx = \int_{-\infty}^\infty g(x) dF_{X}, & \text{if \(X\) is continuous} \\
			    \sum_{x} g(x)f_{X}(x) = \sum_{x \in \mathbf{X}} g(x)P\{X = x\}, & \text{if \(X\) is discrete}\\
			  \end{array} \right.
	\]
where \(X\) is some random variable and \(g(\cdot)\) is some function of \(X\). If this seems weird then just think of the random variable, \(Y = g(X)\).\footnote{If the sum or integral doesn't exist then we say that \(Eg(X)\) doesn't exist. To denote that the expectation exists we generally just write: \(Eg(X) < \infty\).} 
\end{mydef}
These formulas might seem a little overwhelming, but it's just a bunch of notation to describe averages. To see that they're actually quite straightforward, the following theorem will highlights some nice features:
\begin{thm}[Fun with Expectations] Let \(X\) be a random variable and let \(a, b, c\) be constants. Then for any functions, \(g_1(\cdot)\) and \(g_2(\cdot)\) whose expectations exist we have:
	\begin{description}
		\item \(E[ag_1(X) + bg_2(X) + c] = aEg_1(X) + bEg_2(X) + c\)
		\item If \(g_1(x) \ge 0\) for all \(x\), then \(Eg_1(X) \ge 0\).
		\item If \(g_1(x) \ge g_2(x)\) for all \(x\), then \(Eg_1(X) \ge Eg_2(X)\). 
		\item If \(a \le g_1(x) \le b\) for all \(x\), then \(a \le Eg_1(X) \le b\).
	\end{description}
\end{thm}
\noindent The notion of an expectation might seem a bit limited---after all, it's just an average---but it can be expanded to higher orders. As in, the average is the first moment. The variance is the second moment.\footnote{The third moment? Who knows. Skew? Kurtosis?} Here's a definition:
\begin{mydef}[\(n\)th Moment] For each integer \(n\), the \(n^{th}\) raw moment of \(X\) is:
	\[EX^n\]
and the \(n^{th}\) central moment of \(X\) is:
	\[E[(X - EX)^n] = E[(X - \mu_X)^n]\]
\end{mydef}
The second moment, or variance, is so important that we'll recite a few theorems about it here:
\begin{thm}[Fun with Variance] If \(X\) is a random variable with finite variance, then for any constants \(a, b\):
	\[Var(aX + b) = a^2Var(X) \text{.}\]
Also, you can write:
	\[Var(X) = E[(X - EX)^2] = EX^2 - (EX)^2 \text{.}\]

\end{thm}
What about sample variances?
\begin{mydef}[Sample Variance] For \(X_1, \dots, X_n\) in \(\mathbb{R}\), the sample variance,\footnote{This is only for the univariate case, b/t/w.} denoted, \(S^2_n\) is:
	\[S^2_n = \frac{1}{n-1}\sum_{i = 1}^n (X_i - \bar{X}_n)^2\]
\end{mydef}
\noindent For higher dimensions, the sample variance-covariance matrix is:
\begin{mydef}[Sample Variance-Covariance Matrix] For \(X_1, \dots, X_n\) in \(\mathbb{R}^k\) the sample variance-covariance matrix, denoted, \(\hat{\Sigma}\) is:
	\[\hat{\Sigma} = \frac{1}{n-1}\sum_{i = 1}^n (X_i - \bar{X}_n)(X_i - \bar{X}_n)' \]
\end{mydef}

\section{Covariances and correlations}
\newthought{Later} we'll derive these results as a consequence of the Delta Method, but for now we'll just state them, as they're useful properties. 
\begin{mydef}[Covariance] Suppose we have draws from a joint distribution, \((X_1, Y_1), \dots, (X_n, Y_N) \overset{iid}{\sim} P\), on \(\mathbb{R}\). Also suppose that \(Var(X_i) < \infty, Var(Y_i) < \infty\). Then we can define the covariance of \((X_i, Y_i)\) as:
	\begin{align*}
		Cov[X_i, Y_i] & = E[(X_i - EX_i)(Y_i - EY_i)] \\
				& = E[X_iY_i] - E[X_i]E[Y_i] 
	\end{align*}
\end{mydef}
\noindent Of course, being a rigorous course in these matters, we have to ask our selves: Does \(E[X_i Y_i]\) exist?\footnote{Great question, but, stay tuned on that one. We'll prove it in a sidenote later.}
\begin{thm}[Cauchy-Schwarz with \(\mathbb{E}\)] Take \(U, V\) as random variables. Then:
	\[E[UV]^2 \le E[U^2]E[V^2] \text{.}\]
\end{thm}

Covariances are cool and all, but they're lacking in the units department. For an easier to interpret version, we look at correlations:
\begin{mydef}[Correlation] Borrowing the same setup we used when defining covariances, if we also have that \(Var(X_i) > 0\) and \(Var(Y_i) > 0\) then the correlation between \(X_i\) and \(Y_i\) is defined as:
	\[\rho_{X, Y}(P) = Corr(X_i, Y_i) = \frac{Cov(X_i, Y_i)}{\sqrt{Var(X_i)Var(Y_i)}} \]
\end{mydef}
One nice trick for correlations is the following theorem:
\begin{thm}[Perfectly Correlated] \(|\rho_{X, Y}| \le 1\) and with equality \(\iff\) \(X_i\) is a linear function of \(Y_i\).
\end{thm}

How to estimate covariances and correlations, though?\footnote{We haven't really gotten there yet, but it makes more sense to present here than later, so whatevs.}
\begin{mydef}[Sample Covariance]
	\[\hat{\sigma}_{X, Y, n} = \frac{1}{n-1} \sum_{i = 1}^n (X_i - \bar{X}_n)(Y_i - \bar{Y}_n) \text{.}\]
\end{mydef}
\begin{mydef}[Sample Correlations]
	\[\hat{\rho}_{X, Y, n} = \frac{\hat{\sigma}_{X, Y, n}}{S_{X, n} S_{Y, n}} \]
which is consistent if we assume that: \(E[X_i^4] < \infty\) and \(E[Y_i^4] < \infty\). 
\end{mydef}

\chapter{Large sample theory}

\newthought{Given} \(X_1, \dots, X_n \overset{iid}{\sim} P\) which is assumed to satisfy some parametric/non-parametric assumptions.\footnote{iid means independent and identically distributed. By identically distributed we mean:
	\[P\{X_i \le x\} = P\{X_j \le x\}\ \forall x, \forall i, j\]
and by independent we mean:
	\[P\{X_{i_1} \le x_1, \dots, X_{i_k} \le x_k\} = \prod_{1 \le j \le k} P\{X_{i_j} \le x_j\}\]
where \(i_1, \dots, i_k\) are distinct indices.} The goal is to learn about some parameter, \(\theta(P)\), like the mean (\(\mu(P)\)) or the variance (\(\sigma^2(P)\)). By learn, the following three prototypical problems should convey what we mean:
\begin{enumerate}
	\item Estimate \(\theta(P)\). An estimator of \(\theta(P)\) is a function, \(\hat{\theta}(P)(X_1, \dots, X_n)\) that provides a best guess for \(\theta(P)\). 
	\item Test the null hypothesis that \(\theta(P) = \theta_0\) vs. the alternative hypothesis that \(\theta(P) \ne \theta_0\). A test is a function \(\phi_n := \phi_n(X_1, \dots, X_n)\) that takes values in \([0, 1]\) (typically just \(\{0, 1\}\)) and it gives the researcher the probability with which to reject the null. 
	\item Construct a confidence region for \(\theta(P)\). A confidence region is a random set \(C_n = C_n(X_1, \dots, X_n)\) s/t \(P\{\theta(P) \in C_n\} \approx 1-\alpha\) for some pre-specified value of \(\alpha \in (0, 1)\).
\end{enumerate}
And when we work with any of these types of problems we'll be interested in the large-sample behavior because small-sample behavior is sensitive to assumptions about \(P\). 

\section{Convergence in probability} 
\newthought{Before} we get to discussing convergence in probability, it's useful to have a discussion of general forms of convergence, because they'll come up:
\begin{mydef}[Convergence] A sequence \(a_n\) converges to \(a\), sometimes written: \(a_n \rightarrow a\), if  \(\forall\ \epsilon > 0\) there exists \(N \in \mathbb{N}\) such that for all \(n > N\):
	\[|a_n - a| < \epsilon \text{.}\]
\end{mydef}

\begin{thm}[Monotone Convergence]
	If \(\{a_n\}\) is a monotone sequence\footnote{\(a_n \le a_{n+1}\) or \(b_n \ge b_{n+1}\) for all \(n\). Notation note: For the sequences in this footnote we'd write: \(a_n \uparrow\) and \(b_n \downarrow\). If \(a_n\) is a monotone increasing sequence converging to \(a\) then we write: \(a_n \uparrow a\). If \(b_n \downarrow\) is converging to \(b\) then we write: \(b_n \downarrow b\).} then \(\{a_n\}\) has a limit i/f/f \(\{a_n\}\) is bounded.
\end{thm}

\begin{mydef}[Pointwise Convergence]
	We say that \(f_n: \mathbb{R} \rightarrow \mathbb{R}\) converges pointwise\footnote{Also known at p'wise.} to \(f\) i/f/f:
		\[\lim_{n \rightarrow \infty} f_n(x) = f(x)\ \forall\ x\]
\end{mydef}
It's important to note, however, that these notions of convergence aren't super useful for probability. For example, if you're flipping a fair coin, there's a non-zero probability that the coin will always land on tails. This is all just to say that we need something better than these forms of convergence. 

\begin{mydef}[Convergence in Probability] A sequence of random vectors, \(\{X_n: n \ge 1\}\) converges in probability to another random vector, \(X\), i/f/f:\marginnote{Sometimes the \(\delta\) and \(N\) is done away with and the following is used:
	\[\forall\ \epsilon > 0, P\{|X_n - X| > \epsilon\} \rightarrow 0 \text{ as } n \rightarrow \infty \text{.}\]}
	\[\forall\ (\epsilon > 0, \delta > 0), \exists\ N \in \mathbb{N}\ s.t.\ \forall\ n > N, P\{|X_n - X| \ge \epsilon\} < \delta \text{.}\]
\end{mydef}
What do we do if we want to signify that \(X_n \overset{p}{\rightarrow} +\infty\)? Need to show that for all \(c > 0\), \(P\{X_n > c\} \rightarrow 1\). Notation is: \(X_n \overset{p}{\rightarrow} + \infty\) as \(n \rightarrow \infty\). For \(-\infty\) we say that \(X_n \overset{p}{\rightarrow} -\infty\ \) i/f/f for all \(c > 0\), \(P\{X_n < -c\} \rightarrow 1\). 

Before we jump into the most important application of convergence in probability, we'll review one important theorem: \begin{marginfigure} \includegraphics{markov.jpg} \caption{Andrey Markov pondering his inequality.} \end{marginfigure}
\begin{thm}[Markov's Inequality] For any random variable, \(X\) and any \(q > 0, \epsilon > 0\):
	\[P\{|X| > \epsilon\} \le \frac{E\left[|X|^q\right]}{\epsilon^q} \]
	\begin{proof} 
		\(P\{|X| > \epsilon \} = E[I\{|X| > \epsilon\}]\ \text{ and }\ I\{|X| > \epsilon\} \le \frac{|X|^q}{\epsilon^q}\). To see see this last property just consider the two cases where the LHS is 1 and the LHS is 0. Then taking expectations of the inequality yields the theorem. 
	\end{proof}
\end{thm}

\begin{thm}[Weak Law of Large Numbers] Let \(X_1, \dots, X_n \overset{iid}{\sim} P\) and suppose that \(\mu(P), \sigma^2(P)\) exists\footnote{That is, \(E[|X_i|] < \infty\).} then:
	\[\bar{X}_n = \frac{1}{n} \sum_{i = 1}^n X_i \overset{p}{\rightarrow} E[|X_i|] = \mu(P)\ \text{ as } n \rightarrow \infty\]
	\begin{proof}
	\begin{align*}
		P\{|\bar{X}_n - \mu(P)| > \epsilon\} & \le \frac{E[|\bar{X}_n - \mu(P)|^2]}{\epsilon^2}\ \text{by Markov's Inequality with \(q = 2\)} \\
										& = \frac{Var[|\bar{X}_n|]}{\epsilon^2} \\
										& = \frac{1}{n} \frac{\sigma^2(P)}{\epsilon^2}\ \text{because \(Var(|\bar{X}_n|) = \frac{1}{n^2}\sum_i^n Var(X_i) = \frac{\sigma^2(P)}{n}\)} \\
										& \overset{p}{\rightarrow} 0
	\end{align*}
	\end{proof}
\end{thm}
\begin{figure*} \includegraphics{wlln.pdf} \caption{The moving average of \(X_i \sim \text{Bernoulli}(0.3)\) drawn 1000 times and then replicated 25 times. Seems like the WLLN might be onto something.}\end{figure*}
\newthought{We} can write any random variable \(X\) as: \(X = X^+ - X^-\) and \(|X| =  X^+ + X^-\) where \(X^+ = \max\{X, 0\}\) and \(X^- = \max\{-X, 0\}\). Then we can call \(E[X] =: E[X^+] + E[X^-]\) when both \(E[X^+], E[X^-]\) exist.\footnote{Finite.} Some terminology now:
	\begin{description}
		\item \(E[X]\) is called the first (raw) moment of \(X\).
		\item \(E[X^r]\) is called the \(r^{th}\) (raw) moment of \(X\).
		\item \(E[(X - E[X])^r]\) is the \(r^{th}\) centered moment of \(X\) and the second centered moment of \(X\) is the variance.
	\end{description}

\begin{thm}[Jensen's Inequality] Let \(I \subseteq \mathbb{R}\) be a convex set and \(f: I \rightarrow \mathbb{R}\) be a convex function\footnote{For any \(x_1, x_2\ \in I\), \(t \in [0, 1]\), we say \(f\) is a convex function iff: \(f(tx_1 + (1-t)x_2) \le tf(x_1) + (1-t)f(x_2)\).} and let \(X\) be a random variable with \(P\{X \in I\} = 1\). Then if \(E[|X|] < \infty\) and \(E[|f(X)|] < \infty\):\footnote{If \(f(\cdot)\) is concave then the inequality is reversed.}\footnote{Some non-standard convex functions: \(f(x) = |x|\) and \(f(x_1, \dots, x_n) = \max\{x_1, \dots, x_n\}\).}\footnote{A good mneomonic device to remember the direction is that \(Var(X_i) \ge 0\) and: \[Var(X_i) = E[X_i^2] - E[X_i]^2 \ge 0 \text{.}\]}
	\[f(E[X]) \le E[f(X)] \text{.}\]
	\begin{proof}
		Let \(c = E[X]\). Then there are two possibilities for \(c\), either it's in the interior of \(I\) or it's not. 
		
		Suppose \(c \not \in Int\{I\}\), then \(P\{X = c\} = 1\). Then:
			\[f(E[X]) = f(c)\]
		and:
			\begin{align*}
				E[f(X)] & = E[f(c)]\ \text{ because }\ P\{X = c\} = 1 \\
					& = f(c)\ \text{ because } f(c)\ \text{ is just a number.}
			\end{align*}
		and then we have that \(f(E[X]) = E[f(X)]\) and then the inequality trivially holds. 
		
		Now suppose the other case: \(c \in Int\{I\}\). Then define:
			\[\Delta_{+, h}(c) = \frac{f(c + h) - f(c)}{h},\ \Delta_{-, h'}(c) = \frac{f(c) - f(c - h')}{h'} \text{.}\]
		By convexity we have that \(\Delta_{-, h}(c) \le \Delta_{+, h}(c)\).\footnote{Just pick \(x_1 = c+ h\), \(x_2 = c - h\), and \(t = 1/2\) and rearrange the definition of convexity.} Also, convexity gives us \(\Delta_{+, h}(c) \downarrow\) as \(h \downarrow 0\). And equivalently \(\Delta_{-, h}(c) \uparrow\) as \(h \downarrow 0\).\footnote{The notation here means that as \(h\) goes from a positive number to 0, it creates a sequence of \(\Delta_{+, h}(c)\) that is monotonically decreasing. A proof of this observation follows from fixing \(h > 0\) and picking \(h' = (1- \theta)h\) where \(\theta \in (0, 1)\) and then expanding and applying the definition of convexity to \(\Delta_{+, h'}(c)\).} Then we have that:
			\[-\infty < \Delta_{-, h'}(c) \le \Delta_{-, h}(c) \le \Delta_{+, h}(c) \le \Delta_{+, h'}(c) < \infty\]
		for \(h' > h\). Then if we define:
			\[D_{+}(c) = \lim_{h \downarrow 0} \Delta_{+, h}(c),\ D_{-}(c) = \lim_{h \downarrow 0} \Delta_{-, h}(c)\]
		we know that both exist and \(D_{+}(c) \ge \Delta_{-, h}(c) > - \infty\) and likewise for \(D_{-}(c)\). Why? Just see what happens to the full inequality with the \(\Delta\) terms above as \(h \downarrow 0\). 
		\begin{marginfigure}[1in]
			\includegraphics{jensen.jpg}
			\caption{Johan Jensen in his later years.} 
		\end{marginfigure}
		Now choose \(m \in [D_{-}(c), D_{+}(c)]\) and let \(L(x) = f(x) + m(x- c)\). If \(x = c+ h\) then \(L(c + h) = f(c) + m(h) \le f(c) + D_{+}(c)h \le f(c) + \Delta_{+, h}(c)h = f(c + h)\) and likewise for \(x = c - h\). So \(L(x) \le f(x)\ \forall x\ \in I\) and \(L(c) = f(c)\). This is the key step because look what happens next.
		
		Then \(E[f(X)] \ge E[L(X)] = L(E[X]) = L(c) = f(c) = f(E[X])\) and we're done! 
	\end{proof}
\end{thm}

Now for a useful fact about the existence of moments:
\begin{thm}[Existence of Moments] If \(E[|X|^k] < \infty\) then \(E[|X|^j] < \infty\) for \(j < k\). 
	\begin{proof}
		Suppose \(E[|X|^k] < \infty\). Then pick \(j < k\) and define \(f(x) = |x|^{k/j}\) which is a convex function. Then also define \(Y = |X|^j\) and \(Y_n = \min\{|X|^j, n\}\). The strategy of the proof is going to be to show that \(Y_n \uparrow Y\). Then we'll use the Monotone Convergence Theorem to finish up.
		
		Fix \(n\). Then \(E[|Y_n|] < \infty\) because if \(|X|^j \not < \infty\) then \(Y_n = n\). Also note that: \(E[|f(Y_n)|] = E[|\min\{|X|^k, n\}|] \le E[|X|^k] < \infty\), where the last inequality is the assumption of the proof. Then by Jensen's inequality we get:
			\[f(E[Y_n]) \le E[f(Y_n)] \le E[|X|^k] < \infty\]
		and if we apply \(f(\cdot)\) we get:
			\[|E[Y_n]|^{k/j} \le E[|X|^k]\ \implies E[Y_n] \le \left(E[|X|^k]\right)^{j/k} < \infty\]
		Then because \(Y_n \uparrow Y\), by the Monotone Convergence Theorem we get that \(E[Y_n] \uparrow E[Y]\) this implies that \(E[|X|^j] < \infty\). \footnote{Note that a more intuitive proof that (probably) isn't perfect would just apply Jensen's inequality to \(E[|X^j|]\):
			\[E[|X|^k] = E[f(|X|^j)] \ge f\left(E[|X|^j]\right) = \left(E[|X|^j]\right)^{k/j}\]
		which we can pick the first and last terms from and do a little bit of algebra to get:
			\[E[|X|^j] \le \left(E[|X|^k]\right)^{j/k} < \infty \text{.}\]	}
	\end{proof}
\end{thm}
It's worth noting that this existence of moments theorem is particularly useful when we're given something like: \(E[X^4] < \infty\). Why? Well it tells us that the mean and variance exist and that we can do all the fun stuff that that conclusion gives us.\footnote{Also note that if \(f: I \rightarrow \mathbb{R}\) is convex and \(I\) is open the proof shows that:
	\[f(x) = \sup_{L \in \mathbb{L}} L(x)\]
where \[\mathbb{L} = \{L \text{ linear s.t. } L(x) \le f(x) \forall x \in I\} \text{.}\]}

\begin{ex}[] \(X_1, \dots, X_n \overset{iid}{\sim} P\) on \(\mathbb{R}\) and suppose \(\mu(P)\) exists. Goal is to estimate \(\mu(P)\). A natural estimator of \(\mu(P)\) is:
	\[\bar{X}_n = \frac{1}{n} \sum_{i = 1}^n X_i\]
which is just the sample mean. By the WLLN \(\bar{X}_n \overset{p}{\rightarrow} \mu(P)\). That is, the sample average is a consistent estimator for \(\mu(P)\). 
\end{ex}
This estimator is an example of the \emph{analog principle} an estimator obtained by replacing the unknown \(P\) with an empirical estimator of it, \(\hat{P}_n\). In an iid setting, \(\hat{P}_n\) is just the empirical distribution and puts equal mass on \(X_1, \dots, X_n\). For example, we can write the empirical CDF as: 
	\[\hat{F}_n(x) = \frac{1}{n} \sum_i I\{X_i \le x\}\]
and in the homework we'll show that \(\hat{F}_n(x) \overset{p}{\rightarrow} F(x)\) for all \(x\). You can actually show further than:
	\[\sup_x |\hat{F}_n(x) - F(x)| \overset{p}{\rightarrow} 0\]
which is the Glivenko-Cantelli theorem, a big part of empirical process theory.

\newthought{Now} we've only discussed convergence in probability in terms of scalars. How does it extend to vectors? The next theorem will help us think about that.

\begin{lem}[Marginal and Joint Convergence] Take the sequence \(\{X_n: n \ge 1\}\) and \(X\), two random vectors in \(\mathbb{R}^k\). Define \(X_{n_j}, X_j\) as the \(j^{th}\) component of those vectors. That is, \(X = (X_1, \dots, X_j, \dots, X_k)\) and equivalently for \(X_n\). Then if \(\forall\ 1 \le j \le k, X_{n_j} \overset{p}{\rightarrow} X_j\) then \(X_n \overset{p}{\rightarrow} X\).\footnote{In words, this theorem just says that if each component converges, then the entire vector converges. }
	\begin{proof} w/t/s that for \(\epsilon > 0\), \(P\{|X_n - X| > \epsilon\}\) goes to 0.
	\begin{align*}
		P\{|X_n - X| > \epsilon\} & = P\left\{\sum_{1 \le j \le k} (X_{n_j} - X_j)^2 > \epsilon^2\right\} \text{ by def. of the metric space} \\
							& \le P\left\{\bigcup_{1 \le j \le k} \left\{(X_{n_j} - X_j)^2 > \frac{\epsilon^2}{k}\right\}\right\}\\
							& \le \sum_{1 \le j \le k} P\left\{|X_{n_j} - X_j| > \frac{\epsilon}{\sqrt{k}}\right\} \text{ by Boole's Inequality} \\
							& \rightarrow 0
	\end{align*}
	\marginnote[-1.5in]{Where the second step follows because if \(A = \{\sum_{1 \le j \le k} (X_{n_j} - X_j)^2 > \epsilon^2\}\) and \(B = \left\{\bigcup_{1 \le j \le k} (X_{n_j} - X_j)^2 > \frac{\epsilon^2}{k}\right\}\) then \(A \subseteq B\).}
	\end{proof}
\end{lem}

The WLLN is pretty awesome, right? Well. What if it were even better? It would be nice if it could speak to more than just sample averages. The continuous mapping theorem steps up big time here:
\begin{thm}[Continuous Mapping Theorem (CMT1)] Take the sequence \(\{X_n: n \ge 1\}\) and \(X\), two random vectors in \(\mathbb{R}^k\). Let the function, \(g(\cdot)\), where \(g: \mathbb{R}^k \rightarrow \mathbb{R}^d\) be a continuous function on a set \(C\)\footnote{\(g(\cdot)\) is continuous on a set \(C\) means that \(g(\cdot)\) is continuous at each point \(x \in C\). That is:
	\[\forall\ x \in C,\ \exists\ \delta > 0\ s/t\ |x- y| < \delta \implies |g(x) - g(y)| < \epsilon\ \text{.}\]
} such that \(P\{X \in C\} = 1\) and \(X_n \overset{p}{\rightarrow} X\). Then:
	\[g(X_n) \overset{p}{\rightarrow} g(X) \text{.}\]
	\begin{proof}For \(\epsilon > 0\), \(P\{|g(X_n) - g(X)| > \epsilon \}\), define the problematic points as:
		\[B_\delta = \{x \in \mathbb{R}^k:\ \exists y\ \text{with}\ |x-y| \le \delta\ \&\ |g(x) - g(y)| > \epsilon\}\]
	Then if \(x \not \in B_\delta\) we get:
		\[x \not \in B_\delta \implies \forall\ y, |x-y| > \delta\ \text{or}\ |g(x) - g(y)| \le \epsilon\]
	and if \(X \not \in B_\delta\) we get:\footnote{Seems like we're just moving to random variables in this step.}
		\[X \not \in B_\delta \implies \forall\ X_n, |X-X_n| > \delta\ \text{or}\ |g(X) - g(X_n)| \le \epsilon\]
	Then:
		\begin{align*}
			P\left\{|g(X_n) - g(X)| > \epsilon\right\} & = P\left\{\{|g(X_n) - g(X)| > \epsilon\}\ \cap \{X\not \in B_\delta\} \right\} + P\left\{\{|g(X_n) - g(X)| > \epsilon\}\ \cap \{X \in B_\delta\} \right\} \\
											& \le  P\left\{|g(X_n) - g(X)| > \delta\right\} + P\left\{X \in B_\delta \right\} \\
											& = \underbrace{P\left\{|g(X_n) - g(X)| > \delta\right\}}_\textrm{\(\rightarrow 0\) as \(n \rightarrow \infty\)} + \underbrace{P\left\{\{X \in B_\delta\} \cap C\right\}}_\textrm{\(\rightarrow 0\) as \(\delta \downarrow 0\)}\\
											& \rightarrow 0
		\end{align*}
	\end{proof}
\end{thm}

\begin{ex}[Applying the CMT1] \(S^2_n\) is a consistent estimator of \(\sigma^2(P)\):
	\begin{align*}
		S^2_n & = \frac{1}{n-1}\sum_{i = 1}^n (X_i - \bar{X}_n)^2 \\
			  & = \frac{n}{n-1}\left[\frac{1}{n} \sum_{i = 1}^n X_i^2 - \bar{X}_n^2\right]\ \text{ by expanding \(\bar{X}_n\) and rearranging.} \\
		   	  & = f\left(\frac{n}{n-1}, \frac{1}{n}\sum_i X_i^2, \bar{X}_n\right)\ \text{ with \(f(x,y,z) = x(y-z^2)\) which is a nice and continuous function. }\\
			& \overset{p}{\rightarrow} f(1, E[X_i^2], E[X_i]) \\
			& = E[X_i^2] - E[X_i]^2 \\
			& = \sigma^2(P)
	\end{align*}
\end{ex}

\begin{mydef}[Convergence in \(q^{th}\) moment] For \(q > 0\), we say that \(\{X_n: n \ge 1\}\) converges in the \(q^{th}\) moment to \(X\) if:
	\[E[|X_n - X|^q] \rightarrow 0\]
\end{mydef}
Convergence in \(q^{th}\) moment implies that \(X_n \overset{p}{\rightarrow} X\).\footnote{Proof? Use Markov's inequality.}\footnote{The converse is false.}

\begin{ex}[Convergence in probability doesn't imply convergence in \(q^{th}\) moment] For example, pick \(X_n = n\) with probability \(\frac{1}{n}\) and 0 otherwise. Check that \(X_n \rightarrow X\) but that with \(q = 1\):\footnote{Work this out. He told us to check this in class and we know how that sort of thing works!}
	\[E[|X_n - X|] = E[|X_n|] = E[X_n] = 1 \ne 0 = E[X]\]
\end{ex}

\section{Convergence in distribution}
\newthought{Having} a less restrictive sense of convergence than convergence in probability would be useful because at this point there's very little we can say about the limiting distribution of parameters. Sure the WLLN tells us that an analog to a parameter converges in probability to that parameter, but it would be nice if we could put some sort of weight on how certain we are that an estimator jives with a hypothesis we have. Convergence in distribution (in concert with convergence in probability) allows us to do just that.
\begin{mydef}[Convergence in Distribution] Say a sequence of random vectors \(\{X_n: n \ge 1\}\) converges in distribution to another random vector \(X\) iff \(P\{X_n \le x\} \rightarrow P\{X \le x\}\ \forall\ x\) at which \(P\{X \le x\}\) is continuous, where the \(\le\) is assessed component by component of the vectors in question. 
\end{mydef}

\begin{marginfigure}
\begin{tikzpicture}
\begin{axis}[
	no markers, domain=0:6, samples=100,
	axis lines*=left, xlabel=\(x\), ylabel=\(f_X(x)\),
	every axis y label/.style={at=(current axis.above origin),anchor=south},
	every axis x label/.style={at=(current axis.right of origin),anchor=west},
	height=4cm, width=6cm, 
	xtick=\empty, ytick=\empty,
	enlargelimits=false, clip=false, axis on top,
 	grid = major
  ]

	\addplot [very thick,cyan!50!black] {.5*exp(-.5*x)};
\end{axis}
\end{tikzpicture}
\caption{PDF of \(X \sim P = Exp\left(\frac{1}{2}\right)\).}
\end{marginfigure}


\noindent One note about ``common'' distributions like, \(\mathcal{N}(0,1)\) or Exp(\(\lambda\)) is that we'll write:
	\[X_n \overset{d}{\rightarrow} \mathcal{N}(0, 1)\text{.}\]
\begin{ex}[Importance of Continuity Assumption] The definition of convergence in distribution requires convergence only at values of \(x\) where \(P\{X \le x\}\) is continuous. For example, take \(X_n = \frac{1}{n}\) and \(X = 0\). Then the c/d/f of \(X_n\) is continuous everywhere except \(x = 0\). Then:
	\[P\{X_n \le x\} \rightarrow P\{X \le x\}\ \forall\ x \ne 0\]
but
	\[0 = P\{X_n \le 0\} \not \rightarrow P\{X \le 0\} = 1\]
where \(P\{X = 0\}\) is discontinuous because it jumps up to 1 at \(x=0\).
\end{ex}
Convergence is distribution is super important, but the definition above isn't that easy to work with. Luckily there's a lemma that establishes some easier ways to work with convergence in distribution.
\begin{lem}[Portmanteau's Lemma] The following are equivalent:
	\begin{enumerate}
		\item \(X_n \overset{d}{\rightarrow} X\)
		\item \(E[f(X_n)] \rightarrow E[f(X)]\) for all continuous, bounded, and real valued functions, \(f(\cdot)\). 
		\item \(E[f(X_n)] \rightarrow E[f(X)]\) for all bounded Lipschitz,\footnote{\(f(\cdot)\) is Lipschitz with constant \(L\) iff \(|f(x) - f(y)| \le L|x-y|\).} real valued functions, \(f(\cdot)\). 
		\item \(\liminf_{n \rightarrow \infty} E[f(X_n)] \ge E[f(x)]\) for all non-negative, continuous, real valued functions, \(f(\cdot)\). 
		\item \(\liminf_{n \rightarrow \infty} P\{X_n \in G\} \ge P\{X \in G\}\) for all open sets \(G\).
		\item \(\limsup_{n \rightarrow \infty} P\{X_n \in H\} \le P\{X \in H\}\) for all closed sets \(H\).
		\item \(P\{X_n \in B\} \rightarrow P\{X \in B\} \) for all Borel sets \(B\) with \(P\{X\in bdd(B)\} = 0\).\marginnote{Azeem comments that the first and last are the most useful, even though we'll exclusively (spoiler alert!) work with the first and second or the first and third in these notes.}
	\end{enumerate}
\end{lem}

\section{Convergence in distribution and probability}
\newthought{It} isn't immediately apparent how convergence in probability and distribution interact with each other but it's critical for understanding limiting distributions. Conceptually, probability is the stronger condition, but this isn't always true. If this intro seems muddled and confusing it's because the issue isn't that straightforward. Let's just delve into some theorems and lemmas that relate the two concepts:
\begin{lem} Let \(\{X_n: n \ge 1\}\) be a sequence of random vectors and \(X\) another random vector s/t \(X_n \overset{d}{\rightarrow} X\). Let \(Y_n\) be a sequence of random vectors s/t \(Y_n - X_n \overset{p}{\rightarrow} 0\). Then \(Y_n \overset{d}{\rightarrow} X\).
	\begin{proof}
	By Portmanteau's Lemma it's enough to show that:
		\[E[f(Y_n)] \rightarrow E[f(X)]\]
	for all bounded Lipschitz real-valued \(f(\cdot)\) bounded by \(B\). Then our desire is to show that there exists an \(N\) s/t \(\forall n > N, \epsilon > 0\):
		\[|E[f(Y_n)] - E[f(X)]| < \epsilon\]
	so starting with our desired statement and adding 0 and applying the \(\bigtriangleup\) inequality we get:
		\[|E[f(Y_n)] - E[f(X))]| \le \underbrace{|E[f(Y_n)] - E[f(X_n))]|}_\textrm{Hmm} + \underbrace{|E[f(X_n)] - E[f(X))]|}_\textrm{\(\rightarrow 0\) b/c \(X_n \overset{d}{\rightarrow} X\) and P.'s Lem.}\]
	and we can play with our Hmmm term to get:
		\begin{align*}
			|E[f(Y_n)] - E[f(X_n))]| & \le |E[f(Y_n) - f(X_n)]| \\
							& \le E[|f(Y_n) - f(X_n)|]\ \text{ by the \(\bigtriangleup\) inequality } \\
							& = E[|f(Y_n) - f(X_n)|]I\{|Y_n - X_n| \le \epsilon\} + E[|f(Y_n) - f(X_n)|]I\{|Y_n - X_n| > \epsilon\} \\
							& \le L\epsilon + 2B E[I\{|Y_n - X_n| > \epsilon\} ]\ \text{ b/c \(f(\cdot)\) is bounded by \(B\) } \\
							& \le L\epsilon + 2B\underbrace{P\{|Y_n - X_n| > \epsilon\}}_\textrm{\(\rightarrow 0\) by assumption}\ \text{ because E of I's are P's}\\
							& \rightarrow 0\ \text{ by choice of small enough \(\epsilon\) }
		\end{align*}
	and then we've taken that Hmmmm term to 0 and we've established our desired result.
	\end{proof}
\end{lem}

\begin{lem} Take \(\{X_n: n \ge 1\}\). If \(X_n \overset{p}{\rightarrow} X\) then \(X_n \overset{d}{\rightarrow} X\). 
	\begin{proof} Let \(Y_n = X_n\), \(X_n = X\), and \(X = X\) in the lemma above. Then we have that \(X \overset{d}{\rightarrow} X\) and \(X_n - X \overset{p}{\rightarrow} 0\). Then applying the lemma, we get that \(X_n \overset{d}{\rightarrow} X\), which is the conclusion of the claim. Boom.
	\end{proof}
\end{lem}

\noindent However, generally the converse is not true.
\begin{ex} Define \(X\) as follows:
	\[X = \begin{cases} 1 &, \mbox{ with } p = \frac{1}{2} \\
					-1 &, \mbox{ o/w} \end{cases}\]
and let \(X_n = -X\). Then dwelling on what the CDF for \(X\) looks like, you'll arrive at:\footnote{In words, this just says that \(X\) isn't going to less than \(-1\), it's not going to be bigger than \(1\), and if it's between \(-1\) and \(1\) then it's a 50/50 proposition.}
	\[P\{X \le x\} = \begin{cases} 1 &,\ x \ge 1 \\
						\frac{1}{2} &,\ x \in [-1, 1) \\
						0 &, x < -1\end{cases} \]
and if you do the same for \(X_n\) you'll find that:
	\[P\{X_n \le x\} = P\{X \le x\},\ \forall x\text{.}\]
Then we have that \(X_n \overset{d}{\rightarrow} X\), but:\footnote{Where we've fixed \(\epsilon = 1\) and shown that there's clearly no freedom to choose a \(\delta\) that'll satisfy the required definition of convergence in probability.}
	\[P\{|X_n - X| > 1\} = 1\] 
which tells us that \(X_n \not \overset{p}{\rightarrow} X\).
\end{ex}

But there is one important exception:
\begin{lem} Take \(\{X_n: n \ge 1\}\) and \(c\) a non-random vector (constant) with \(X_n \overset{d}{\rightarrow} c\) then \(X_n \overset{p}{\rightarrow} c\).
	\begin{proof} Starting from \(X_n \overset{d}{\rightarrow} c\) we can apply P.'s Lem. to obtain the fact that:
		\[\limsup_{n \rightarrow \infty} P\{X_n \in H\} \le P\{c \in H\}\]
	where \(H\) is any closed set. Now, recall what we n/t/s:
		\[\forall\ \epsilon > 0,\ P\{|X_n - c| > \epsilon\} \rightarrow 0\]
	which is an open set. But we can easily turn it into a closed set with the following move:
		\[P\{|X_n - c| > \epsilon\} \le P\{|X_n - c| \ge \epsilon\}\]
	and from P.'s Lem. we know that for this closed set the following holds:
		\[\limsup_{n \rightarrow \infty} P\{|X_n - c| \ge \epsilon\} \le P\{|c-c| \ge \epsilon\} = 0\]
	which establishes that we get convergence in probability because the results have held for any \(\epsilon > 0\).\footnote{And \(\limsup P\{|X_n - c| > \epsilon\} < \delta\) is equivalent to \(\lim P\{|X_n - c| > \epsilon\} < \delta\)?}
	\end{proof}
\end{lem}

Another important distinction between convergence in probability and convergence in distribution is that marginal convergence does not imply joint convergence in distribution. 
\begin{ex}[Marginal and Joint Convergence in Distribution] Take:
	\[ \left( \begin{array}{ccc}
		X_n  \\
		Y_n 	\end{array} \right)\overset{iid}{\sim}N \left(  \begin{array}{ccc} 0 \\ 0 \end{array} , \left[\begin{array}{ccc} 1 & \rho_n \\ \rho_n & 1 \end{array} \right] \right) \]
	with \(\rho_n = -1^{n}\). Then \(X_n \overset{d}{\rightarrow} \mathcal{N}(0, 1)\) and \(Y_n \overset{d}{\rightarrow} \mathcal{N}(0, 1)\) but the joint distribution's covariance terms will never settle down and there is no convergence in distribution. 
\end{ex}
\noindent And of course, there's an exception to this result, which we establish with the next Lemma. 
\begin{lem} Let \(\{(X_n, Y_n): n \ge 1\}\) be a sequence of random vectors with \(X_n \overset{d}{\rightarrow} X\) and \(Y_n \overset{d}{\rightarrow} c\), \(c\) a constant, then: 
	\[(X_n, Y_n) \overset{d}{\rightarrow} (X, c) \text{.}\]
	\begin{proof}
		From an earlier lemma we have that:
			\[Y_n \overset{p}{\rightarrow} c\]
		and we also have that:
			\[|(X_n, Y_n) - (X_n, c)| = |Y_n - c| \overset{p}{\rightarrow} 0\]
		because there's no distance between \(X_n\) and itself.\footnote{This statement is a little jenky because it doesn't note the lack of distance in the first argument. I could imagine writing:\[|(X_n, Y_n) - (X_n, c)| = |(0, Y_n - c)| \] instead, but I guess this other approach is more notationally convienent.} So it's enough to show that \((X_n, c) \overset{d}{\rightarrow} (X, c)\).
		
		Then using P.'s Lem., it's enough to show that for all bounded, continuous, and real-valued \(f(\cdot)\) that:
			\[E[f(X_n, c)] \rightarrow E[f(X, c)] \text{.}\]
		But from P.'s Lem. we already have that:
			\[E[f(X_n)] \rightarrow E[f(X)]\]
		which we can use to get the desired result by picking the functions, \(f(\cdot, c)\), from those that the condition without \(c\) that are continuous, bounded, and real-valued. Then we've satisfied P.'s Lem. for \(f(\cdot, c)\), giving us \((X_n,c) \overset{d}{\rightarrow} (X, c)\), which gives us the conclusion we wanted. \marginnote{For those scoring at home, this proof really goes the extra mile to use the Portmanbro Lemma as many times as possible.}
	\end{proof}
\end{lem}

\begin{thm}[Continuous Mapping Theorem (CMT2)] Let \(\{X_n: n \ge 1\}\) be a sequence of random vectors on \(\mathbb{R}^k\) and \(X\) another random vector also on \(\mathbb{R}^k\) such that: \(X_n \overset{d}{\rightarrow} X\). Also, suppose there is a function, \(g: \mathbb{R}^k \rightarrow \mathbb{R}^d\), which is continuous at each point \(x \in C\) and \(P\{X \in C\} = 1\). Then:
	\[g(X_n) \overset{d}{\rightarrow} g(X) \text{.} \]
	\begin{proof}
		By P.'s Lem., it's enough to show that for any closed set, \(H\):
			\[\limsup_{n \rightarrow \infty} P\{g(X_n) \in H\} \le P\{g(X) \in H\}\]
		but we don't really know anything about \(g(X)\), so it's useful to relate it to \(g(X_n)\). Note:
			\[g(X_n) \in H \iff X_n \in g^{-1}(H)\]
		where \(g^{-1}(H) = \{x \in \mathbb{R}^k:\ g(x) \in H\}\) and to make sure that we're dealing with a closed set, we note that:
			\[g^{-1}(H) \subseteq cl(g^{-1}(H)) \subseteq g^{-1}(H) \cup C^c\]
		where \(C\) is a set of continuity points. Then take \(x \in cl(g^{-1}(H))\).\footnote{\(cl(A) = \{x:\ \exists x_n \in A, x_n \rightarrow x\}\).} Then there exists an \(x_n \in g^{-1}(H),\ s/t\ x_n \rightarrow x\) by the definition of \(cl(\cdot)\). If \(x \not \in C\) then we're done because then \(x \in C^c\). If \(x \in C\), then by continuity, \(g(x_n) \rightarrow g(x) \in H\), because \(H\) is a closed set and thus contains all of its limit points, so \(x \in g^{-1}(H)\). 
		
		Then:
			\begin{align*}
				\limsup_{n\rightarrow \infty} P\{g(X_n) \in H\} & = \limsup_{n\rightarrow \infty} P\{X_n \in g^{-1}(H)\} \\
													& \le \limsup_{n\rightarrow \infty} P\{X_n \in cl(g^{-1}(H))\} \\
													& \le P\{X \in cl(g^{-1}(H))\} \\
													& \le P\{X \in g^{-1}(H) \cup C^c\} \\
													& = P\{g(X) \in H)
			\end{align*}
		and then applying P.'s Lem., we get \(g(X_n) \overset{d}{\rightarrow} g(X)\).
	\end{proof}
\end{thm}

A related Lemma to the CMT2 that's probably used way more than it should goes as follows:
\begin{lem}[Slutsky's Lemma] Let \(Y_n \overset{p}{\rightarrow} c\), where \(c\) is a constant on \(\mathbb{R}\), and \(X_n \overset{d}{\rightarrow} X\), also on \(\mathbb{R}\). Then:
	\begin{enumerate}
		\item \(X_n + Y_n \overset{d}{\rightarrow} X + c\)
		\item \(X_nY_n \overset{d}{\rightarrow} Xc\)
		\item \(X_n / Y_n \overset{d}{\rightarrow} X/c\) if \(c \ne 0\) \text{.}
	\end{enumerate}
		\begin{marginfigure}
			\includegraphics{slutsky.jpg}			\caption{Eugen Slutsky. Gotta respect that hair. Damn.}		\end{marginfigure}	
	\begin{proof}
		\(Y_n \overset{p}{\rightarrow} c \implies Y_n \overset{d}{\rightarrow} c\) so \((X_n, Y_n) \overset{d}{\rightarrow} (X, c)\) and apply CMT2 with \(g(x, y) = x+y\) or \(g(x, y) = xy\), etc.
	\end{proof}
\end{lem} Why do we use this bad boy so often? That'll become more apparent once we jump into the next theorem.
\begin{figure*} \includegraphics{clt.pdf} \caption{Central Limit Theorem in action. Each panel plots the density of \(n\) observations of \(\bar{X}_{100}\) 25 times, where \(X_i \sim \text{Bernoulli}(0.3)\). Panel A has \(n = 10\). Panel B has \(n = 50\) and Panel C has \(n = 1000\). Looks pretty normal to me.} \end{figure*}

\newthought{When} we were working with convergence in probability we had a property that allowed us to relate sample averages to their expectations. Thus far we have no tools that allow us to connect sample moments to a distribution, so you might be like, ``Brandon, I've got a question: Who cares about this CMT2 or Slutsky's Lemma because we've got no limiting distributions to mess around with?'' That's totally fair, which is why we introduce the following game-changer.
\begin{thm}[Univariate Central Limit Theorem] Let \(X_1, \dots, X_n\) be sequence of random variables that are iid according to \(P\) and suppose \(\sigma^2(P) < \infty\) then:
	\[\sqrt{n}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \mathcal{N}(0 , \sigma^2(P)) \text{.}\]
\end{thm}

\begin{thm}[Cramer-Wold Device]\marginnote{That's Wold, not Wald of Transylvanian Abraham Wald Test fame. And Cramer, not Kramer of Seinfeld fame.} Let \(\{X_n: n \ge 1\}\) be a sequence of random vectors and \(X\) a random vector, both in \(\mathbb{R}^k\). Then:
	\[X_n \overset{d}{\rightarrow} X\ \iff \forall t \in \mathbb{R}^k\ t'X_n \overset{d}{\rightarrow} t'X \text{.}\]
\end{thm}

\begin{thm}[Multivariate Central Limit Theorem] Let \(\{X_n: n \ge 1\}\) be a sequence of random vectors in \(\mathbb{R}^k\) distributed according to \(P\). Furthermore, suppose \(\Sigma(P) < \infty\) then:\footnote{Recall, \[\Sigma(P) = E[(X_i - \mu(P))(X_i - \mu(P)'] \text{.}\]}
	\[\sqrt{n}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \mathcal{N}(0, \Sigma(P)) \text{.}\]
	\begin{proof}
		By Cramer-Wold, enough to show:
		\begin{align*}
			t'[\sqrt{n}(\bar{X}_n - \mu(P))] & \overset{d}{\rightarrow} t' \mathcal{N}(0, \Sigma(P)) \\
									& = \mathcal{N}\left(0, t'\Sigma(P)t\right)\ \text{ which we should check, }
		\end{align*}
		and: 
			\[t'[\sqrt{n}(\bar{X}_n - \mu(P))] = \sqrt{n}\left(\frac{1}{n} \sum_i^n t'X_i - t' \mu(P)\right) \overset{d}{\rightarrow}  \mathcal{N}\left(0, t'\Sigma(P)t\right)\]
		because \(E[t'X_i] = t'\mu(P)\) and \(Var[t'X_i] = t'\Sigma(P)t\). 
	\end{proof}
\end{thm}

\begin{ex}[Application of CMT2] Suppose \(X_1, \dots, X_n \overset{iid}{\sim} P\) with \(\sigma^2(P) < \infty\) then the CLT tells us that:
	\[\sqrt{n}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \mathcal{N}(0, \sigma^2(P)) \text{.}\]
Recall that above we proved that \(S_n^2 \overset{p}{\rightarrow} \sigma^2(P)\). Then using the CMT1 we have that:
	\[1/S_n \overset{p}{\rightarrow} 1/\sigma(P)\]
which requires us to assume that \(\sigma^2(P) > 0\) to give us the continuity requirement. Then by Slutky's Lemma we have:
	\[\frac{\sqrt{n}(\bar{X}_n - \mu(P))}{S_n} \overset{d}{\rightarrow} \frac{1}{\sigma^2(P)} \mathcal{N}(0, \sigma^2(P)) = \mathcal{N}(0, 1)\]
where \(Z := \mathcal{N}(0, 1)\). 
\end{ex}

\section{Hypothesis testing}
\newthought{Consider} testing the null hypothesis, \(H_0: \mu(P) \le 0\), vs. an alternative hypothesis, \(H_A: \mu(P) > 0\) and trying to minimize the following types of error:
	\begin{description}
		\item Type I error: Rejecting \(H_0\) when it is actually true.
		\item Type II error: Failing to reject \(H_0\) when it is false. 
	\end{description}
More formally, a \emph{test} is a function, \(\phi_n := \phi_n(X_1, \dots, X_n)\), that takes values in \([0, 1]\) and equals the probability with which we should reject the null hypothesis. Generally we only consider tests, \(\phi_n \in \{0, 1\}\). 

\begin{marginfigure} \includegraphics{fisher.jpg} \caption{R.A. Fisher looking like a huge nerd at a eugenics conference. Creepy. Anyways, Fisher's credited with the term ``test of significance.'' He also did a bunch of other useful stuff. Real bummer about the eugenics stuff, though.}  \end{marginfigure}
We'll think about \emph{power} as: \(E[\phi_n] = E_P[\phi_n]\) when viewed as a function of the distribution, \(P\), this is called the power function of a test and it equals the probability of rejecting the null when the distribution is actually \(P\). For \(P\) satisfying the null hypothesis, \(E_P[\phi_n]\) is the probability of a Type I error. For \(P\) satisfying the alternative hypothesis, \(1-E_P[\phi_n]\) is the probability of a Type II error. 

The customary solution to picking \(\phi_n\) is to restrict attention to tests that are \emph{consistent in level}, for \(P\) satisfying the null hypothesis:
	\[\limsup_{n \rightarrow \infty} E_P[\phi_n] \le \alpha\]
for \(\alpha \in (0, 1)\) where \(\alpha\) is the significance level. Subject to this constraint, the goal of most test statistics is to make the probability of a Type II error ``small.'' In this class, we'll further restrict our attention to tests of the form:\footnote{This formulation is convienent because taking expectations of indicator functions yields probabilities. In that vein: 
\begin{mydef}[\(\phi_n\) is Consistent in Level] The hypothesis test \(\phi_n =  I\{T_n > c_n\}\) is consistent in level i/f/f:
	\[\limsup_{n \rightarrow \infty} P\{\phi_n = 1\} \le \alpha\]
under the assumption of \(H_0\). 
\end{mydef} }
	\[\phi_n = I\{T_n > c_n\}\]
where \(T_n\) is our test statistic, which will be a function of the data (\(X_1, \dots, X_n\)), s/t large values provide evidence against \(H_0\) and \(c_n\) is our critical value, which provides a definition of too large. \(T_n\) is assessed under the assumptions of the null hypothesis (e.g., \(\mu(P) \le 0\)).

\begin{ex}[Z-Test] Picking up where we left off with our last example (Application of CMT2), a reasonable test statistic would be:
	\[T_n = \frac{\sqrt{n}\bar{X}_n}{S_n}\]
and a reasonable critical value would be:
	\[c_n = z_{1-\alpha} = \Phi^{-1}(1-\alpha) \text{.}\]
Are these consistent in level? Pick any \(P\) satisfying the null. Then we w/t/s:
	\[\limsup_{n \rightarrow \infty} E_P[\phi_n] = \limsup_{n \rightarrow \infty} P\{T_n > c_n\} \le \alpha \text{.}\]
Plugging in our choices of test statistic and critical value, we get:
	\begin{align*}
		P\{T_n > c_n\} & = P\left\{\frac{\sqrt{n}\bar{X}_n}{S_n} > z_{1-\alpha}\right\} \\
					& = P\left\{\frac{\sqrt{n}(\bar{X}_n - \mu(P))}{S_n} + \underbrace{\frac{\sqrt{n}\mu(P)}{S_n}}_\textrm{\(\le 0\) for \(P\) under \(H_0\)} > z_{1-\alpha}\right\} \\
					& \le P\left\{\frac{\sqrt{n}(\bar{X}_n - \mu(P))}{S_n} > z_{1-\alpha}\right\}
	\end{align*}
Then taking \(\limsup\) on both sides we get:
	\begin{align*}
		\limsup_{n\rightarrow \infty} P\{T_n > c_n\} & \le \limsup_{n\rightarrow \infty} P\left\{\frac{\sqrt{n}(\bar{X}_n - \mu(P))}{S_n} > z_{1-\alpha}\right\} \\
										& = 1- \liminf_{n \rightarrow \infty} P\left\{\frac{\sqrt{n}(\bar{X}_n - \mu(P))}{S_n} \le z_{1-\alpha}\right\} \\
										& = 1 - \Phi(z_{1-\alpha})\ \text{ b/c \(T_n \overset{d}{\rightarrow} Z\ \&\ P\{Z \le z_{1-\alpha}\} = \Phi(z_{1-\alpha})\)} \\
										& = \alpha
	\end{align*}
which establishes that the Z-Test is consistent in level.\begin{marginfigure}[-2in] \includegraphics{gosset.jpg} \caption{William Sealy Gosset, AKA Student, looking a little schlitzed. E.L. Lehmann notes: The term ``studentization'' is a misnomer. The idea of replacing \(z_{1-\alpha}\) with \(t_{n-1, 1-\alpha}\) was already used by Laplace. Student's contribution was to work out the \emph{exact} distribution in the one-sample situation.} \end{marginfigure}
\end{ex}

\begin{ex}[T-Test is Consistent in Level] The T-test is:
	\begin{align*}
		\phi_n & = I\{T_n > c_n\} \\
			& = I\left\{\frac{\sqrt{n}\bar{X}_n}{S_n} > t_{n-1, 1-\alpha}\right\}
	\end{align*}
Recall from earlier that our general strategy with this sort of example is to establish the convergence properties of \(T_n\) and \(c_n\) so that we can use:\footnote{Proof? Suppose \(T_n \overset{d}{\rightarrow} T\) and \(c_n \overset{d}{\rightarrow} c\). Then how can we show that:
	\[P\{T_n \le c_n\} \rightarrow P\{T \le c\} \text{?}\]
First note that \(T_n - c_n \overset{d}{\rightarrow} T - c\) by CMT2 because both pieces converge in distribution and the function is continuous. Then by the definition of convergence in distribution we have:
	\[P\{T_n - c_n \le x\} \rightarrow P\{T - c \le x\}\]
which if we pick \(x = 0\) implies:
	\[P\{T_n - c_n \le 0\} \rightarrow P\{T - c \le 0\} \]
which implies:
	\[P\{T_n \le c_n\} \rightarrow P\{T \le c\} \text{.}\] }

	\[T_n \overset{d}{\rightarrow} T\ \text{ and }\ c_n \overset{d}{\rightarrow} c\ \implies P\{T_n \le c_n\} \rightarrow P\{T \le c\} \text{.}\]
We already know that we can mess around with \(T_n\) to get an estimator that converges in distribution to \(Z\), so what can we do about \(c_n\)? 
	\[t_n \overset{d}{=} \frac{Z}{\sqrt{\chi^2_n/n}} \]
and:
	\[\frac{1}{n}\chi_n^2 = \frac{1}{n}\sum_{i = 1}^n z_i^2 \overset{p}{\rightarrow} \mathbf{E}Z_i = 1\]
so the numerator of \(t_n\) converges in distribution to \(Z\) and the denominator converges in probability to \(1\). Then by Slutsky's Lemma we have: 
	\[t_n \overset{d}{\rightarrow} Z\]
and the rest of showing that the T-Test is consistent in level is pretty obvious:
	\begin{align*}
		\limsup_{n \rightarrow \infty}\ P\{T_n > c_n\} & = \limsup_{n \rightarrow \infty}\ P\left\{\frac{\sqrt{n}\bar{X}_n}{S_n} > t_{n - 1, 1-\alpha}\right\} \\
							& = \limsup_{n \rightarrow \infty}\ P\left\{\frac{\sqrt{n}(\bar{X}_n - \mu(P))}{S_n} + \frac{\sqrt{n}\mu(P)}{S_n} > t_{n-1, 1-\alpha}\right\} \\
							& \le \limsup_{n \rightarrow \infty}\ P\left\{\frac{\sqrt{n}(\bar{X}_n - \mu(P))}{S_n} > t_{n-1, 1-\alpha}\right\} \text{ because under \(H_0\) we have: \(\mu(P) \le 0\)} \\
							& = 1 - \liminf_{n \rightarrow \infty}\ P\left\{\frac{\sqrt{n}(\bar{X}_n - \mu(P))}{S_n} \le z_{1-\alpha}\right\} \\
							& = 1 - \Phi(z_{1-\alpha}) \text{ because \(T_n \overset{d}{\rightarrow} Z, t_{n-1, 1-\alpha} \overset{d}{\rightarrow} z_{1-\alpha}\), and \(P\{Z \le z_{1-\alpha}\} = \Phi(z_{1-\alpha})\)} \\
							& = \alpha \text{.}
	\end{align*}
So we're good. 		
\end{ex}

\begin{ex}[Chi-Square Test] Suppose we have \(X_1, \dots, X_n\) an iid sequence of random vectors on \(\mathbb{R}^k\) distributed according to \(P\). Also, suppose that the variance-covariance matrix, \(\Sigma(P) < \infty\). How should we go about constructing \(T_n\) and \(c_n\)? 

Well, from the Multivariate Central Limit Theorem we have:
	\[\sqrt{n}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \mathcal{N}(0, \Sigma(P)) \text{.}\]
If \(\Sigma(P)\) is non-singular then:
	\[n(\bar{X}_n - \mu(P))'\Sigma(P)^{-1}(\bar{X}_n - \mu(P)) \sim \chi^2_k \]
where \(Z\) is a multivariate normal with mean zero and variance-covariance matrix \(\Sigma(P)\).\footnote{This result can be derived using a bit of linear algebra. First take the Cholesky decomposition of \(\Sigma(P) = \Lambda \Lambda'\). Then from a property of triangular inversion: \(\Sigma^{-1}(P) = \Lambda^{-1'} \Lambda^{-1}\). Then premultiplying our application of the multivariate CLT we get:
	\begin{align*}
		& \sqrt{n}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \mathcal{N}(0, \Sigma(P)) \\
		& \implies \Lambda^{-1} \sqrt{n}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \mathcal{N}(0, \Lambda^{-1}\Sigma(P)\Lambda^{-1'}) \\
		& \implies \Lambda^{-1} \sqrt{n}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \mathcal{N}(0, \Lambda^{-1}\Lambda \Lambda'\Lambda^{-1'}) \\
		& \implies \Lambda^{-1} \sqrt{n}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \mathcal{N}(0, I) \\
		& \implies \left(\Lambda^{-1} \sqrt{n}(\bar{X}_n - \mu(P))\right)' \Lambda^{-1} \sqrt{n}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \chi^2_{k} \\
		& \implies n (\bar{X}_n - \mu(P))' \Lambda^{-1'} \Lambda^{-1} (\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \chi^2_k \\
		& \implies n (\bar{X}_n - \mu(P))' \Sigma^{-1}(P) (\bar{X}_n - \mu(P))  \overset{d}{\rightarrow} \chi^2_k \text{.}
	\end{align*}} 
Then the Continuous Mapping Theorem tells us that:
	\[n(\bar{X}_n - \mu(P))'\Sigma(P)^{-1}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \chi^2_k \]
but we can actually do one better than just pretend that we have \(\Sigma(P)\). How? Well recall our definition of the sample variance-covariance:
	\[\hat{\Sigma}(P) = \frac{1}{n-1}\sum_{i = 1}^n (X_i - \bar{X}_n)(X_i - \bar{X}_n)' \]
Then we could show (but we won't) that:
	\[\hat{\Sigma}(P) \overset{p}{\rightarrow} \Sigma(P)\]
and as long as our estimator for the variance-covariance matrix is invertible the CMT2 gives us:
	\[n(\bar{X}_n - \mu(P))'\hat{\Sigma}(P)^{-1}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \chi^2_k \text{.}\]

So now that we've pinned down the distribution of our data, how should we go about testing \(H_0: \mu(P) = 0\) against \(H_A: \mu(P) \ne 0\) while satisfying:
	\[P\{T_n > c_n) \rightarrow \alpha\ \text{ if \(H_0\) is true?}\]
Just choose:
	\[T_n = n\bar{X}_n'\hat{\Sigma}(P)^{-1}\bar{X}_n\ \text{ because under \(H_0\) we have: \(\mu(P) = 0\)}\]
and:
	\[c_n = c_{k, 1-\alpha}\]
which is the \((1-\alpha)\)th quantile of the \(\chi^2_k\) distribution. 
\end{ex}


\section{P-values}
\newthought{Sometimes} we'll want to say more than just whether our function \(\phi_n\) has returned a 0 or 1 for our choice of \(\alpha\). In particular, we might want to know the \(\alpha\) at which our test switches from returning a 0 to returning a 1. This \(\alpha\) is called the p-value.\footnote{E.g., \(\alpha\) is less than or equal to 0.05.}
\begin{mydef}[P-value] The smallest value of \(\alpha\) for which we can reject \(H_0\), or the p-value, is:
	\[p_n := \text{inf}\{\alpha \in (0, 1)\ : \phi_n = 1\} \]
\end{mydef}

\begin{ex}[p-value for Z-test] 
	\begin{align*}
		\hat{p}_n & :=  \text{inf}\left\{\alpha \in (0, 1)\ :\ \frac{\sqrt{n} \bar{X}_n }{S_n} > \Phi^{-1} (1-\alpha) \right\} \\
				& = \text{inf}\left\{\alpha \in (0, 1)\ :\ \Phi\left(\frac{\sqrt{n} \bar{X}_n}{S_n}\right) > (1-\alpha) \right\} \\
				& = \text{inf}\left\{\alpha \in (0, 1)\ :\ \alpha > 1 - \Phi\left(\frac{\sqrt{n} \bar{X}_n}{S_n}\right) \right\} \\
				& = 1 - \Phi(T_n)
	\end{align*}
\end{ex}

\section{Delta method}
\newthought{Thus} far we have a limited number of tools to deal with changing the dimensionality of our data. The CMT2 helps, but in conjunction with the CLT, it's not always straightforward what the variance-covariance structure looks like for the limiting distribution. This is where the Delta Method comes in handy.
\begin{marginfigure}
	\includegraphics{delta.jpg}
	\caption{Hold on! I have something for this.}
\end{marginfigure}
\begin{thm}[Delta Method] Let \(\{X_n: n \ge 1\}\) be a sequence of random vectors on \(\mathbb{R}^k\). Let \(\{\tau_n\}\) be a sequence of real numbers converging to \(\infty\) and define a vector of constants, \(c\) such that:
	\[\tau_n(X_n - c) \overset{d}{\rightarrow} X\text{.}\]
Define the function, \(g: \mathbb{R}^k \rightarrow \mathbb{R}^d\) as differentiable at \(c\) and denote the \((d, k)\) matrix of partial derivatives of \(g\) evaluated at \(c\) as: \(Dg(c)\). Then:
	\[\tau_n(g(X_n) - g(c)) \overset{d}{\rightarrow} Dg(c)X \text{.}\]
	
	\begin{proof} The place to start is the assumption that \(g(\cdot)\) is differentiable at \(c\). Then by Taylor's Theorem we get:
			\[g(x) = g(c) + Dg(c)(x-c) + R(x-c)\]
		where \(R(\cdot)\) is the remainder term and here \(R(h) = o(|h|)\).\footnote{\(R(h) = o(|h|)\) means that \(\frac{R(h)}{|h|} \rightarrow 0\) as \(|h| \rightarrow 0\), with \(\frac{R(h)}{|h|} = 0\) if \(h = 0\).} Then:
			\[\tau_n(g(X_n) - g(c)) = \underbrace{Dg(c)\underbrace{\tau_n(X_n - c)}_\textrm{\(\overset{d}{\rightarrow} X\)}}_\textrm{\(\overset{d}{\rightarrow} Dg(c)X\) by CMT} + \tau_n R(X_n - c)\]
		and now we n/t/s:
			\[\tau_n R(X_n - c) \overset{d}{\rightarrow} 0\]
		\marginnote{Aside: \[X_n - c = \underbrace{\tau_n(X_n - c)}_\textrm{\(\overset{d}{\rightarrow} X\)} \underbrace{\frac{1}{\tau_n}}_\textrm{\(\rightarrow 0\)} \overset{d}{\rightarrow} 0 \text{.}\]}
		Then:
			\[\tau_n R(X_n - c) = \underbrace{\tau_n |X_n - c|}_\textrm{\(\overset{d}{\rightarrow} X\) by CMT} \underbrace{\frac{ R(X_n - c)}{|X_n - c|}}_\textrm{\(\overset{d}{\rightarrow}0\) by CMT} \overset{d}{\rightarrow} 0\]
		by CMT.
	\end{proof}
\end{thm}

The example below gives a sense of how the notation works for applying the Delta Method.
\begin{ex}[Multivariate Normal] If \(X \overset{d}{\rightarrow} \mathcal{N}(0, \Sigma)\) then the Delta Method tells us that:
	\[\tau_n(g(X_n) - g(c)) \overset{d}{\rightarrow} \mathcal{N}(0, Dg(c)\Sigma Dg(c)') \text{.}\]
Using this we can test \(H_0: \mu(P) = 0\) vs. \(H_A: \mu(P) \ne 0\) at level \(\alpha\) and construct \(C_n\) s/t:
	\[P\{\mu(P) \in C_n\} \rightarrow 1-\alpha\]
which highlights the duality b/t hypothesis testing and confidence regions.
\end{ex}
One interesting observation about the Delta Method is that the theorem is even valid if \(Dg(c) = 0\). In that case \(\tau_n (g(X_n) - g(c)) \overset{d}{\rightarrow} 0\). That said, it can suck if \(Dg(c) = 0\) because then our estimators are converging to a degenerate distribution. Consider the example below and the proposed solution when this crops up:
\begin{ex}[Bernoulli Distribution] Take \(X_1, \dots, X_n \overset{iid}{\sim} \text{Bernoulli}(q)\) where \(q \in (0, 1)\). Then the Central Limit Theorem tells us:
	\[\sqrt{n}(\bar{X}_n - q) \overset{d}{\rightarrow} \mathcal{N}(0, q(1-q)) \]
but suppose we're interested in the distribution of the sample variance: \(g(\bar{X}_n) - g(q) = \bar{X}_n(1-\bar{X}_n) - q(1-q)\). Well, we can use the Delta Method:
	\begin{align*}
		\sqrt{n}(g(\bar{X}_n) - g(q)) \overset{d}{\rightarrow} & \mathcal{N}(0, Dg(q)g(q)Dg(q)') \\
													& \mathcal{N}(0, (1-2q)^2 (1-q)q)\ \text{ because \(X_i\) is a scalar and \(g'(q) = 1 - 2q\).}
	\end{align*}
But what if we have that \(q = \frac{1}{2}\)? Then we'd have: 
	\[\sqrt{n}(g(\bar{X}_n) - g(q)) \overset{d}{\rightarrow} \mathcal{N}(0, 0)\] 
which pretty much sucks.

What to do? Take a second-order Taylor Series Expansion of \(g(x) - g(q)\):
	\[g(x) - g(q) = Dg(q)(x-q) + \frac{D^2g(q)}{2}(x-q)^2 + R(x-q)\]
where \(R(\cdot)\) is the remainder term. We already know that \(Dg(\frac{1}{2}) = 0\) and as usual we'll just neglect the remainder term.\footnote{Recall that \(R(h) = o(|h|^2)\), i.e., \(\frac{R(h)}{|h|^2} \rightarrow 0\) as \(h \downarrow 0\) with the fraction equal to 0 when \(h = 0\). Just as we showed in our proof of the Delta Method, we get:
	\[nR(\bar{X}_n - q) \rightarrow 0 \text{.}\]	} 
Setting \(x = \bar{X}_n\), \(q = \frac{1}{2}\), and pre-multiplying by \(n\) for convenience we get:\footnote{So we can apply the CLT.}
	\begin{align*}
		n(g(\bar{X}_n) - g(q)) & = -n \left(\bar{X}_n - q\right)^2 \\
							& = - \left(\sqrt{n}(\bar{X}_n - q)\right)^2 \\
							& \overset{d}{\rightarrow} -\left[\mathcal{N}(0, \frac{1}{4})\right]^2\ \text{ because \(q = \frac{1}{2}\)} \\
							& = -\left[\frac{1}{2} \mathcal{N}(0, 1)\right]^2 \\
							& = -\frac{1}{4} \chi_1^2 
	\end{align*}
which tells us that if we pre-multiply by \(n\), we get a full distribution of the estimator for the sample variance of a Bernoulli random variable. Neat and relatively easy trick. 	
\end{ex}

\begin{ex}[Correlations] Let \((X_1, Y_1), \dots, (X_n, Y_n) \overset{iid}{\sim} P\) on \(\mathbb{R}^2\) with \(E[X_i^2] < \infty\) and \(E[Y_i^2] < \infty\). Then the covariance of these two terms is just:
	\[Cov(X_i, Y_i) = E[X_iY_i] - E[X_i]E[Y_i]\]
and we know that \(E[X_iY_i]\) exists.\footnote{Why? Well first note that:
	\[(|u| - |v|)^2 \ge 0\]
which we can expand to get:
	\[|uv| \le \frac{1}{2}|u|^2 + \frac{1}{2}|v|^2\]
and we can take expectations and pick a convenient choice of \(u\) and \(v\) to get:
	\[E[|X_i Y_i|] \le \frac{1}{2} E[X_i^2] + \frac{1}{2} E[Y_i^2] < \infty \text{.}\]} And if in addition, \(Var(X_i) > 0\) and \(Var(Y_i) > 0\) then:
	\[Corr(X_i, Y_i) = \rho_{X, Y}(P) = \frac{Cov(X_i, Y_i)}{\sqrt{Var(X_i)Var(Y_i)}}\text{.}\]
\end{ex}

\begin{thm}[Cauchy-Schwarz Inequality] For any random variables \(U\) and \(V\) s/t \(E[U^2] < \infty\) and \(E[V^2] < \infty\), then:
	\[E[UV]^2 \le E[U^2]E[V^2] \text{.}\]
	\begin{proof} By the same argument we used in the Correlations example, we have that \(E[UV]\) exists. Either we're in world where \(E[U^2] = 0\) or \(E[V^2] = 0\) or we're in a world where both variances are non-zero. If either of the variances are zero then we're done because \(0 = 0\).\footnote{Does \(E[U^2] = 0 \implies E[UV]^2 = 0\)?}
	
	Now the hairier case assumes non-zero variances for both. Then consider \(E[(U - \alpha V)^2] \ge 0\). We can expand and separate to get:
		\[E[U^2] - 2\alpha E[UV] + \alpha^2 E[V^2] \ge 0\]
	and also note that:
		\[E[U^2] - 2\alpha E[UV] + \alpha^2 E[V^2] \ge \arg \min_{\alpha} E[U^2] - 2\alpha E[UV] + \alpha^2 E[V^2] \ge 0\]
	so taking the FOC, setting equal to 0, and solving for \(\alpha\) gives us:
		\[\alpha = \frac{E[UV]}{E[V^2]}\]
	which we can plug into the original expression to get:
		\[E[U^2] E[V^2] \ge E[UV]^2\]
	which was our goal!.
	\end{proof}
\end{thm}

\begin{lem} Let \((X_1, Y_1), \dots, (X_n, Y_n) \overset{iid}{\sim} P\) on \(\mathbb{R}^2\) with \(E[X_i^2] < \infty\) and \(E[Y_i^2] < \infty\). Then:
	\[|\rho_{X, Y}(P)| \le 1\]
and with equality i/f/f there exists \(a, b\) s/t \(P\{a + bX_i = Y_i\} = 1\).\footnote{That is, there is a perfect linear relationship between \(X_i\) and \(Y_i\).}

	\begin{proof}
	First we'll look at the claim that:
		\[|\rho_{X, Y}(P)| \le 1\]
	This follows from expanding \(\rho_{X, Y}(P)\) and applying Cauchy-Schwarz by choice of \(U = X_i - E[X_i]\) and \(V = Y_i - E[Y_i]\). 
	
	What about the claim w/r/t equality? Starting from:
		\[E[UV]^2 = E[U^2]E[V^2]\ \iff\ \exists\ \alpha\ s/t\ P\{U = \alpha V\} = 1\]
	we then just have to apply Cauchy-Schwarz with choices of:
		\[U = X_i - E[X_i]\ \text{ and } V = Y_i - E[Y_i] \text{.}\]
	\end{proof}

\end{lem}
Suppose we wanted to estimate correlations or covariances? Well, the natural choice of an estimator for \(Cov(X_i, Y_i)\) is:
	\[\hat{\sigma}_{X, Y, n} = \frac{1}{n} \sum_{i = 1}^n (X_i - \bar{X}_n)(Y_i - \bar{Y}_n)\]
and a natural estimator of \(Corr(X_i, Y_i)\) is:
	\[\hat{\rho}_{X, Y, n} = \frac{\hat{\sigma}_{X, Y, n}}{S_{Y, n}S_{X, n}}\]
so what can we say about:
	\[\sqrt{n}(\hat{\rho}_{X, Y, n} - \rho_{X, Y, n }) \overset{d}{\rightarrow} ??\]
Well, we know that these are all smooth functions of sample averages so it'll converge to a normal with mean 0 and a god-awful variance because of the CLT and Delta Method. But the point is that without the Delta Method we'd have a hard time saying much of anything. 

\newthought{Thus} far we've spent just about all of our time on sample averages.\footnote{With the exception of that stuff about medians on the first problem set.} This example will help us move beyond just that:
\begin{ex}[Beyond Sample Averages] Suppose \(X_1, \dots, X_n \overset{iid}{\sim} P\) on \(\mathbb{R}\) and let \(F\) denote the CDF. Define:
	\[\theta = \inf \{x \in \mathbb{R}:\ F(x) \ge 0.5\}\]
and further suppose that \(F(\cdot)\) is differentiable at \(\theta\) with \(F'(\theta) = f(\theta) > 0\). Then the sample median is just:
	\[\hat{\theta}_n = \inf \{x \in \mathbb{R}:\ \hat{F}_n(x) \ge 0.5\}\]
where \(\hat{F}_n(x)\) is the empirical c/d/f and we want to show:
	\[\sqrt{n}(\hat{\theta}_n - \theta) \overset{d}{\rightarrow} \mathcal{N}\left(0, \frac{1}{4f^2(\theta)}\right) \text{.}\]
	
\begin{proof}
To show this we'll use a slightly stronger CLT than we're used to.\footnote[][-2in]{Moving beyond the bland vanilla CLT we all use, we can use:\begin{thm}[Berry-Esseen CLT] If \(X_1, \dots, X_n \overset{iid}{\sim} P\) on \(\mathbb{R}\) with \(0 < \sigma^2(P) < \infty\). Then:
	\begin{align*}
		\sup_{x \in \mathbb{R}} \Bigg| P\left\{ \frac{\sqrt{n}(\bar{X}_n - \mu(P))}{\sigma(P)} \le x \right\} - \Phi(x) \Bigg| \le \\
		\frac{C}{\sqrt{n}}\frac{E[(X_i - \mu(P))^2]}{\sigma^3(P)}
	\end{align*}
where \(C\) is given to you and doesn't depend on \(P\) or \(n\).\end{thm} } Then suppose \(n\) is even. Then we know that \(\hat{\theta}_n = \frac{n}{2}^{th}\) highest observation of \(X_i\). Then:
	\begin{align*}
		P\{\sqrt{n} (\hat{\theta}_n - \theta) \le x\} & = P\left\{\hat{\theta}_n \le \theta + \frac{x}{\sqrt{n}}\right\}\ \text{ which is the \# of }\ X_i > \theta + \frac{x}{\sqrt{n}} \le \frac{n}{2} - 1 \\
									& = P\left\{\sum_{i = 1}^n Z_i \le \frac{n}{2} - 1\right\}\ \text{ when } Z_i = I\left\{X_i > \theta + \frac{x}{\sqrt{n}}\right\} \\
									& = P\left\{\frac{\sqrt{n}(\bar{Z}_n - \mu_n)}{\sigma_n} \le x_n\right\}\ \text{ where } \mu_n = E[Z_i] = 1 - F\left(\theta + \frac{x}{\sqrt{n}}\right) \rightarrow 0.5 \text{,} \\
									& \hspace{3cm} x_n = \frac{\sqrt{n}(0.5 - 1/n - \mu_n)}{\sigma_n} = \frac{\sqrt{n}(0.5 - \mu_n)}{\sigma_n} - \frac{1/\sqrt{n}}{\sigma_n} \text{,} \\
									& \hspace{3cm} \text{ and } \sigma^2_n = \mu_n (1- \mu_n) \rightarrow 0.25 \\
									&= P\left\{\frac{\sqrt{n}(\bar{Z}_n - \mu_n)}{\sigma_n} \le x_n\right\} - \Phi(x_n) + \Phi(x_n) \\
									& \le \underbrace{\frac{C}{\sqrt{n}}\frac{E[(Z_i - \mu_n)^2]}{\sigma_n^3}}_\textrm{\(\rightarrow 0\)} + \underbrace{\Phi(x_n)}_\textrm{Hmmm}\ \text{ by the B.E. CLT } 
	\end{align*}
\marginnote{Azeem writes in class that: \(\frac{C}{\sqrt{n}}\frac{E[(Z_i - \mu_n)^2]}{\sigma_n^3} \rightarrow \frac{8C}{\sqrt{n}\sigma^3_n} \rightarrow 0 \). Not clear to me where 8 is coming from as the numerator has a variance that's not 8 and the cubed term in the denominator is 8 and I dunno\dots}
Then what to do with out Hmmmmm term? Well working with our broken-up \(x_n\) we get:
	\[x_n = \underbrace{\frac{\sqrt{n}(0.5 - \mu_n)}{\sigma_n}}_\textrm{\(\rightarrow 2f(\theta)x\)} + \underbrace{\frac{1/\sqrt{n}}{\sigma_n}}_\textrm{\(\rightarrow 0\)}\]
and:
	\[\sqrt{n}(0.5 - \mu_n) = \sqrt{n}(F(\theta + x/\sqrt{n}) - F(\theta)) \rightarrow f(\theta)x\]
so:
	\[\Phi(x_n) \rightarrow \Phi(2f(\theta)x)\]
which is just the CDF of a \(\mathcal{N}\left(0, \frac{1}{4f^2(\theta)}\right)\) which gives us the result we wanted because the Hmmmmmmm term from above is converging to our desired distribution and the other term in that inequality is going to 0.\footnote{We have not proved the result for \(n\) odd, but the proof follows the same logic.}
\end{proof}
\end{ex}
\section{Confidence intervals}
\newthought{Another} popular way of testing a parameter is to construct a confidence interval around it. The general idea is to construct a set \(C_n\) for a choice of \(\alpha\) such that:
	\[P\{\theta(P) \in C_n\} \rightarrow 1-\alpha\]
where \(\theta(P)\) is the true value of our parameter of interest. Normally we'll think about constructing large-sample confidence intervals, but there's really no reason to limit our imagination:
\begin{ex}[Finite Sample Confidence Interval for Bernoulli] Suppose \(X_1, \dots, X_n \overset{iid}{\sim} Bernoulli(q)\) where \(q \in (0, 1)\) and we want to building a confidence set, \(C_n = C_n (X_1, \dots, X_n)\) such that:
	\[P\{\mu(P) \in C_n\} \ge 1-\alpha\]
for some choice of \(\alpha \in (0, 1)\).\footnote{With the caveat that we can't just pick \(C_n = [0, 1]\).} It follows that from the Bernoulli distribution that \(\mu(P) = q\). 

For finite samples there's really only one tool at our disposal for a task like this, Markov's Inequality:
	\begin{align*}
		P\{|\bar{X}_n - \mu(P)| > \epsilon\} & \le  \frac{Var(\bar{X}_n - q)}{\epsilon^2}\ \text{ because \(\mu(P) = q\)} \\
									& = \frac{Var(X_i)}{n \epsilon^2} \\
									& = \frac{q(1-q)}{n \epsilon^2}\ \text{ because of the Bernoulli assumption} \\
									& \le \frac{1}{4n \epsilon^2}\ \text{ because \(q(1-q) \in (0, 1)\) and \(x \le x\frac{1}{q(1-q)}\).} \\
	\end{align*}
Then we can \(\epsilon\) such that:\footnote{Fix rest of this for: \(\alpha = \frac{1}{4n\epsilon^2}\) because \(q(1-q) \le \frac{1}{4}\)!!}
	\[\alpha = \frac{1}{n\epsilon^2} \implies \epsilon = \sqrt{\frac{1}{4n\alpha}}\]
and if we plug in this choice of \(\epsilon\) we get:
	\begin{align*}
		& P\left\{|\bar{X}_n - \mu(P)| > \sqrt{\frac{1}{n\alpha}}\right\} \le \alpha \\
		\implies & 1- P\left\{|\bar{X}_n - \mu(P)| \le \sqrt{\frac{1}{n\alpha}}\right\} \le \alpha \\
		\implies & P\left\{|\bar{X}_n - \mu(P)| \le \sqrt{\frac{1}{n\alpha}}\right\} \ge 1-\alpha \\
		\implies & P\left\{-\sqrt{\frac{1}{n\alpha}} \le \bar{X}_n - \mu(P) \le \sqrt{\frac{1}{n\alpha}}\right\} \ge 1-\alpha \\
		\implies & P\left\{-\bar{X}_n -\sqrt{\frac{1}{n\alpha}} \le - \mu(P) \le -\bar{X}_n + \sqrt{\frac{1}{n\alpha}}\right\} \ge 1-\alpha \\
		\implies & P\left\{\bar{X}_n - \sqrt{\frac{1}{n\alpha}} \le \mu(P) \le \bar{X}_n + \sqrt{\frac{1}{n\alpha}}\right\} \ge 1-\alpha \\
		\implies & P\{\mu(P) \in C_n\} \ge 1- \alpha
	\end{align*}
where:
	\[C_n = \left(\bar{X}_n - \sqrt{\frac{1}{n\alpha}},\bar{X}_n + \sqrt{\frac{1}{n\alpha}}\right) \]
which is pretty neat.
\end{ex}
But what if we're interested in confidence intervals as \(n\) gets very large. That is, we want confidence sets that satisfy:
	\[P\{\mu(P) \in C_n\} \rightarrow 1-\alpha \text{?}\]
Let's consider a relatively simple example:
\begin{ex}[Bernoulli Confidence Interval] \begin{marginfigure}
	\includegraphics{bernoulli.jpg}
	\caption{Jakob Bernoulli in all his snooty glory.}
\end{marginfigure}
To ape the setup of the finite sample example above: Suppose \(X_1, \dots, X_n \overset{iid}{\sim} \text{Bernoulli}(q)\) where \(q \in (0, 1)\) and we want to building a confidence set, \(C_n = C_n (X_1, \dots, X_n)\) such that:
	\[P\{\mu(P) \in C_n\} \rightarrow 1-\alpha\]
for some choice of \(\alpha \in (0, 1)\). 

To start, note that:
	\[\frac{\sqrt{n}(\bar{X}_n - q)}{\sqrt{\bar{X}_n(1-\bar{X}_n)}} \overset{d}{\rightarrow} \mathcal{N}(0, 1)\]
which means that:
	\[P\left\{\frac{\sqrt{n}(\bar{X}_n - q)}{\sqrt{\bar{X}_n(1-\bar{X}_n)}} \ge x\right\} \rightarrow P\{Z \ge x\}\ \text{ by the definition of convergence in distribution} \]
and we'll make a convenient choice of \(x\):
	\begin{align*}
		P\left\{z_{1- \alpha/2} \le \frac{\sqrt{n}(\bar{X}_n - q)}{\sqrt{\bar{X}_n(1-\bar{X}_n)}} \le z_{\alpha/2} \right\} & = P\left\{\bar{X}_n - z_{1- \alpha/2}\sqrt{\frac{\bar{X}_n(1-\bar{X}_n)}{n}} \le q \le \bar{X}_n + z_{\alpha/2}\sqrt{\frac{\bar{X}_n(1-\bar{X}_n)}{n}} \right\} \\
		& \rightarrow P\{z_{1-\alpha/2} \le Z \le z_{\alpha/2}\} = 1 - \alpha \text{.}
	\end{align*}
So our choice of \(C_n\) is:
	\[C_n(X_1, \dots, X_n) = \left\{y \in [0, 1]:\ \bar{X}_n - z_{\alpha/2}\sqrt{\frac{\bar{X}_n(1-\bar{X}_n)}{n}} \le y \le \bar{X}_n + z_{\alpha/2}\sqrt{\frac{\bar{X}_n(1-\bar{X}_n)}{n}} \right\} \]
and it has the desired property of consistency in level.

But we obviously don't observe infinite amounts of data and \(C_n\) may behave poorly in finite-samples. In particular, for any \(n\) and \(\epsilon > 0\) there is a \(P\) (that is, a \(q\)) s/t \(P\{\mu(P) \in C_N\} \le \epsilon\). Let \(n, \epsilon\) be given and choose \(q = (1-\epsilon)^{1/n}\). Then \(P\{X_1 = 1, \dots, X_n = 1\} = 1-\epsilon\) and \(X_1 = 1, \dots, X_n = 1\) implies \(C_n = \{1\}\) which implies that \(q \not \in C_n\). Then \(1- \epsilon \le P\{q \ne C_n\}\) implies \(P\{q \in C_n\} \le \epsilon\).\footnote{This observation is very similar to the weak instruments literature.} 

\end{ex}

How much do we get when we move to asymptotic? Well let's compare the two confidence sets we've derived for \(n = 100, \bar{X}_n = 0.5, \alpha = 0.05\):
	\[\text{Finite: } C_n = (0.06, 0.94)\]
	\[\text{Asymptotic: } C_n = (0.4, 0.6)\text{.}\]
So it obviously follows that we get a lot of power with our asymptotic. 

%N \left(  \begin{array}{c} 5 \\ 20 \\ 7.5 \end{array} , \left[\begin{array}{ccc} 7 & 2 & 0 \\ 2 & 5 & 0.5 \\ 0 & 0.5 & 3 \end{array} \right] \right)\]

Thus far we've only thought about confidence intervals for univariate problems. What if we move to higher dimensional problems? 
\begin{ex}[Confidence Interval for Multivariate Case] Suppose we have \(X_1, \dots, X_n \overset{iid}{\sim} P\) where \(X_i \in \mathbb{R}^k\) with \(\Sigma(P) < \infty\) and non-singular\footnote[][-1in]{Equivalently it's strictly positive definite.} and we want to construct a confidence region \(C_n\) such that: 
\begin{marginfigure} \includegraphics{mvn.pdf} \caption{Several vantage points of 1000 i.i.d. draws from \(\mathcal{N}(\mu, \Sigma)\) in \(\mathbb{R}^3\). Sweeeeeet.} \end{marginfigure}
	\[P\{\mu(P) \in C_n\} \rightarrow 1 - \alpha \text{.}\]
Recall that the CLT gives us:
	\[\sqrt{n}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \mathcal{N}(0, \Sigma(P))\]
then from non-singularity we get the following fact:
	\[\text{For }\ Z \sim \mathcal{N}(0, \Sigma(P)) \implies Z'\Sigma^{-1}(P)Z \sim \chi_k^2 \text{.}\]
Then by CMT2:
	\[n(\bar{X}_n - \mu(P))'\Sigma(P)^{-1}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \chi_k^2\]
and we can use the analog principle for \(\Sigma(P)\):
	\[\hat{\Sigma}_n = \frac{1}{n} \sum_{i = 1}^n (X_i - \bar{X}_n)(X_i - \bar{X}_n)'\]
and then we can use the further fact that:
	\[\hat{\Sigma}_n \overset{p}{\rightarrow} \Sigma(P) \implies \hat{\Sigma}_n^{-1} \overset{p}{\rightarrow} \Sigma(P)^{-1}\]
because \(\Sigma(P)\) is non-singular. 

So,
	\[n(\bar{X}_n - \mu(P))'\hat{\Sigma}_n^{-1}(\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \chi_k^2\]
and we can use this to test the null, e.g., \(H_0: \mu(P) = 0\) vs. \(H_A: \mu(P) \ne 0\) at level \(\alpha\) with:
	\[T_n = n \bar{X}_n' \hat{\Sigma}_n^{-1} \bar{X}_n\]
and:
	\[c_n = c_{k, 1-\alpha} = 1-\alpha\ \text{ quantile of a } \chi_k^2\]
and it's easy to show that \(P\{T_n > c_n\} \rightarrow 1-\alpha\). Then:
	\[1 - P\{T_n \le c_n\} \rightarrow \alpha \implies P\{T_n \le c_n\} \rightarrow 1 - \alpha \text{.}\]
In this spirit define:
	\[C_n(X_1, \dots, X_n) = \left\{y \in \mathbb{R}^k:\ n(\bar{X}_n - y)'\hat{\Sigma(P)}^{-1}(\bar{X}_n - y) \le c_{k, 1-\alpha}\right\}\]
and it follows from the earlier example that:
	\[P\{\mu(P) \in C_n) \rightarrow 1-\alpha \text{.}\]
\end{ex}	


\section{Tightness}
\newthought{Let's} just jump into the definition on this fucker:
\begin{mydef}[Tight] A sequence of random vectors, \(\{X_n:\ n \ge 1\}\), is tight\footnote{Also known as bounded in probability. Recall that a sequence of non-random vectors, \(x_n\), is bounded if \(\exists\ M\) s.t.:
	\[|x_n| \le M\ \forall n\text{.}\]
} if for every \(\epsilon > 0\) there exists \(B > 0\) such that:
	\[\inf_n P\{|X_n| \le B\} \ge 1 - \epsilon \text{.}\]
Equivalently, if for every \(\epsilon > 0\) and every \(n \ge 1\) there exists \(B > 0\) such that:\footnote{The non-random parallel is that if \(x_n \rightarrow x\) then \(x_n\) is bounded.}
	\[P\{|X_i| \le B\} \ge 1- \epsilon \]
which just says that the probability that any member of the sequence is less than some constant, \(B\), won't exceed \(1-\epsilon\). Sometimes, though, it's convenient to re-arrange and express tightness with:
	\[P\{|X_i| > B\} \le \epsilon \]
which you can easily derive by taking the complement and multiplying out the negative.
\end{mydef}
\begin{marginfigure} \includegraphics{tighten.jpg} \caption{``We didn't say lose weight\dots I might say tighten.''} \end{marginfigure}


\begin{lem}[Convergence is Distribution \(\implies\) Tight] Suppose \(X_n \overset{d}{\rightarrow} X\). Then \(X_n\) is tight.
	\begin{proof}
		Fix \(\epsilon\) and pick \(B\) such that: 
			\[P\{|X| > B\} \le \epsilon \text{.}\] 
		Suppose \(X_n \overset{d}{\rightarrow} X\). Then: \(P\{|X_n| > B\} \rightarrow P\{|X| > B\}\). Then from the definition of limits, we know there exists some \(N\) such that for all \(n \ge N\):
			\[P\{|X_n| > B\} \le \epsilon \text{.}\]
		Then for all \(n \in \{1, \dots, N-1\}\) pick \(B'\) such that:
			\[B' = \max_{n \in \{1, \dots, N-1\}}\{|X_n|\} + \delta\]
		where \(\delta > 0\). Then:
			\[P\{|X_n| > B'\} = 0\ \text{for }\ n \in \{1, \dots, N-1\}\text{.} \]
		Pick \(B^* = \max\{B, B'\}\). Then for \(B^*\) we have that for all \(n\):
			\[P\{|X_n| > B^*\} \le \epsilon \text{.}\]	
		Then \(X_n\) is tight.
	\end{proof}
\end{lem}
In general, the converse is not true, however, there is a sense in which tightness implies convergence in distribution:\footnote{The Balzano-Weierstrass Theorem is the non-random equivalent, which tells us that if \(x_n\) is bounded then \(\exists\ n_j\) and \(x\) s.t. \(x_{n_j} \rightarrow x\).}
\begin{thm}[Prokhorov's Theorem] If \(\{X_n: n \ge 1\}\) is a tight sequence of random vectors, then there exists a subsequence, \(n_j\) and a random vector \(X\) such that:
	\[X_{n_j} \overset{d}{\rightarrow} X\]
\end{thm}
\begin{ex}[Counterexample of Convergence in Distribution and Tightness] Pick \(X_n = (-1)^n\) and \(X = 1\). Then \(X_n\) is tight. Pick \(B = 10\) then:
	\[P\{|X_n| > B\} = P\{1 > 10\} = 0\]
which holds for any \(\epsilon > 0\). But \(P\{X_n \le x\} \not \rightarrow P\{X \le x\}\). Interestingly though, there exists a subsequence, \(n_j \in 2\mathbb{N}\), such that the sequence does converge in distribution, which is exactly Prokhorov's claim. 
\end{ex}
\begin{marginfigure}
	\includegraphics{prokhorov.jpg}
	\caption{Yuri Vasilyevich Prokhorov. Known associate of Andrey Nikolaevich Kolmogorov.}
\end{marginfigure}
\begin{ex} Suppose \(X_n \overset{p}{\rightarrow} 0\). Then \(X_n\) is tight. For a proof use the exact same argument in the theorem above.
\end{ex}

\newthought{A stronger} version of consistency for an estimator is \(\tau_n\) consistency.
\begin{mydef}[\(\tau_n\) Consistent] Suppose you have a sequence, \(\tau_n \rightarrow \infty\). If \(\tau_n(\hat{\theta}_n - \theta)\) is tight then we say that \(\hat{\theta}_n\) is a \(\tau_n\) consistent estimator of \(\theta\). 
\end{mydef}

\begin{ex} Suppose \(X_1, \dots, X_n \overset{iid}{\sim} P\) on \(\mathbb{R}\) with \(\sigma^2(P) < \infty\) then the CLT tells us that:
	\[\sqrt{n} (\bar{X}_n - \mu(P)) \overset{d}{\rightarrow} \mathcal{N}(0, \sigma^2(P))\]
which tells us that \(\sqrt{n} (\bar{X}_n - \mu(P)) \) is tight, which tells us that \(\bar{X}_n\) is \(\sqrt{n}\)-consistent for \(\mu(P)\). 
\end{ex}

\begin{lem}[\(\tau_n\) consistent \(\implies\) consistent] 
\begin{proof}Suppose \(\hat{\theta}_n\) is a \(\tau_n\) consistent estimator for \(\theta\). Then:
	\[\tau_n(\hat{\theta}_n - \theta)\]
is tight. Which means that for every \(\delta > 0\) and every \(n \ge 1\) there exists \(B > 0\) such that:
	\begin{align*}
		& P\{|\tau_n(\hat{\theta}_n - \theta)| \le B\} \ge 1- \delta \\
		\implies & P\left\{|\hat{\theta}_n - \theta| \le \frac{B}{|\tau_n|}\right\} \ge 1- \delta \text{ because \(|xy| = |x||y|\)} \\
		\implies & P\left\{|\hat{\theta}_n - \theta| > \frac{B}{|\tau_n|}\right\} < \delta\ \text{ because \(P\{A\} = 1 - P\{A^c\}\).}
	\end{align*}
Then fix some \(\epsilon\). Because \(B\) is a fixed number and \(\tau_n \rightarrow \infty\) we know that for some \(N\), all \(n > N\) will have the following feature: \(\epsilon > \frac{B}{|\tau_n|}\). Then:
	\[\underbrace{\delta > P\left\{|\hat{\theta}_n - \theta| > \frac{B}{|\tau_n|}\right\}}_\textrm{\(\forall\ n\)} \underbrace{\ge P\left\{|\hat{\theta}_n - \theta| > \epsilon \right\}}_\textrm{\(\forall\ n>N\)} \]
which gives us consistency!
\end{proof}
\end{lem}

\section{Stochastic order notation}
\newthought{If} \(X_n \overset{p}{\rightarrow} 0\) then we can write: \(X_n = o_P(1)\). If \(X_n\) is tight then we write: \(X_n = O_P(1)\).\marginnote{The non-random equivalents are: If \(x_n \rightarrow 0\) then we may write \(x_n = o(1)\) and if \(x_n\) is bounded then we may write \(x_n = O(1)\).} More generally:
	\begin{description}
		\item \(X_n = o_P(R_n) \) for \(X_n = R_n Y_n\) with \(Y_n = o_p(1)\).
		\item \(X_n = O_P(R_n) \) for \(X_n = R_n Y_n\) with \(Y_n = O_p(1)\).
	\end{description}
\begin{lem}[Calculus of Stochastic Order Notation] Stochastic order notation has the following rules:
	\begin{enumerate}
		\item \(o_P(1) + o_P(1) = o_P(1)\)
		\item \(o_P(1) + O_P(1) = O_P(1)\)
		\item \(o_P(1)O_P(1) = o_P(1)\)
		\item \(\frac{1}{1 + o_P(1)} = O_P(1) \)
		\item \(o_p(O_p(1)) = o_p(1)\)
	\end{enumerate}
	\begin{proof} We'll prove \(o_p(1)O_p(1) = o_p(1)\). Let \(X_n = o_p(1)\) and \(Y_n = O_p(1)\) and we want to show that \(X_nY_n = o_p(1)\). That is, we want to show that:
			\[\forall \epsilon > 0,\ P\{|X_n Y_n| > \epsilon \} \rightarrow 0\]
		We'll tackle this with a proof by contradiction. Suppose not. Then there exists \(\epsilon > 0\) s.t.:
			\[P\{|X_n Y_n| > \epsilon \} \not \rightarrow 0\]
		Then by Balzano-Weierstrass gives us that there exists \(n_j\) and \(\delta > 0\) s.t.:
			\[P\{|X_{n_j}Y_{n_j}| > \epsilon \} \rightarrow \delta \]
		and then by Prokhorov's Theorem there exists \(n_{j_k}\) and \(Y\) s.t. \(Y_{n_{j_k}} \overset{d}{\rightarrow} Y\). Then \(X_{n_{j_k}} \overset{p}{\rightarrow} 0\) because if \(X_n \overset{p}{\rightarrow} 0\) then any subsequence is doing the same. Then by Slutsky's Lem.:
			\[X_{n_{j_k}}Y_{n_{j_k}} \overset{p}{\rightarrow} 0\]
		but we originally supposed that:
			\[P\{|X_{n_{j_k}}Y_{n_{j_k}}| > \epsilon\} \rightarrow \delta > 0\]
		which establishes our contradiction. \marginnote{\#dumbledoresarmy}
		\renewcommand{\qedsymbol}{{\(\Rightarrow \Leftarrow\)}}
	\end{proof}
\end{lem}

\section{Large sample theory Core questions}
\begin{ex}[Summer 2012] Let \((X_i, Y_i),\ i = 1, \dots, n\) be a sequence of independent bivariate random vectors such that \(X_i\) and \(Y_i\) are independent, \(X_i \sim \mathcal{N}(\mu, \sigma^2_i)\) and \(Y_i \sim \mathcal{N}(\mu, \sigma^2_i)\). Suppose further that there exists \(\epsilon > 0\) and \(B < \infty\) s.t.:
	\[\epsilon \le \sigma_i^2 \le B\ \forall i \text{.}\]
In this question, you are asked to do inference on \(\mu\) despite the fact that the number of nuisance parameters (i.e., the \(\sigma_i^2\)) is growing with the sample size \(n\). To this end, consider the estimator of \(\mu\) given by:
	\[\hat{\mu}_n = \frac{1}{2n} \sum_{i=1}^n (X_i + Y_i) \text{.}\]

\subsection{(a) Is \(\hat{\mu}_n\) an unbiased estimator of \(\mu\)? Justify your answer.} Applying the expectations operator yields:
	\begin{align*}
		E[\hat{\mu}_n] &= E\left[ \frac{1}{2n} \sum_{i=1}^n (X_i + Y_i) \right] \\
					&= \frac{1}{2n} \left(\sum_{i=1}^n E[X_i] + \sum_{i=1}^n E[Y_i] \right) \\
					&= \frac{1}{2n} (n\mu + n\mu) \\
					&= \mu \text{.}
	\end{align*}
Then by simply invoking the definition of unbiasedness we can conclude that \(\hat{\mu}_n\) is unbiased.

\subsection{(b) Compute \(Var(\hat{\mu}_n)\). Is \(\hat{\mu}_n\) a consistent estimator of \(\mu\)? Justify your answer.}
	\begin{align*}
		Var(\hat{\mu}_n) &= \frac{1}{4n^2} \sum_{i=1}^n Var(X_i + Y_i)\ \text{ b/c ind. } \\
					&= \frac{1}{4n^2} \sum_{i=1}^n Var(X_i) + Var(Y_i)\ \text{ b/c ind. } \\
					&= \frac{1}{4n^2} \sum_{i=1}^n 2\sigma_i^2 \\
					&= \frac{1}{2n^2} \sum_{i=1}^n \sigma_i^2 \text{.}
	\end{align*}
But what about consistency? 
	\begin{align*}
		\hat{\mu}_n &= \frac{1}{2n} \sum_{i=1}^n (X_i + Y_i) \\
				&= \frac{1}{2} \left( \frac{1}{n} \sum_{i=1}^n X_i + \frac{1}{n} \sum_{i=1}^n Y_i \right) \\
				&\overset{p}{\rightarrow} \frac{1}{2}(\mu + \mu)\ \text{ by W.L.L.N. and C.M.T.} \\
				&= \mu
	\end{align*}
which establishes consistency.
\subsection{(c) Is \(\hat{\sigma}_i^2\) an unbiased estimator of \(\sigma_i^2\)? Justify your answer.}
The proposed estimator is:
	\[\hat{\sigma}_i^2 = \frac{1}{2} (Y_i - X_i)^2\]
and to see if it's unbiased or not we can simply apply the expectations operator to it:
	\begin{align*}
		E[\hat{\sigma}_i^2] &= E\left[\frac{1}{2} (Y_i - X_i)^2 \right] \\
						&= \frac{1}{2} E[Y_i^2 - 2Y_i X_i + X_i^2] \\
						&= \frac{1}{2} \left( E[Y_i^2] + E[X_i^2] - 2E[Y_i X_i] \right) \\
						&= \frac{1}{2} \left( E[Y_i^2] + E[X_i^2] - 2E[Y_i] E[X_i] \right)\ \text{ b/c of independence} \\
						&= \frac{1}{2} \left( E[Y_i^2] + E[X_i^2] - 2\mu^2 \right) \\
						&= \frac{1}{2}\left( \underbrace{E[Y_i^2] - \mu^2}_\textrm{\(\sigma_i^2\)} + \underbrace{E[X_i^2] - \mu^2}_\textrm{\(\sigma_i^2\)} \right) \\
						&= \sigma_i^2
	\end{align*}
which establishes that our estimator is unbiased.

\subsection{(d) Compute \(Var(\hat{\sigma}_i^2)\). Is \(\hat{\sigma}_i^2\) consistent? (Hint: If \(Z \sim \mathcal{N}(0, \tau^2)\) then \(E[Z^4] = 3\tau^4\).)} 
To compute the variance of \(\hat{\sigma}_i^2\) we'll just apply the formula for variance:
	\begin{align*}
		Var(\hat{\sigma}_i^2) &= E\left[ \left(\frac{1}{2}(Y_i - X_i)^2 - \sigma_i^2 \right)^2 \right] \\
						&= E\left[\frac{1}{4}(Y_i - X_i)^4 - (Y_i-X_i)^2 \sigma_i^2 + \sigma_i^4 \right] \\
						&= \frac{1}{4} E[(Y_i - X_i)^4] - \sigma_i^4
	\end{align*}
and we know that \(E[Y_i - X_i] = 0\) and that \(Var(Y_i - X_i) = 2 \sigma_i^2\) because the two variables are independent. Then:
	\[(Y_i - X_i) \sim \mathcal{N}(0, 2\sigma_i^2)\]
and the hint tells us that:
	\[E[(Y_i - X_i)^4] = 3(2\sigma_i^2)^2 \]
which we can apply to get:
	\[Var(\hat{\sigma}_i^2) = 2\sigma_i^4 \text{.}\]
	
Is the estimator consistent? 
\end{ex}

\chapter{Conditional expectations}
\newthought{Traditionally} conditional expectations are defined by with something like:
	\[E[Y|X=x] = \int y f(y|x)dy\]
where \(f(y|x)\) is the conditional density of \(Y\). That's a useful definition but it presupposes the existence of a continuous conditional density function to integrate over.\footnote{Also, what a pain in the butt to work with! You aren't going to get closed-form solutions for some of even the best behaved p.d.f.'s.} Instead we'll work with an equivalent (but far more general) definition that's tightly connected to linear regressions. 

\begin{mydef}[Conditional Expectation of \(Y\) given \(X\)] Let \(Y, X\) be a random vector with \(Y\in \mathbb{R}\) and \(X \in \mathbb{R}^k\) and we'll assume \(E[Y^2] < \infty\). Then define: 
	\[\mathbb{M} := \left\{m(X):\ m:\mathbb{R}^k \rightarrow \mathbb{R}\ \text{ and } E[m^2(X)] < \infty \right\}\]
and consider the following minimization problem:
	\[\inf_{m(X) \in \mathbb{M}} E[(Y - m(X))^2]\]
then it's possible to show that there exists \(m^*(X) \in \mathbb{M}\) s.t.:
	\[E[(Y - m^*(X))^2] = \inf_{m(X) \in \mathbb{M}} E[(Y - m(X))^2]\]
and define:
	\[E[Y|X] := m^*(X)\]
and we think of \(E[Y|X]\) as the ``best predictor'' of \(Y\) given \(X\). 
\end{mydef}

\begin{thm} \(m^*(X)\) solves the minimization problem i.f.f. for all \(m(X) \in \mathbb{M}\):
	\[E[(Y-m^*(X))m(X)] = 0\]
which we can think of as an orthogonality condition.\footnote{To see this, call \(e := Y - m^*(X)\). Then we just require: \[E[m(X)e] = 0\] which is the analog of the familiar orthogonality restriction for OLS.}  And if \(\tilde{m}(X) \in \mathbb{M}\) also solves the minimization problem, then:
	\[P\{m^*(X) = \tilde{m}(X)\} = 1\]
which is just to say that the they're the same solution in a probabilistic sense. 
	\begin{proof} There are really two claims in this theorem. The first is an i.f.f. and the second is not. We'll tackle the i.f.f. part first.
	
	Suppose the orthogonality condition holds for all \(m(X) \in \mathbb{M}\). Then we want to show that the solution to the minimization problem is \(m^*(X)\). Fix \(m(X)\). Then starting with the minimization problem, we can add and subtract \(m^*(X)\) to get:
		\begin{align*}
			E[(Y - m(X))^2] & = E[(Y - m^*(X) + m^*(X) - m(X))^2] \\
						& = E[(Y - m^*(X))^2] + 2E[(Y - m^*(X))\underbrace{(m^*(X) - m(X))}_\textrm{Call \(\tilde{m}(X) \in \mathbb{M}\)}] + E[(m^*(X) - m(X))^2] \\
						\diamond \diamond \diamond & = E[(Y - m^*(X))^2] + E[(m^*(X) - m(X))^2]\ \text{ b/c orthog. gives us }\ E[(Y-m^*(X))\tilde{m}(X)] = 0 \\
						& \ge E[(Y-m^*(X))^2]\ \text{ b/c } (m^*(X) - m(X))^2 > 0
		\end{align*}
	which holds for any \(m(X)\), which establishes that \(m^*(X)\) is the minimizing value of \(m(X)\) for the minimization problem in the theorem. 
	
	Now we do the other direction of the i.f.f. claim. Suppose that \(m^*(X)\) solves the minimization problem. We want to find\footnote{wtf} that the orthogonality condition holds. To do this we'll compare \(m^*(X)\) with \(\check{m}(X) := m^*(X) + \alpha m(X)\) for \(\alpha \in \mathbb{R}\). Fix \(m(X)\) in \(\check{m}(X)\). Then because \(m^*(X)\) is the minimizer we get that:
		\[E[(Y-m^*(X))^2] \le E[(Y - \check{m}(X))^2] = E[(Y - m^*(X) - \alpha m(X))^2]\ \text{ for any } \alpha\]
	which we can expand and re-arrange to get:
		\[2\alpha E[m(X)(Y - m^*(X))] \le \alpha^2E[m^2(X)]\ \forall \alpha\]
	now recall that our goal is to show that the LHS term is equal to 0. Divide through by \(2\alpha\) and pick \(\alpha = 0\). The choice of \(\alpha = 0\) makes the inequality an equality.\footnote{Because:
		\[E[(Y - m^*(X))^2] = E[(Y-\check{m}(X))^2]\]
	when \(\alpha = 0\).} Then we get:
		\[E[m(X)(Y - m^*(X))] = 0\]
	and then we've attained our goal because this holds for any \(m(X)\). 
		
	Now we just need to deal with the \(\tilde{m}\) part of the theorem. Suppose \(\tilde{m}(X)\) solves the minimization problem. Then plug in \(m = \tilde{m}\) into \(\diamond \diamond \diamond \) above to get: \marginnote{\(\diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond \diamond\) }
		\begin{align*}
			E[(\tilde{m}(X) - m^*(X))^2] & = E[(Y - \tilde{m}(X))^2] - E[(Y - m^*(X))^2] \\
								& = 0\ \text{ b/c both \(m^*(X)\) and \(\tilde{m}(X)\) minimize. }
		\end{align*}
	And \(E[(\tilde{m}(X) - m^*(X))^2] = 0 \implies P\{m^*(X) = \tilde{m}(X)\} = 1\).
	\end{proof}
\end{thm}

\newthought{A less} restrictive definition of conditional expectations only requires \(E[|Y|] < \infty\). In this case we define \(E[Y|X]\) to be any \(m^*(X)\) with \(E[|m^*(X)|] < \infty\) s.t.:
	\[E[(Y - m^*(X)) I\{X \in \mathcal{B}\}] = 0\ \forall\ \text{(Borel) set } \mathcal{B}\]
Working off of just this definition we can derive many of the properties we know and love about conditional expectations: 
\begin{lem}If \(Y = f(X)\) then \(E[Y | X] = f(X)\). 
	\begin{proof} If \(Y = f(X)\) then \(E[Y|X]\) is any \(m^*(X)\) s.t.:
			\[E[(Y - m^*(X))I\{X \in \mathcal{B}\}] = 0\]
		for any \(\mathcal{B}\). Plugging in \(Y = f(X)\) we clearly see that the \(m^*(X)\) that solves the formula is \(f(X)\):
			\[E[(f(X) - m^*(X))I\{X \in \mathcal{B}\}] = 0\]
		and if \(m^*(X) = f(X)\) then we say that: \(E[Y|X] = f(X)\ \).
	\end{proof}
\end{lem}
\begin{lem} \(E[Z + Y | X] = E[Z | X] + E[Y | X]\). \begin{proof} For \(E[Z + Y | X]\) we have that \(m^*(X) = E[Z + Y| X]\) is the solution to:
		\[E[(Z+Y - m^*(X))I\{X\in \mathcal{B}\}] = 0\]
	for any set \(\mathcal{B}\). Then we want to show that another solution is \(E[Z|X]\) and \(E[Y|X]\). To see why, we'll simply do some algebra:
	\begin{align*}
		E[(Z + Y - E[Z|X] - E[Y | X])I\{X \in \mathcal{B}\}] & = E[(Z - E[Z|X])I\{X \in \mathcal{B}\}] + E[(Y - E[Y | X])I\{X \in \mathcal{B}\}] \\
											& = 0
	\end{align*}
	which establishes that \(m^*(X) = E[Z|X] + E[Y|X]\) is also a solution because of the properties for each of those conditional expectations. 
	\end{proof}
\end{lem}
\begin{lem}\(E[f(X)Y|X] = f(X)E[Y|X]\).
	\begin{proof} We'll prove for the case when \(f(X) = I\{X \in \tilde{\mathcal{B}}\}\) to get intuition. In this case:
			\[E[(f(X)Y - f(X)E[Y|X])I\{X \in \mathcal{B}\}] = E[(Y - E[Y|X])I\{X \in \tilde{\mathcal{B}} \cap \mathcal{B}\}] = 0\]
	\end{proof}
\end{lem}
\begin{lem} If \(P\{Y \ge 0\} = 1\) then \(P\{E[Y|X] \ge 0\} = 1\). \end{lem}
\begin{lem}[Law of Iterated Expectations] If \(\mathcal{B} = \mathbb{R}^k\) we get:
			\[E[Y - E[Y|X]] = 0 \implies E[Y] = E[E[Y|X]] \text{.}\]
	More generally we can write:
			\[E[E[Y|X_1, X_2]| X_1] = E[Y | X_1]\]
which is also sometimes called the Tower Property.
\end{lem}
\begin{lem} If \(Y\) is independent of \(X\), then:\footnote{This is sometimes called \emph{mean independence}. Independence \(\implies\) mean independence \(\implies\) uncorrelated but the reverse direction is generally false.}
	\[E[Y |X] = E[Y]\]
	\begin{proof}
		\[E[(Y - E[Y])I\{X \in \mathcal{B}\}] = E[(Y - E[Y])]P\{X \in \mathcal{B}\} = 0\]
	\end{proof}
\end{lem}

\newthought{We} can also extend several results we had for expectations for conditional expectations:
\begin{thm}[Jensen's Inequality---Conditional Expectations] Let \(\phi:\mathbb{R} \rightarrow \mathbb{R}\) be a convex function. Suppose \((Y,X)\) is a random vector on \(\mathbb{R}\times\mathbb{R}^k\) such that \(E[|Y|] < \infty\) and \(E[|\phi(Y)|] < \infty\). Then with probability 1:
	\[E[\phi(Y)|X] \ge \phi(E[Y|X]) \text{.}\]
\end{thm}
\begin{mydef}[Conditional Variances] Let \((X,Y)\) be a random vector where \(X\) takes on values in \(\mathbb{R}^k\) and \(Y\) takes on values in \(\mathbb{R}\). Then is \(E[Y^2] < \infty\) we define:\footnote{One useful result of this definition is:
	\[Var(Y) = E[Var(Y|X)] + Var(E[Y|X])\text{.}\]}
	\[Var(Y|X) = E[(Y-E[Y|X])^2|X] = E[Y^2|X] - E[Y|X]^2 \text{.}\]
\end{mydef}
	
\chapter{Linear Regression}
\newthought{Let} \((Y, X, U)\) be a random vector with: \(Y \in \mathbb{R}\), \(X \in \mathbb{R}^{k+1}\), and \(U \in \mathbb{R}\). We'll assume:
	\[X = (X_0, \dots, X_k)'\ \text{ with } X_0 = 1\]
and:
	\[\beta \in \mathbb{R}^{k+1},\ \beta = (\beta_0, \dots, \beta_k)'\]
and:
	\[Y = X' \beta + U \text{.}\]
Then there are three ways to think about interpreting:
\begin{enumerate}
\item \emph{Linear Conditional Expectation}: Assume \(E[Y | X] = X'\beta\) and define:
	\[U := Y - E[Y | X]\]
so \(Y = X'\beta + U\). Then:
	\[E[U|X] = 0 \implies E[U] = 0,\ E[XU] = 0 \text{.}\]
But there is no causal interpretation to \(\beta\) here.\footnote{Or marginal effect interpretation here: \[\frac{\partial E[Y|X]}{\partial X_j}\text{.}\]} That is, if there's a one unit increase in \(X_j\) there's nothing that says we should expect a \(\beta_j\) unit increase in \(Y\). What's missing is a model of how \(Y\) is determined as a function of \(X\).\footnote{E.g., ice cream cones don't kill kids in swimming pools.} In the Linear Conditional Expectations approach \(\beta\) is just a convenient way of summarizing \(E[Y|X]\).  

\item \emph{Best Linear Prediction\footnote{The non-IO BLP.} of \(Y\) given \(X\)}: We don't have to be certain that \(E[Y|X]\) is linear, but we can ask for a linear approximation. That is, a function of the form \(X'b\) where \(b\) is some \(b \in \mathbb{R}^{k+1}\) that is ``close'' to \(E[Y|X]\). More precisely:
	\[\min_{b \in \mathbb{R}^{k+1}} E\left[(E[Y|X] - X'b)^2\right] \text{.}\]
Equivalently we can work with:
	\[\min_{b \in \mathbb{R}^{k+1}} E\left[(Y - X'b)^2\right] \text{.}\]
Why? Well we'll work through a proof because the equivalence between these two minimization problems is one of the canonical examples of the mechanics of OLS. To do so we'll start with the first expression: \marginnote[1.2in]{We get that \(E[VX'] = 0\) because:
	\begin{align*}
		E[VX'] & = E[E[Y|X]X' - YX'] \\
			& = E[E[YX'|X]] - E[YX'] \\
			& = E[YX'] - E[YX']\ \text{ by the law of it. exp.} \\
			& = 0\ \text{ by mathematics. }
	\end{align*}}
	\begin{align*}
		E\left[(E[Y|X] - X'b)^2\right] = & E\left[(\underbrace{E[Y|X] - Y}_\textrm{V} + Y - X'b)^2\right]\\
							= & E[V^2] + 2E[V(Y-X'b)] + E[(Y - X'b)^2] \\
							= & E[V^2] + 2E[VY] -2\underbrace{E[VX']}_\textrm{\(=0\)}b + E[(Y - X'b)^2] \\
							= & A + E[(Y - X'b)^2]\	
	\end{align*}
and if we take a derivative w/r/t \(b\) the constant, \(A\), will drop out because it doesn't depend on \(b\). Then we've proved that the two minimization problems are equivalent. Sweet.

What does the minimization problem actually look like? Well, the F.O.C. is:
	\[-2E[X(Y - X'b)] = 0\]
and we can neglect the \(-2\) and argue that any solution to the choice of \(b\), which we'll call \(\beta\), has to satisfy:
	\[E[X(Y - X'b)] = 0 \text{.}\]
Then in the BLP context we can define the error term as a function of \(Y\), \(X\), and the solution to the minimization problem, \(\beta\): \(U := Y - X'\beta\). Then we can rearrange to get the familiar:
	\[Y = X'\beta + U\] 
and we can re-express the F.O.C. to get the familiar OLS moment restriction:
	\[E[XU] = 0 \text{.}\] 
However, just because we've managed to express the BLP problem in terms familiar to, say, estimating treatment effects from an experimental intervention, doesn't mean we have a causal interpretation here. \(U\) is just a residual that makes everything work. We need to impose actual structure on \(U\) in order to get the interpretation economists' desire. 

\item \emph{Causal Model}: Assume \(Y = g(X, U)\) where \(X\) is observed determinants of \(Y\) and \(U\) is observed determinants of \(Y\) and \(g(\cdot)\) is a model for how \(Y\) is determined.\footnote{Hopefully \(g(\cdot)\) is coming from economics or physics or logic or some other place we like.} Then:
	\[\frac{\partial g(X, U)}{\partial X_j}\ \text{ is the effect of \(X_j\) on \(Y\) }\]
and if we assume further that:
	\[g(X, U) = X'\beta + U\]
then the partial derivative above is just \(\beta_j\). 

In general, we don't know much about \(U\). For a given problem we generally ask: \(E[U] = 0\)? \(E[XU] = 0\)? \(E[U|X] = 0\)? These are just statements about the relationship between observed and unobserved determinants of \(Y\). But even though we may have \(E[U] \ne 0\), we can normalize so that it is. That is, replace \(\beta_0\) with \(\beta_0 + E[U]\) and \(U\) with \(U - E[U]\). 
\end{enumerate}

\section{Linear regression with exogeneity assumption}
\newthought{If there's} some reason that know \(E[XU] = 0\) then we're necessarily in the third interpretation of linear regression from above.\footnote{That's because in Interpretations \#1,2, \(E[XU] = 0\) was just a result of a functional relationship that the setup imposed, whereas Interpretation \#3 requires us to know something about the relationship between \(X\) and \(U\) that could give us a condition like \(E[XU] = 0\) that we ex-ante assume.} Now let's get down to brass tax and think about how to solve for \(\beta\). We'll use the following setup as above: \((Y, X, U)\) where \(Y \in \mathbb{R}, U \in \mathbb{R}, X \in \mathbb{R}^k\), and:
	\[Y = X'\beta + U\]
with \(E[XU] = 0, E[XX'] < \infty\), and no perfect collinearity in \(X\).\footnote{Don't know what those words mean? Here's some help: \begin{mydef}[Perfect Colinearity] We say there is perfect colinearity (or multicolinearity) in \(X\) if:	
	\[\exists\ c \ne 0, c \in \mathbb{R}^{k+1}\ s.t.\ P\{X'c = 0\} = 1 \text{.}\] 
I.e., we can express one component of \(X\) as a linear combination of the others. \end{mydef}} Why do we assume no perfect collinearity in \(X\)? The following lemma should make it clear why.

\begin{lem} Assume \(E[XX'] < \infty\) then \(E[XX']\) is invertible i.f.f. there is no perfect collinearity in \(X\).
	\begin{proof} Easy to show the equivalent statement: \(E[XX']\) is not invertible i.f.f. there is perfect collinearity in \(X\). 
	
	First we'll start from perfect collinearity. Suppose there is perfect collinearity in \(X\). Then \(\exists\ c\ne 0\) s.t. \(P\{X'c=0\} = 1\). Then \(E[XX']c = E[X(X'c)] = 0\). Then \(E[XX']\) isn't invertible because it's equal to 0 and that's a quantity that's notoriously difficult to invert. 
	
	To take care of the other direction suppose that \(E[XX']\) isn't invertible. Then there's some number \(c \ne 0\) s.t. \(E[XX']c = 0\). Then if we multiply through by \(c'\) we get:
		\[c'E[XX']c = E[(X'c)^2] = 0\]
	which only holds if \(P\{X'c = 0\} = 1\). Then we've established that there's perfect collinearity \(X\) and we're all set with this proof.
	\end{proof}
\end{lem}

\noindent Plugging \(U = Y - X'\beta\) into \(E[XU] = 0\) we get:
	\[E[XY] = E[XX']\beta \implies \beta = E[XX']^{-1}E[XY] \text{.}\]
If there is perfect collinearity in \(X\), then there are generally multiple solutions to \(E[XY] = E[XX']\beta\) but any two solutions, \(\tilde{\beta}, \beta\) satisfy:
	\[P\{X'\beta = X'\tilde{\beta}\} = 1\]
which doesn't tell us that \(\beta = \tilde{\beta}\) but still worth noting.

\newthought{Solving} for sub-vectors of \(\beta\) is a really useful exercise, so we'll go over that now.\footnote{It's worth noting that nothing is based on sample data right now. Later we'll look at the sample analog of the solution to \(\beta\) and of the solution for sub-vectors of \(\beta\).} Write:
	\[Y = X_1'\beta_1 + X_2'\beta_2 + U\]
and our expression for \(\beta\) becomes:
	\[\beta = \left(\begin{array}{c} \beta_1 \\ \beta_2\end{array}\right) = \left(\begin{array}{cc} E[X_1X_1'] & E[X_1X_2'] \\ E[X_2X_1'] & E[X_2X_2']\end{array}\right)^{-1} \left(\begin{array}{c}E[X_1Y] \\ E[X_2Y]\end{array}\right)\]
and our goal is to get an expression for \(\beta_1\). One approach would be to use the partition matrix inverse formula, but that's messy and not illuminating.\footnote{Thank god!} Instead we'll define the following notation for two random vectors, \(A, B\):\footnote{\(BLP(A|B)\) is \(B'\delta\) where \(\delta\) is from the minimization of:
	\[E[(A - B'\delta)^2]\]
	where for \(W = A - B'\delta\) the F.O.C. gives us \(E[BW] = 0\) for free.}
	\[BLP(A| B) = \text{ Best Linear Predictior of \(A\) given \(B\).}\]
Then call:
	\[\tilde{Y} = Y - BLP(Y|X_2) = Y - X_2' \gamma\]
	\[\tilde{X}_1 = X_1 - BLP(X_1 | X_2) = X_1 - X_2'\delta\]
and consider:
	\[\tilde{Y} = \tilde{X}_1'\tilde{\beta}_1 + \tilde{U}\ \text{ with } E[\tilde{X}_1 \tilde{U}] = 0\ \text{ as in Interp. \#2}\]
and we claim:
	\[\tilde{\beta}_1 = \beta_1 \text{.}\]
\emph{Why?} Well let's get our Q.E.D. on.
\begin{proof} Starting with \(\tilde{\beta}_1\) we get:
	\begin{align*}
		\tilde{\beta}_1 & = E[\tilde{X}_1\tilde{X}_1']^{-1}E[\tilde{X}_1\tilde{Y}]\\
					& = E[\tilde{X}_1\tilde{X}_1']^{-1}\left(E[\tilde{X}_1Y] - E[\tilde{X}_1 BLP(Y|X_2)]\right)\\
					& = E[\tilde{X}_1\tilde{X}_1']^{-1}E[\tilde{X}_1Y]\ \text{ b/c }\ E[\tilde{X}_1BLP(Y|X_2)] = E[\tilde{X}_1 X_2'\gamma] = E[(X_1 - X_2'\delta)X_2']\gamma = E[(X_2 U)']\gamma = 0 \\
					& = E[\tilde{X}_1 \tilde{X}_1']^{-1} E[\tilde{X}_1(X_1' \beta_1 + X_2'\beta_2 + U)] \\
					& = E[\tilde{X}_1\tilde{X}_1']^{-1}E[\tilde{X}_1 X_1']\beta_1\ \text{ b/c }\ U\ \text{ is orthogonal to both } X_1, X_2 \\
					& = E[\tilde{X}_1\tilde{X}_1']^{-1}E[\tilde{X}_1(\tilde{X}_1 + BLP(X_1|X_2))']\beta_1 \\
					& = E[\tilde{X}_1\tilde{X}_1']^{-1}\left(E[\tilde{X}_1\tilde{X}_1]\beta_1 + E[\tilde{X}_1BLP(X_1|X_2))']\right)\beta_1 \\
					& = E[\tilde{X}_1\tilde{X}_1']^{-1}E[\tilde{X}_1\tilde{X}_1]\beta_1\ \text{ b/c of the same orthogonality condition used above} \\
					& = \beta_1 \text{.}
	\end{align*}
	which was our claim.\footnote{It's worth pointing out that we're basically just using the same orthogonality condition over and over and over again here.}
\end{proof}
And what's so cool about this result is that it gives meaning to the phrase, \(\beta_1\) is the effect of \(X_1\) on \(Y\) after controlling for \(X_2\).\footnote{Residual regression, dawg.}\footnote{Could have used \(Y\) instead of \(\tilde{Y}\) and we'd get the same result.} Can also apply result with \(X_2 = 1\) with \(\tilde{X}_1 = X_1 - E[X_1]\) and \(\tilde{Y} = Y - E[Y]\) would give us:
	\[\beta_1 = E[(X_1 - E[X_1])(X_1 - E[X_1])']^{-1} E[(X_1 - E[X_1])(Y-E[Y])] = Var(X_1)^{-1}Cov(X_1, Y)\]
which is super cool.

\section{Omitted variable bias}
\newthought{Suppose}:
	\[Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + U\]
with \(E[X_1 U] = E[X_2U] = E[U] = 0\) (all scalars) and consider: 
	\[Y^* = \beta^*_0 + \beta^*_1 X_1 + U^*\]
with the same moment restrictions. In general \(\beta_1 \ne \beta_1^*\). Why?
	\begin{align*}
		\beta_1^* & = \frac{Cov(X_1, Y)}{Var(X_1)} \\
				& = \frac{Cov(X_1, \beta_0 + \beta_1X_1 + \beta_2 X_2 + U)}{Var(X_1)} \\
				& = \beta_1 + \underbrace{\beta_2 \frac{Cov(X_1, X_2)}{Var(X_1)}}_\textrm{The Bias}
	\end{align*}
where The Bias can be pretty substantial if \(X_1\) and \(X_2\) covary. For example, if you're looking at drownings and ice-cream sales and omit weather, the covariance of weather and ice-cream sales is going to be substantial and really mislead your interpretation of \(\beta_1^*\). Same thing is going on with police and crime in the figure.\begin{marginfigure} \includegraphics{ovbias.png} \caption{Figure 1 from Levitt (1997). Are police causing more crime to occur?} \end{marginfigure}

If we do the same exercise with vectors then we start with:
	\[Y = \beta_0 + X_1'\beta_1 + X_2'\beta_2 + U\]
and consider:
	\[Y = \beta_0^* + X_1'\beta_1^* + U^*\]
then:
	\begin{align*}
		\beta_1^* & = Var(X_1)^{-1}Cov(X_1, Y) \\
				& = \beta_1 + Var(X_1)^{-1}Cov(X_1, X_2)\beta_2
	\end{align*}
which is a less straight-forward expression. Also, it's generally what we're thinking about in a regression and can differ from the scalar intuition above.

\section{Measurement error}
\newthought{Consider} the follow scalar model:
	\[Y = \beta_0 + \beta_1X_1 + U\]
with the standard moment restrictions. Then instead of observing \(X_1\) we actually observe \(\hat{X}_1 = X_1 + V\) with \(E[V] = 0,\ Cov(X_1, V) = 0,\) and \(Cov(U, V) = 0\).\footnote{The classical error-in variables model. Classic!} Then how does \(\beta_1^*\) compare to \(\beta_1\)?
	\[Y = \beta_0^* + \beta_1^* \hat{X}_1 + U^*\]
with it's own moment restrictions. Then:
	\begin{align*}
		\beta_1^* & = \frac{Cov(\hat{X}_1, Y)}{Var(\hat{X}_1)} \\
				& = \beta_1 \frac{Cov(\hat{X}_1, X_1)}{Var(\hat{X}_1)} + \frac{Cov(\hat{X}_1, U)}{Var(\hat{X}_1)}\\
				& = \beta_1 \frac{Cov(\hat{X}_1, X_1)}{Var(\hat{X}_1)}\ \text{ b/c } Cov(\hat{X}_1, U) = Cov(X_1, U) + Cov(V, U) = 0\\
				& = \beta_1 \underbrace{\frac{Var(X_1)}{Var(X_1) + Var(V)}}_\textrm{Signal-to-noise ratio}
	\end{align*}	\begin{marginfigure} \includegraphics{signal_noise.jpg} \caption{Nate Silver possibly misinterpreting what a signal-to-noise ratio musses up.} \end{marginfigure}
\noindent where the signal-to-noise ratio will be less than 1 and attenuate the observed value of \(\beta_1\), \(\beta_1^*\).

More generally if we move out of the scalar case with:
	\[Y = \beta_0 + X_1'\beta_1 + U\]
and:
	\[Y = \beta_0^* + \hat{X}_1'\beta_1^* + U^*\]
where:
	\[\hat{X}_1 = X_1 + V \text{.}\]
Then:
	\[\beta_1^* = (\underbrace{Var(X_1) + Var(V)}_\textrm{Nightmare!})^{-1}Var(X_1)\beta_1 \text{.}\]

But we aren't always screwed if we're in search of intuition because we can use our residual regression approach from above. I.e., think of:
	\[Y = \beta_0 + \beta_1 X_1 + X_2'\beta_2 + U\]
with the usual assumptions but with \(\hat{X}_1 = X_1 + V\) and \(E[V] = 0,\ Cov(X_1, V) = 0,\ Cov(U, V) = 0,\ \text{ and } Cov(X_2, V) = 0\) and we run the following regressions as in Interp. \#2:
	\[Y = \beta_0^* + \beta_1^* \hat{X}_1 + X_2'\beta_2^* + U^*\]
and define:
	\begin{align*}
		\tilde{\hat{X}}_1 & = \hat{X}_1 - BLP(\hat{X}_1 | X_2) \\
					& = X_1 + V + BLP(X_1|X_2)\\
					& = \tilde{X}_1 + V\ \text{ where } \tilde{X}_1 = X_1 - BLP(X_1|X_2)
	\end{align*} 
and we can then think of our estimator of interest as:\footnote{Where: \[Var(\tilde{\hat{X}}_1) = Var(\tilde{X}_1 + V) = Var(\tilde{X}_1) + Var(V)\] 
	because we've assumed they're uncorrelated and:
		\begin{align*}
			Cov(\tilde{\hat{X}}_1, Y) &= Cov(\tilde{X}_1, X_1)\beta_1\\
			&= Cov(\tilde{X}_1, \tilde{X}_1 + BLP(X_1|X_2))\beta_1 \\
			&= Var(\tilde{X}_1)\beta_1 \text{.}
		\end{align*}
	}
	\[\beta_1^* = \frac{Cov(\tilde{\hat{X}}_1, Y)}{Var(\tilde{\hat{X}}_1)} = \beta_1\underbrace{\frac{Var(\tilde{X}_1)}{Var(\tilde{X}_1) + Var(V)}}_\textrm{Signal-to-noise ratio again} \text{.}\]
This is the classic result of attenuation bias for \(\beta_1\) in the presence of measurement error.\footnote{Bias on \(\beta_2\) is hard to analyze. Typically people will say that everything gets attenuated, but that's not neccesarily true.}

\section{Estimating \(\beta\)}
\newthought{Suppose} \((Y, X, U)\) satisfies:
	\[Y = X'\beta + U\]
with:
	\[E[XU] = 0\ \text{ and } E[XX'] < \infty\]
with no perfect collinearity. This implies that:
	\[\beta = E[XX']^{-1}E[XY] \text{.}\]
Suppose we also have \((Y_1, X_1), \dots, (Y_n, X_n)\), an i.i.d. sample from distribution of \((Y, X)\). Then an analog estimator for \(\beta\) would be:
	\[\hat{\beta}_n := \left(\frac{1}{n} \sum_{i = 1}^n X_iX_i'\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n X_iY_i\right)\]
which we call the ordinary least squares (OLS) estimator.\footnote{Why do we call it that? Because \(\hat{\beta}_n\) minimizes:
	\[\frac{1}{n} \sum_{i=1}^n (Y_i - X_i'b)^2\]
and the first order condition of this minimization problem gives us a \(\hat{\beta}_n\) that satisfies:
	\[\frac{1}{n} \sum_{i =1}^n X_i(Y_i - X_i'\hat{\beta}_n) = 0\]
which we can write as:
	\[\left(\frac{1}{n}\sum_{i=1}^n X_i X_i'\right) \hat{\beta}_n = \frac{1}{n}\sum_{i = 1}^n X_i Y_i\]
where the LHS will be invertible with high probability for large \(n\).} Some popular terminology for OLS is: \begin{description}
	\item \(\hat{Y}_i := X_i'\hat{\beta}_n =: i^{th}\) fitted value.
	\item \(\hat{U}_i := Y_i - \hat{Y}_i = Y_i - X_i'\hat{\beta}_n =: i^{th}\) residual.
\end{description} And two useful properties of these values are: \begin{enumerate}
	\item We can move between fitted values and the observed outcome with the following relationship:
		\[Y_i = \hat{Y}_i + \hat{U}_i \text{.}\] 
	\item The sum of the product of the residuals and covariates is 0:
		\[\frac{1}{n} \sum_{i=1}^n X_i\hat{U}_i = 0 \text{.}\]
\end{enumerate}

\section{Projection interpretation of OLS}
\newthought{For} \((Y_1, X_1), \dots, (Y_n, X_n)\), an i.i.d. sample from distribution of \((Y, X)\) with \(Y \in \mathbb{R}\) and \(X \in \mathbb{R}^k\), define:\begin{marginfigure} \includegraphics{frisch.jpg} \caption{Ragnar Frisch of Frisch-Waugh-Lowell and Frisch labor supply elasticity fame.} \end{marginfigure}
	\begin{align*}
		\mathbb{Y} &= (Y_1, \dots, Y_n)' \\
		\mathbb{X} &= (X_1, \dots, X_n)' \\
		\mathbb{U} &= (U_1, \dots, U_n)' \\
		\hat{\mathbb{Y}} &= (\hat{Y_1}, \dots, \hat{Y_n})' = \mathbb{X}\hat{\beta}_n \\
		\hat{\mathbb{U}} & = (\hat{U}_1, \dots, \hat{U}_n)' = \mathbb{Y} - \hat{\mathbb{Y}}
	\end{align*}
Then in this notation:
	\[\hat{\beta}_n = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}' \mathbb{Y}\]
and \(\hat{\beta}_n\) solves:
	\[\min_b |\mathbb{Y} - \mathbb{X}b|^2\]
where \(\mathbb{X}b\) is a vector in \(col(\mathbb{X})\).\footnote{I.e., all vectors \(\mathbb{X}b\).} One nice interpretation of the solution to this representation is that \(\mathbb{X}\hat{\beta}_n\) is the vector in the column space of \(\mathbb{X}\) that's orthogonal to the project of \(\mathbb{Y}\) onto the column space of \(\mathbb{X}\). That is, \(\mathbb{X}\hat{\beta}_n\) is the vector in the column space that's closest to \(\mathbb{Y}\). We can think of this mathematically by:
	\[\mathbb{X}\hat{\beta}_n = \underbrace{\mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}}_\mathrm{:=\mathbb{P}}\mathbb{Y} = \mathbb{P}\mathbb{Y}\]
where \(\mathbb{P}\) is the matrix that projects an \(n\) dimensional vector onto the space \(col(\mathbb{X})\). The projection matrix is nicely well-behaved\footnote{That is, \(\mathbb{P}\) is symmetric and \(\mathbb{P}^2 = \mathbb{P}\).} and we can use that behavior to develop some tools to think about the sample/projection equivalent of solving for sub-vectors of \(\beta\). Call:
	\[\mathbb{M} := \mathbb{I} - \mathbb{P}\]
the ``residual maker'' matrix which is also a projection matrix and is orthogonal to \(col(\mathbb{X})\). Then with just a little bit of algebra we get:\footnote{If you're curious, the algebra is just: \[\mathbb{M}\mathbb{Y} = \mathbb{Y} - \mathbb{P}\mathbb{Y} = \mathbb{Y} - \mathbb{X}\hat{\beta}_n \text{.}\]}
	\[\mathbb{M}\mathbb{Y} = \hat{\mathbb{U}} \text{.}\]

\newthought{Then} to estimate subvectors of \(\beta\) we try to estimate: 
	\[Y = X_1'\beta_1 + X_2'\beta_2 + U\]
where:
	\[(Y_1, X_{1,1}, X_{2,1}), \dots, (Y_n, X_{1,n}, \dots, X_{2,n})\]
are i.i.d. and we want to derive an expression for \(\hat{\beta}_{1,n}\) by itself. Then define:
	\[\mathbb{P}_1 = \text{ projection matrix onto }\ col(\mathbb{X}_1)\]
giving us: \(\mathbb{M}_1 = \mathbb{I} - \mathbb{P}_1\). Define \(\mathbb{M}_2, \mathbb{P}_2\) equivalently. Then if we note that:
	\[\mathbb{Y} = \mathbb{X}_1'\hat{\beta}_{1,n} + \mathbb{X}_2'\hat{\beta}_{2,n} + \hat{\mathbb{U}}\]
we can pre-multiply by \(\mathbb{M}_2\) to get:
	\[\mathbb{M}_2 \mathbb{Y} = \mathbb{M}_2\mathbb{X}_1 \hat{\beta}_{1,n} + \underbrace{\mathbb{M}_2\mathbb{X}_2 \hat{\beta}_{2,n}}_\textrm{\(=0\)} + \underbrace{\mathbb{M}_2\hat{\mathbb{U}}}_\textrm{\(= \hat{\mathbb{U}}\)}\]
and if we pre-multiply again by \((\mathbb{M}_2\mathbb{X}_1)'\) then we get:
	\[(\mathbb{M}_2\mathbb{X}_1)'\mathbb{M}_2 \mathbb{Y} = (\mathbb{M}_2\mathbb{X}_1)'\mathbb{M}_2\mathbb{X}_1\hat{\beta}_{1,n} + \underbrace{(\mathbb{M}_2\mathbb{X}_1)'\hat{\mathbb{U}}}_\textrm{\( = \mathbb{X}_1'\hat{\mathbb{U}} = 0\)}\]
and \( (\mathbb{M}_2\mathbb{X}_1)'\mathbb{M}_2\mathbb{X}_1\) is invertible if \(\mathbb{X}'\mathbb{X}\) is invertible. Then we get the following nice expression for \(\hat{\beta}_{1,n}\):
	\[\hat{\beta}_{1,n} = \left[(\mathbb{M}_2\mathbb{X}_1)'(\mathbb{M}_2\mathbb{X}_1)\right]^{-1}[(\mathbb{M}_2\mathbb{X}_1)'(\mathbb{M}_2\mathbb{Y})]\]
which is the Frisch-Waugh-Lowell estimator and we can think of \((\mathbb{M}_2\mathbb{X}_1)'\) as the residuals for a regression of \(X_1\) onto \(X_2\). 

\section{Measures of fit}
\newthought{A popular} number to report in the estimation of a regression is \(R^2\), which is merely a measure of how well the model you estimate fits the data. We write:
	\[R^2 := \frac{ESS}{TSS} = 1- \frac{SSR}{TSS}\]
where:\footnote{ESS stands for explained sum of squares. TSS stands for total sum of squares and SSR is an acronym for sum of squared residuals. As you can see in the definitions, they're all exactly what they sound like they'd be.}
	\begin{align*}
		ESS := & \sum_{i=1}^n(\hat{Y}_i - \bar{Y}_n)^2 \\
		TSS := & \sum_{i=1}^n(Y_i - \bar{Y}_n)^2 \\
		SSR := & \sum_{i=1}^n \hat{U}_i^2  \text{.}
	\end{align*}
\begin{lem} \(TSS = ESS + SSR\).
\begin{proof}
	To prove this, we'll just bust out some algebra:
		\begin{align*}
			TSS & = \sum_{i=1}^n(Y_i - \bar{Y}_n)^2 \\
				& = \sum_{i=1}^n(Y_i - \hat{Y}_i + \hat{Y}_i - \bar{Y}_n)^2 \\
				& = \sum_{i=1}^n(\hat{U}_i + \hat{Y}_i - \bar{Y}_n)^2 \\
				& = \sum_{i=1}^n\hat{U}_i^2 + 2\sum_{i=1}^n \hat{U}_i(\hat{Y}_i - \bar{Y}_n) + \sum_{i=1}^\mathcal{N}(\hat{Y}_i - \bar{Y}_n)^2 \\
				& = SSR + ESS + 2\sum_{i=1}^n\hat{U}_i(\hat{Y}_i - \bar{Y}_n) \\
				& = SSR + ESS + 2\left(\sum_{i=1}^n\hat{U}_i\hat{Y}_i - \sum_{i=1}^n\hat{U}_i \bar{Y}_n\right) \\
				& = SSR + ESS + 2\underbrace{\left(\sum_{i=1}^n\hat{U}_iX_i'\right)}_\mathrm{=0}\hat{\beta}_n - 2\left(\sum_{i=1}^n\hat{U}_i\right)\bar{Y}_n \\
				& = SSR + ESS - 2\bar{Y}_n\sum_{i=1}^n\hat{U}_i \\
				& = SSR + ESS\ \text{ b/c }\ \sum_{i=1}^n \hat{U}_i = 0\ \text{ if constant in \(X\).}
		\end{align*}
\end{proof}
\end{lem}
\noindent This also establishes that: \begin{enumerate}
	\item \(0 \le R^2 \le 1\)
	\item \(R^2 = 1 \implies \hat{U}_i = 0\ \forall i\)
	\item \(R^2 = 0 \implies \hat{Y}_i=\bar{Y}_n\ \forall i\)
\end{enumerate}
Also \(R^2\) is monotone in regressions.\footnote{It always goes up if you add a regressor. \emph{Azeem suggests we try to prove this.}} Also, we can represent \(R^2\) as:
	\[R^2 = 1 - \frac{SSR}{TSS} = 1 - \frac{\frac{1}{n}\sum_i^n \hat{u}_i^2}{\frac{1}{n}\sum_{i=1}^\mathcal{N}(Y_i - \bar{Y}_n)^2}\]
which we can view as the sample analog of:
	\[1 - \frac{Var(U)}{Var(Y)} \text{.}\]

The sense in which \(R^2\) is monotone in regressors suggests a degree of freedom adjustment:
	\[\bar{R}^2 := 1 - \frac{n-1}{n-k-1}\frac{SSR}{TSS}\]
which establishes that \(\bar{R}^2 \le R^2 \le 1\) but we could also get \(\bar{R}^2 < 0\). Also \(\bar{R}^2\) is not monotone in regressors. We call \(\bar{R}^2\) the adjusted \(R^2\). Depending on what you're going for, \(R^2\) or \(\bar{R}^2\) isn't that useful. A low or high \(R^2\) doesn't tell us anything about the validity of a causal interpretation (Interp. 3).

\section{Properties of OLS estimator}
\newthought{We'll} now go back to the scalar setup with the following assumptions:
	\[Y = X'\beta + U,\ E[XU] = 0,\ E[XX'] < \infty\]
and no perfect colinearity in \(X\) with \((Y_1, X_1), \dots, (Y_n, X_n)\) is an i.i.d. sample from distribution of \((Y,X)\). \begin{marginfigure} \includegraphics{gauss.jpg} \caption{I held back as long as possible. Without further adieu, ladies and gentleman, the main attraction: Carl Friedrich Gauss! ::crowd noises::} \end{marginfigure}

\subsection{Bias} A desirable property of an estimator is unbiasedness.\footnote{A small sample property.} If we assume further that \(E[U|X] = 0\) then \(E[\hat{\beta}_n] = \beta\). Why? 
	\[\hat{\beta}_n = (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbb{Y} = \beta + (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbb{U}\]
then if we take expectations:
	\[E[\hat{\beta}_n|X_1, \dots, X_n] = \beta + (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\underbrace{E[\mathbb{U}|X_1, \dots, X_n]}_\textrm{\(=0\)}\]
giving us unbiasedness. But unbiasedness doesn't come for free. We need the conditional expectation of the error term to be 0.\footnote{\(E[\mathbb{A}'\mathbb{Y}|X_1, \dots, X_n] = \mathbb{A}'\mathbb{X}\beta + \mathbb{A}'E[\mathbb{U}|X_1, \dots, X_n] = \mathbb{A}'\mathbb{X}\beta\). The unbiasedness condition, then, is equivalent to requiring:
	\[\mathbb{A}'\mathbb{X} = \mathbb{I} \text{.}\]}

\subsection{Efficiency} Another desirable property of an estimator would be that it's the most efficient. The following theorem helps formalize that thinking for OLS.
\begin{thm}[Gauss-Markov Theorem] This is another finite sample property. If we assume \(E[U|X] = 0\) and homoskedasticity\footnote{I.e., \(Var(U|X) = \sigma^2\).} and we restrict attention to estimators of \(\beta\) of the form \(\mathbb{A}'\mathbb{Y}\) for:
	\[\mathbb{A} := \mathbb{A}(X_1, \dots, X_n)\]
the class of conditionally linear functions of \(Y\) on \(X_1, \dots, X_n\), and:
	\[E[\mathbb{A}'\mathbb{Y} | X_1, \dots, X_n] = \beta\]
which are just two properties we might demand in an estimator.\footnote{Clearly OLS satisfies these conditions.} Then among this class, the ``best'' estimator is OLS. By best we mean that:
	\[Var(\mathbb{A}'\mathbb{Y}|X_1, \dots, X_n)\]
is minimized. I.e., partial order given by \(B \le \tilde{B}\) if \(\tilde{B} - B\) is positive semi-definite.\footnote{\(Var(\mathbb{A}'\mathbb{Y}|X_1, \dots, X_n) = Var(\mathbb{A}'\mathbb{U}|X_1, \dots, X_n) = \mathbb{A}'Var(\mathbb{U}|X_1, \dots, X_n)\mathbb{A} = \sigma^2\mathbb{A}'\mathbb{A}\) where the last step follows from homoskedasticity assumption.}
	\begin{proof} OLS choice of \(\mathbb{A}\) gives us:
			\[Var(\mathbb{A}'\mathbb{Y}|X_1, \dots, X_n) = \sigma^2(\mathbb{X}'\mathbb{X})^{-1}\]
		by plugging in the OLS \(\mathbb{A}\). Then we need to show for any \(\mathbb{A}\) s.t. \(\mathbb{A}'\mathbb{X} = \mathbb{I}\) that:
			\[\mathbb{A}'\mathbb{A} - (\mathbb{X}'\mathbb{X})^{-1}\]
		is positive semi-definite. Define:
			\[\mathbb{C} := \mathbb{A} - \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}\]
		and then:
			\begin{align*}
				\mathbb{A}'\mathbb{A} - (\mathbb{X}'\mathbb{X})^{-1} & = [\mathbb{C} + \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}]' [\mathbb{C} + \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}] - (\mathbb{X}'\mathbb{X})^{-1} \\
															& = [\mathbb{C}' + (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'][\mathbb{C} + \mathbb{X}(\mathbb{X}'\mathbb{X})^{-1}] - (\mathbb{X}'\mathbb{X})^{-1} \\
															& = \mathbb{C}'\mathbb{C} + \mathbb{C}\mathbb{X}(\mathbb{X}'\mathbb{X})^{-1} + (\mathbb{X}'\mathbb{X})^{-1}\mathbb{X}'\mathbb{C} \\
															& = \mathbb{C}'\mathbb{C} + \mathbb{C}' + \mathbb{X}'\mathbb{C}
			\end{align*}
		where the last step establishes positive semi-definiteness. Note: We're using that implication of unbiasedness in these steps. We might not be obsessed with unbiasedness. Still, it's small sample, dawg. Deal with it.
	\end{proof}
\end{thm}

\subsection{Consistency}
Our OLS moment restriction, \(E[XU] = 0\), tells us that \(E[XY]\) exists because:
	\[E[XY] = E[XX']\beta + E[XU]\]
and those subcomponents all exist. Then:
	\[\hat{\beta}_n = \underbrace{\left(\frac{1}{n}\sum_{i=1}^n X_iXi'\right)^{-1}}_\textrm{\(\overset{p}{\rightarrow} E[XX']^{-1}\)} \underbrace{\left(\frac{1}{n}\sum_{i=1}^nX_iY_i\right)}_\textrm{\(\overset{p}{\rightarrow} E[XY]\)}\]
which we can apply CMT1 to in order to get:
	\[\hat{\beta}_n \overset{p}{\rightarrow} E[XX']^{-1}E[XY] = \beta\]
which gives us consistency. 

\subsection{Limiting distribution of \(\hat{\beta}_n\)}
If we assume further that \(Var(XU)\) exists, then:
	\[Var(XU) = E[(XU - E[XU])(XU - E[XU])'] = E[XX'U^2]\]
and then:
	\[\sqrt{n}(\hat{\beta}_n - \beta) \overset{d}{\rightarrow} \mathcal{N}(0, \Omega)\]
where:
	\[\Omega = E[XX']^{-1}Var(XU)E[XX']^{-1} \text{.}\]
Why? Let's bust out our proof shoes:
\begin{proof} Just working with \(\hat{\beta}_n\) we get:
	\begin{align*}
		\hat{\beta}_n & = \left(\frac{1}{n}\sum_{i=1}^n X_iXi'\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^nX_iY_i\right) \\
					& = \beta + \left(\frac{1}{n}\sum_{i=1}^n X_iXi'\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^nX_iU_i\right)
	\end{align*}
and then:
	\[\sqrt{n}(\hat{\beta}_n - \beta) = \underbrace{\left(\frac{1}{n}\sum_{i=1}^n X_iXi'\right)^{-1}}_\textrm{\(\overset{p}{\rightarrow} E[XX']^{-1}\)}\underbrace{\left(\frac{1}{\sqrt{n}}\sum_{i=1}^nX_iU_i\right)}_\textrm{\(\overset{d}{\rightarrow} \mathcal{N}(0, Var(XU))\)}\]
which we can apply Slutsky's Lem. to in order to get:
	\[\sqrt{n}(\hat{\beta}_n - \beta) \overset{d}{\rightarrow} E[XX']^{-1}\mathcal{N}(0, Var(XU)) = \mathcal{N}(0, \Omega)\text{.}\]
\end{proof}

\subsection{Consistent estimation of \(\Omega\)}
Here we'll work with the same assumptions that we had for deriving the limiting distribution. First we'll assume homoskedasticity.\footnote{I.e., \(E[U|X] = 0\) and \(Var(U|X) = \sigma^2\).} Then:
	\[Var(XU) = E[XX'U^2] = E[XX'E[U^2|X]] = \sigma^2E[XX']\]
allows us to write:
	\[\Omega = E[XX']^{-1}\sigma^2\]
which we can estimate with:
	\[\hat{\Omega} = \left(\frac{1}{n} \sum_{i=1}^n X_iX_i'\right)^{-1}\left(\frac{1}{n}\sum_{i=1}^n \hat{U}_i^2\right) =  \left(\frac{1}{n} \sum_{i=1}^n X_iX_i'\right)^{-1}\hat{\sigma}^2_n \text{.}\]
But is this bad-boy consistent? Let's look at it piece-by-piece:
	\[\hat{\sigma}_n^2 = \frac{1}{n}\sum_{i=1}^n \hat{U}_i^2 = \underbrace{\frac{1}{n}\sum_{i=1}^n U_i^2}_\textrm{\(\overset{p}{\rightarrow} E[U^2]\)} + \underbrace{\frac{1}{n}\sum_{i=1}^n (\hat{U}_i^2 - U_i^2)}_\textrm{Hmm}\]
where \(E[U^2] = E[U^2|X] = \sigma^2\). What about that Hmmm term, though? Well note:
	\[\hat{U}_i = Y_i - X_i'\hat{\beta}_n = Y_i - X_i'\beta - X_i'(\hat{\beta}_n - \beta) = U_i - X_i'(\hat{\beta}_n - \beta)\]
which tells us that:
	\[\hat{U}_i^2 - U_i^2 = -2U_iX_i'(\hat{\beta}_n - \beta) + (X_i'(\hat{\beta}_n - \beta))^2\]
so our Hmmmm term then becomes:
	\[\frac{1}{n}\sum_{i=1}^n \hat{U}_i^2 = \underbrace{-2\frac{1}{n}\sum_{i=1}^n U_iX_i'(\hat{\beta}_n - \beta)}_\textrm{A} + \underbrace{\frac{1}{n}\sum_{i=1}^\mathcal{N}(X_i'(\hat{\beta}_n - \beta))^2}_\textrm{B}\]
and:
	\[\textrm{A} = -2\left[\frac{1}{n}\sum_{i=1}^n U_iX_i'\right](\hat{\beta}_n - \beta) = -2\left[\frac{1}{n}\sum_{i=1}^n U_iX_i'\right]o_p(1)\]
and:
	\[\textrm{B} \le \frac{1}{n} \sum_{i=1}^n |X_i'(\hat{\beta}_n - \beta)|^2 \le \frac{1}{n} \sum_{i=1}^n |X_i|^2 |\hat{\beta}_n - \beta|^2 \overset{p}{\rightarrow} 0\]
and then we've established that our estimator is consistent because all of Hmmmmmm is going to 0 in probability.

\newthought{Homoskedasticity} is more or less a dumb assumption, though. We'll scrap that now and do some real work to show consistency. Now \(\Omega\) becomes:
	\[\Omega = E[XX']^{-1}Var(XU)E[XX']^{-1}\]
and to estimate \(Var(XU) = E[XX'U^2]\) we'll use:
	\[\frac{1}{n}\sum_{i=1}^n X_iX_i'\hat{U}_i^2 = \underbrace{\frac{1}{n}\sum_{i=1}^n X_iX_i' U_i^2}_\textrm{\(\overset{p}{\rightarrow} E[XX'U^2]\)} + \underbrace{\frac{1}{n}\sum_{i=1}^\mathcal{N}(\hat{U}_i^2 - U_i^2)}_\textrm{Ugh}\]
and look at the \((j,l)^{th}\) element of Ugh. I.e.,
	\[\lvert \frac{1}{n} \sum_{i=1}^n X_{i,j} X_{i,l}(\hat{U}^2_i - U_i^2)\rvert \le \frac{1}{n}\sum_{i=1}^n |X_{i,j}X_{i,l}|\ |\hat{U}^2_i - U_i^2| \le \underbrace{\frac{1}{n}\sum_{i=1}^n |X_{i,j}X_{i,l}|}_\textrm{\(\overset{p}{\rightarrow} E[|X_jX_l|]\)} \underbrace{\max_{1 \le m \le n} |\hat{U}^2_m - U_m^2|}_\textrm{\(o_p(1)?\)}\]
where we know that \(E[|X_j X_l|] \) is finite by a component-by-component application of \(E[XX']'s\) finiteness. But we still need to show that:
	\[\max_{1 \le m \le n} |\hat{U}^2_m - U^2_m| \overset{p}{\rightarrow} 0 \text{.}\]
We'll start with a Lemma that'll help us deal with that claim.
\begin{lem} Let \(Z_1, \dots, Z_n\) be an i.i.d. sequence of random variables s.t. \(E[|Z_i|^r] < \infty\) then:
	\[n^{-1/r} \max_{1 \le i \le n} |Z_i| \overset{p}{\rightarrow} 0\]
or, equivalently:
	\[\max_{1 \le i \le n} |Z_i| = o_p\left(n^{1/r}\right)\text{.}\]
	\begin{proof} Let \(\epsilon > 0\) be given. Then note:\marginnote{\begin{thm}[Markov's Inequality\(+\)] For any random variable \(W\):
		\[P\{|W| > \epsilon \} \le \frac{1}{\epsilon} E[|W| I\{|W| > \epsilon\}] \text{.}\]
		\begin{proof} If \(I\{|W| > \epsilon\} \le \frac{|W|}{\epsilon}I\{|W| > \epsilon\}\) then I can take expectations and get the result of Markov's Inequality\(+\). \end{proof} \end{thm}}
		\begin{align*}
			P\left\{n^{-1/r}\max_{1 \le i \le n} |Z_i| > \epsilon \right\} & = P\left\{\max_{1 \le i \le n} |Z_i| > \epsilon n^{1/r} \right\} \\
														& = P\left\{\cup_{1 \le i \le n} \{|Z_i|^r > \epsilon^r n\}\right\} \\
														& \le \sum_{1 \le i \le n} P\{|Z_i|^r > \epsilon^r n\}\ \text{ by Boole/Bonferonni }\\
														& \le \sum_{i = 1}^n \frac{1}{\epsilon^r n} E[|Z_i|^r I\{|Z_i|^r > \epsilon^r n\}]\ \text{ by Markov's Ineq.\(+\)} \\
														& = \frac{1}{\epsilon^r} E[|Z_i|^r I\{|Z_i|^r > \epsilon^r n\}] \\
														& \rightarrow 0\ \text{ by Dom. Conv. Thm. and assumption that } E[|Z_i|^r] < \infty \text{.}
		\end{align*}
	\end{proof}
\end{lem}
\noindent So returning to our Ughhh term, we can apply the Lem. to get that:
	\[\max_{1 \le m \le n} |\hat{U}_m^2 - U^2_m| \le 2 \underbrace{\sqrt{n} |\hat{\beta}_n - \beta|}_\textrm{\(=O_p(1)\)} \underbrace{n^{-1/2} \max_{1 \le m \le n} |U_m||X_m|}_\textrm{\(=o_p(1)\)} + \underbrace{n|\hat{\beta}_n - \beta|^2}_\textrm{\(=O_p(1)\)} \underbrace{\frac{1}{n}\max_{1\le m \le n}|X_m|^2}_\textrm{\(=o_p(1)\)}\]
because \(|\hat{U}_m^2 - U^2_m| \le 2 |U_m||X_m||\hat{\beta}_n - \beta| + |X_m|^2|\hat{\beta}_n - \beta|^2\).\footnote{This follow from the observation that:
	\[\hat{U}^2_m = U^2_m - 2U_{i,m}X_{i,m}'(\hat{\beta}_n - \beta) + (X_{i,m}(\hat{\beta}_n - \beta))^2\text{.}\]}
Also, worth noting that the first \(o_p(1)\) term follows if \(E[|UX|^2] < \infty\) which follows from \(E[XX'U^2] < \infty\) and the second \(o_p(1)\) term follows if \(E[|X|^2] < \infty\) which follows from \(E[XX'] < \infty\). 

Then we've derived our desired result and proven that our estimator for \(\Omega\), \(\hat{\Omega}_n\) is consistent. Sweeet. 

\section{Inference}
\newthought{Suppose} we're interested in testing a single linear restriction:
	\[H_0: r'\beta = c\ \text{ vs. } H_A: r'\beta \ne c\]
where \(r\) is a \((k+1)\) by \(1\) vector that's not equal to 0. Then we know that:
	\[\sqrt{n}(\hat{\beta}_n - \beta) \overset{d}{\rightarrow} \mathcal{N}(0, \Omega)\]
and that \(\hat{\Omega}_n \overset{p}{\rightarrow} \Omega\), so:
	\[r'[\sqrt{n}(\hat{\beta}_n - \beta)] = \sqrt{n}(r'\hat{\beta}_n - r'\beta) \overset{d}{\rightarrow} r'\mathcal{N}(0, \Omega) = \mathcal{N}(0, r'\Omega r)\]
and that:
	\[r'\hat{\Omega}_n r \overset{p}{\rightarrow} r'\Omega r > 0\]
which all implies that:
	\[\frac{\sqrt{n}(r'\hat{\beta}_n - r'\beta)}{\sqrt{r'\hat{\Omega}_n r}} \overset{d}{\rightarrow} \mathcal{N}(0,1)\]
so a natural test statistic is \(|T_n|\) where:
	\[T_n = \frac{\sqrt{n}(r'\hat{\beta}_n - c)}{\sqrt{r'\hat{\Omega}_n r}}\]
with critical value:
	\[z_{1-\alpha/2} = \Phi(1-\alpha/2) \text{.}\]
Then when the null is true:
	\[P\{|T_n| > z_{1-\alpha/2}\} = 1 - P\{-z_{1-\alpha/2} \le T_n \le z_{1-\alpha/2}\} = 1-  P\{-z_{\alpha/2} \le T_n \le z_{1-\alpha/2}\} \rightarrow 1-(1-\alpha) = \alpha\]
where we can swap between \(-z_{1-\alpha/2} = z_{\alpha/2}\) because the normal distribution is symmetric. So we've got a test that's consistent in level, but how should we think about p-values? Well we reject at level \(\alpha\) if:\footnote{Azeem tosses up: \(2(1-\Phi(|T_n|))\). Why?}
	\[|T_n| > z_{1-\alpha/2} = \Phi^{-1}(1-\alpha/2)\]
Obvious modifications for hypothesis tests with inequalities. Confidence intervals for \(\beta_j\)? Set \(r = e_j = (0, \dots, 1, \dots, 0)'\) where there's a 1 in the \(j^{th}\) slot. Then just use duality between confidence intervals and hypothesis testing.
	
\newthought{Testing} multiple linear restriction requires some more machinery. Suppose we want to test:
	\[H_0: R\beta=c\ \text{ vs. } H_A: R\beta \ne c\]
where \(R\) is a \(p \times k+1\) matrix. Assume rows of \(R\) are linearly independent. Then:
	\[R[\sqrt{n}(\hat{\beta}_n - \beta)] = [\sqrt{n}(R\hat{\beta}_n - R\beta)] \overset{d}{\rightarrow} R\mathcal{N}(0, \Omega) = \mathcal{N}(0, R\Omega R') \text{.}\]
And \(R\Omega R'\) is nonsingular. Why? For \(\alpha \ne 0\), \(\alpha'R\Omega R' \alpha > 0\) because \(\alpha' R \ne 0\) by assumption. Then \(R\hat{\Omega}_n R' \overset{p}{\rightarrow} R \Omega R'\), so, then by the CMT:
	\[\mathcal{N}(R\hat{\beta}_n - R \beta)'(R\hat{\Omega}_n R')^{-1}(R\hat{\beta}_n - R\beta) \overset{d}{\rightarrow} \chi_p^2\]
which suggests a test statistic \(T_n\) that's:
	\[T_n = \mathcal{N}(R\hat{\beta}_n - c)'(R\hat{\Omega}_n R')^{-1}(R\hat{\beta}_n - c)\]
and critical value:
	\[c_{p, 1-\alpha} = F_p^{-1}(1-\alpha)\]
where \(F_p(\cdot)\) is the c.d.f. of \(\chi_p^2\). 

\newthought{And} if we really wanna go crazy we can test non-linear restrictions. So let's try to develop a test-statistic for:
	\[H_0: f(\beta) = c\ \text{ vs. } H_A: f(\beta) \ne c\]
for \(f:\mathbb{R}^{k+1} \rightarrow \mathbb{R}^p\) where \(f(\cdot)\) is continuously differentiable at \(\beta\) and assume that the rows are linearly independent. Then define \(D_\beta f(\beta)\) to be the matrix of partials (\(p \times k+1\)). Then:
	\[\sqrt{n}(f(\hat{\beta}_n) - f(\beta)) \overset{d}{\rightarrow} \mathcal{N}(0, D_\beta f(\beta) \Omega D_\beta f(\beta)')\]
and we can define the test-statistic we can use is:
	\[T_n = n (f(\hat{\beta}_n - c)'((D_\beta f(\beta) \hat{\Omega}_n D_\beta f(\beta)'))^{-1} f(\hat{\beta}_n - c))\]
where the critical value we work with is:
	\[c_{p, 1-\alpha} = F_p^{-1}(1-\alpha)\text{.}\]

\chapter{Instrumental variables}
\newthought{We} can think about IV as linear regression when \(E[XU] \ne 0\). Let \((Y, X, U)\) be s.t.:	
	\[Y = X' \beta + U\]
where:
	\[X = (X_0, \dots, X_k)'\]
with \(X_0 = 1\) and \(E[XU] \ne 0\) and we'll assume that \(E[U] = 0\)\footnote{Even if \(E[U] \ne 0\) we can just have \(\beta_0\) soak it up.} and any \(X_j \) with \(E[X_j U] = 0\) is said to be \emph{exogenous} and any \(X_j\) with \(E[X_j U] \ne 0\) are said to be \emph{endogenous}.
\begin{ex}[Omitted variables] Let \(k = 2\) with the following causal model for \(Y\):
	\[Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + U\]
with \(E[XU] = 0\). The problem is that we don't observe \(X_2\). Rewrite:
	\[Y = \beta_0^* + \beta_1^* X_1 + U^*\]
with:
	\begin{align*}
		& \beta_0^* = \beta_0 + \beta_2 E[X_2] \\
		& \beta_1^* = \beta_1 \\
		& U^* = \beta_2(X_2 - E[X_2]) + U
	\end{align*}
and \(E[X_1 U^*] = \beta_2 Cov(X_1, X_2)\). Obviously if we just run OLS we've got issues with our estimates. 
\end{ex}
\begin{ex}[Measurement error] Partition \(X\) into \(X_0\) and \(X_1 \in \mathbb{R}^k\) with:
	\[Y = \beta_0 + X_1'\beta_1 + U\]
with \(E[XU] = 0\) but \(X_1\) is unobserved. Instead observe:
	\[\hat{X}_1 = X_1 + V\]
with \(E[V] = 0,\ Cov(X_1, V) = 0,\ \&\ Cov(V, U) = 0\). Then rewrite the model as:
	\begin{align*}
		& Y = \beta_0^* + \hat{X}_1' \beta_1^* + U^* \\
		\text{ with } & \beta_0^* = \beta_0 \\
				& \beta_1^* = \beta_1 \\
				& U^* = -V'\beta+1 + U
	\end{align*}
where \(\hat{X}_1\) is endogenous because:
	\[E[\hat{X}_1U^*] = E[(X_1 + V)(-V\beta_1 + U)] = -E[VV']\beta_1\]
which is typically \(\ne 0\).
\end{ex}
\begin{ex}[Simultaneity] The classic\footnote{CLASSIC!} supply and demand example goes as follows: \(Q^d\) is the quantity demanded and \(Q^s\) is the quantity supplied as a function of the (non-market clearing) price \(\tilde{P}\). Assume:
	\begin{align*}
		& Q^d = \beta_0^d + \beta_1^d\tilde{P} + U^d \\ 
		& Q^s = \beta_0^s + \beta_1^s \tilde{P} + U^s
	\end{align*}
with: \(E[U^d] = E[U^s] = E[U^d U^s] = 0\). All that we observe is the market clearing \((Q, P)\) s.t. \(Q^d = Q^s\). That is:
	\[\beta_0^d + \beta_1^dP + U^d = \beta_0^s + \beta_1^s P + U^s\]
which implies that:
	\[P = \frac{1}{\beta_1^d - \beta_1^s}(\beta_0^s - \beta_0^d + U^s - U^d)\]
and plugging in \((Q, P)\) to the supply and demand equations we get that:
	\[E[PU^d] = \frac{-Var(U^d)}{\beta_1^d - \beta_1^s} \ne 0 \text{.}\]
Fuck. What should we do?
\end{ex}

\section{Solving for \(\beta\)}
\newthought{Let} \((Y, X, U)\) s.t. \(Y = X'\beta + U\) and assume that there is a random vector \(Z \in \mathbb{R}^{l+1}\) with:
	\[l + 1 \ge k + 1\ s.t.\ E[ZU] = 0\]
and assume that any exogenous \(X_j\) are included in \(Z\). We'll also have to assume:\begin{enumerate}
	\item No perfect colinearity in \(Z\).
	\item \(E[ZX'] < \infty\).
	\item \(E[ZZ'] < \infty\).
	\item \(E[ZX']\) has rank \(= k + 1\).\footnote{Often called instrument relevance.}\footnote{A sufficient condition for this to hold is \(l+1 > k+1\).}
\end{enumerate} 
Now in order to solve for \(\beta\), note that:
	\[E[ZU] = 0 \implies E[Z(Y - X'\beta)] = 0 \implies E[ZX']\beta = E[ZY]\]
then if \(l+1 = k+1\) then:
	\[\beta = E[Z'X]^{-1}E[ZY]\]
but otherwise the system is over-determined, so to solve explicitly for \(\beta\), the following lemma is useful:
\begin{lem} Assume there is no perfect colinearity in \(Z\). Then \(E[ZX']\) has rank \(k+1\) i.f.f. \(\pi\) has rank \(k+1\) when:
	\[BLP(X|Z) = \pi'Z \text{.}\]
Moreover, \(\pi' E[ZX'] = \pi'E[ZZ']\pi\) is invertible. 
	\begin{proof} As is tradition, we'll start the proof with a note. Note: \(X = \pi'Z + V\) when \(E[ZV'] = 0\). So:
		\[E[ZX'] = E[ZZ'] \pi\]
	Then using the useful Lemma to the right (\(\rightarrow\))\marginnote[-1in]{\begin{lem}[Rank inequality] For any comformable matrices \(A, B\), \(rank(AB) \le \min\{rank(A), rank(B)\}\). \begin{proof} Huh? Azeem does something by the rows and then something by the columns. \end{proof} \end{lem} } we get that:
		\[rank(E[ZX']) = rank(E[ZZ']\pi) \le rank(\pi) = rank(E[ZZ']^{-1}E[ZZ']\pi) \le rank(E[ZZ']\pi) = rank(E[ZX'])\]
	which gives us one direction of the proof.
	
	Next:
		\[\pi'E[ZX'] = \pi'E[ZZ']\pi\]
	and invertible using usual arguments.\footnote{From some problem set?}
	\end{proof}
\end{lem}
\noindent To solve for \(\beta\) explicitly, multiply both sides by \(\pi'\) to get:
	\[\pi'E[ZX']\beta = \pi' E[ZY]\]
which gives us that:
	\[\beta = (\pi'E[ZX'])^{-1}(\pi'E[ZY]) = (\pi'E[ZZ']\pi)^{-1}(\pi'E[ZY]) \text{.}\]
If \(l + 1 = k + 1\) then we say that \(\beta\) is exactly identified.\footnote{Because otherwise, it's overidentified.} When things are exactly identified then \(\pi\) is invertible because it's square and has full rank, so:
	\[\beta = E[ZX']^{-1}E[ZY] \text{.}\]
Also, if \(l +1 = k + 1\) then we can derive an expression for the slope parameter alone:
	\[Y = \beta_0 + X_1\beta_1 + U\]
and:
	\[E[U] = 0 \implies \beta_0 = E[Y] - E[X_1]'\beta_1\]
and write \(Z = (1, Z_1')\) to get:
	\[Y - E[Y] = (X_1 - E[X_1])' \beta_1 + U\]
which we can pre-multiply by \(Z_1\) and take expectations of to get:
	\[E[Z_1(Y - E[Y])] = E[Z_1(X_1 - E[X_1])']\beta_1\]
which gives us the familiar:
	\[\beta_1 = Cov(Z_1, X_1)^{-1}Cov(Z_1, Y) \text{.}\]

\newthought{This} gives us a way to think about reinterpreting the rank condition we imposed earlier. If \(l + 1 = k+1\) and all \(X_0, \dots, X_{k-1}\) are all exogenous and \(X_k\) is endogenous, so:
	\[Z = (Z_0, \dots, Z_k)'\]
with \(Z_j = X_j\) for \(0 \le j \le k-1\). Then:
	\[\pi' = \left[\begin{array}{ccccccc}
		1 & 0 & 0 & 0 & \dots & 0 &0 \\
		0 & 1 & 0 & 0 & \dots & 0 & 0 \\
		\vdots & \vdots &  \vdots &\vdots &\ddots &\vdots & \vdots \\
		0 & 0 & 0 & 0 & \dots & 1 & 0 \\
		\pi_0 & \pi_1 & \pi_2 & \pi_3 & \dots & \pi_{k-1} & \pi_k  \end{array}\right] \]
and in this case the rank condition means that \(\pi_k \ne 0\). That is, the only way for \(\pi'\) to be invertible is for \(\pi_k \ne 0\). In words this means that we require \(Z_k\) to be correlated with \(X_k\) after controlling for \(X_0, \dots, X_{k-1}\). 

\newthought{Revisiting} our examples from above:
\begin{ex}[Omitted variables: Part 2] Require \(Z_1\) s.t. \(Z=(1, Z_1)'\) satisfies an instrument exogeneity and relevance. Exogeneity requires \(Z_1\) to be uncorrelated with \(X_2\) and \(U\). Relevance requires \(\pi_1 \ne 0\) in the BLP regression \(X_1 = \pi_0 + \pi_1 Z_1 + V\). In this particular case, relevance boils down to:\footnote{An example from the empirical literature would be \(Y = log(wages)\) and \(X_1 = yrs. school\) and \(X_2 = unobserved ability\) and \(Z_1\) is something like mother's education or distance to nearest school or something else equally dumb.}
	\[Cov(X_1, Z_1) \ne 0 \text{.}\]
\end{ex}
\begin{ex}[Measurement error: Part 2] Here we require \(Z_1\) s.t. \((1,Z_1')'\) satisifies instrument exogeneity and relevance. Then suppose:
	\[Z_1 = X_1 + W\]
with \(E[W] = 0\) and \(Cov(X_1, W) = 0 = Cov(U, W)\). We assume further that \(Cov(V, W) = 0\). Instrument exogeneity here requires:
	\[E[Z_1U^*] = E[(X_1 + W)(-V'\beta_1 + U)] = 0\]
and instrument relevance requires:
	\[E[Z(1, \hat{X}_1')] = \left[ \begin{array}{cc} 1 & E[\hat{X}_1'] \\ E[Z_1] & E[Z_1\hat{X}_1'] \end{array} \right] = E[XX']\]
where we just plug in the values for \(Z_1\hat{X}_1'\) in the matrix to get that result. This is cool because it gives us the sense in which repeated measurements are instruments. 
\end{ex}
\begin{ex}[Simultaneity: Part 2] Augment models for \(Q^d, Q^s\) as follows:
	\begin{align*}
		& Q^d = \beta_0^d + \beta_1^d \tilde{P} + \beta_2^d Z_1 + U^d \\
		& Q^s = \beta_0^s + \beta_1^s \tilde{P} + U^s
	\end{align*}
with:
	\[E[Z_1U^d] = E[Z_1U^s] = E[U^d] = E[U^s] = E[U^d U^s] = 0 \text{.}\]
Then market clearing implies that the market clearing price, \(P\):
	\[P = \frac{1}{\beta_1^d - \beta_1^s}(\beta_0^s - \beta_0^d - \beta_2^dZ_1 + U^s - U^d)\]
and in the supply equation:
	\[Q = \beta_0^s + \beta_1^s P + U^s\]
with \(Z = (1, Z_1)'\) satisfying instrument exogeneity and relevance.\footnote{We get relevance if \(\beta_2^d \ne 0\).}
\end{ex}

\section{Solving for subvectors of \(\beta\)}
\newthought{Just} as we did with OLS there's some neat tools for solving subvectors of estimators in IV regressions. Recall that we're working with:
	\[Y = X'\beta + U\]
with \(E[ZU] = 0,\ E[ZZ']\) exists, there's no perfect colinearity in \(Z\), \(E[ZX']\) exists, and there's rank of \(E[ZX'] = k+1\). Then we'll split up our model into:
	\[Y = X_1'\beta_1 + X_2'\beta_2 + U\]
where \(X_2\) is exogenous and \(Z_1\) is our instrument for \(X_1\) and \(Z_2 = X_2\). Then:
	\[BLP(Y|Z_2) = BLP(X_1|Z_2)'\beta_1 + X_2'\beta_2 + \underbrace{BLP(U|Z_2)}_\textrm{\(=0\)}\]
and then:
	\[Y^* = X_1^{*'}\beta_1 + U\]
with:
	\[Y^* = Y - BLP(Y|Z_2)\]
and:
	\[X_1^* = X_1 - BLP(X_1 | Z_2)\]
and if exactly identified then:
	\[\beta_1 = E[Z_1X^*_1]^{-1}E[Z_1 Y]\]
and if overidentified, then let \(\hat{X}_1^* = BLP(X_1^*|Z_1)\), and we get:
	\[\beta_1 = E[\hat{X}_1^*X_1^*]^{-1}E[\hat{X}_1^* Y^*] = E[\hat{X}_1^*\hat{X}_1^{*'}]^{-1}E[\hat{X}_1^* Y]\]
because \(X_1^* = \hat{X}_1^* + V\) with \(E[\hat{X}_1^* V'] = 0\). 

\section{Estimating \(\beta\)}
\newthought{With} the same setup as above, with sample observations \((Y_1, X_1, Z_1), \dots, (Y_1, X_n, Z_n)\) i.i.d. \((Y, X, Z)\). If exactly identified then we can just write the sample analog for \(\beta = E[ZX']^{-1}E[ZY]\):
	\[\hat{\beta}_n = \left(\frac{1}{n}\sum_{i=1}^n Z_i X_i'\right)^{-1} \left(\frac{1}{n} \sum_{i=1}^n Z_i Y_i\right)\]
and equivalently, \(\hat{\beta}_n\) satisfies:
	\[\frac{1}{n}\sum_{i=1}^n Z_i (Y_i - X_i'\hat{\beta}_n) = \frac{1}{n}\sum_{i=1}^n Z_i \hat{U}_i = 0 \text{.}\]
In matrix notation, define:
	\[\mathbb{X} = (X_1, \dots, X_n)'\]
	\[\mathbb{Z} = (Z_1, \dots, Z_n)'\]
	\[\mathbb{Y} = (Y_1, \dots, Y_n)'\]
then:
	\[\hat{\beta}_n = (\mathbb{Z}'\mathbb{X})^{-1}\mathbb{Z}\mathbb{Y} \text{.}\]

\newthought{Another} approach to estimating \(\beta\) is the two-stage least squares\footnote{TSLS.} estimator. Recall:
	\[\beta = E[\pi'ZX']^{-1}E[\pi'ZY] = E[\pi'ZZ'\pi]^{-1} E[\pi'ZY]\]
and so by analogy:
	\[\hat{\beta}_n = \left(\frac{1}{n} \sum_{i=1}^n \hat{\pi}_n' Z_i X_i'\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^n \hat{\pi}_n'Z_i Y_i  \right)\]
and because \(X_i = \hat{X}_i + \hat{V}_i = \hat{\pi}_n'Z_i + \hat{V}_i\) when \(\frac{1}{n} Z_i \hat{V}_i' = 0\) we can write:\footnote{This definition of \(\hat{\beta}_n\) satisifes:
	\[\frac{1}{n} \sum_{i=1}^n \hat{\pi}_n'Z_i(Y_i - X_i'\hat{\beta}_n) = \frac{1}{n} \sum_{i=1}^n \hat{\pi}_n'Z_i\hat{U}_i = 0 \text{.}\]}
	\[\hat{\beta}_n = \left(\frac{1}{n} \sum_{i=1}^n \hat{\pi}_n' Z_i Z_i' \hat{\pi}_n \right)^{-1}\left(\frac{1}{n} \sum_{i=1}^n \hat{\pi}_n'Z_i Y_i  \right)\]
and to get the analog for \(\pi = E[ZZ']^{-1}E[ZX']\) we use:
	\[\hat{\pi}_n = \left(\frac{1}{n} \sum_{i=1}^n Z_i Z_i'\right)^{-1}\left(\frac{1}{n} Z_i X_i'\right) \text{.}\]
We can think of \(\hat{\beta}_n\) as the IV estimator using \(\hat{X}_i\) as instruments. \(\hat{U}_n\) is orthogonal to included exogenous regressions. Furthermore, if \(\hat{\pi}_n\) is invertible, the formula reduces to the IV formula, which allows us to see why it's called two-stage least squares. In matrix notation:
	\[\hat{\mathbb{X}} = (\hat{X}_1, \dots, \hat{X}_n)' = \mathbb{P}_2\mathbb{X}\]
where \(\mathbb{P}_2 = \mathbb{Z}(\mathbb{Z}'\mathbb{Z})^{-1}\mathbb{Z}'\), so:
	\[\hat{\beta}_n = (\hat{\mathbb{X}}'\mathbb{X})^{-1}(\hat{\mathbb{X}}'\mathbb{Y}) = (\mathbb{X}'\mathbb{P}_2\mathbb{X})^{-1}(\mathbb{X}'\mathbb{P}_2\mathbb{Y}) \text{.}\]

\newthought{To estimate} subvectors of \(\beta\) in this framework, partition \(X\) into \(X_1, X_2\) where \(X_2\) is exogenous and \(Z\) into \(Z_1, Z_2\) where \(Z_2 = X_2\):
	\[Y = X_1'\beta_1 + X_2'\beta_2 + U\]
and define:
	\[\hat{\mathbb{U}} = (\hat{U}_1, \dots, \hat{U}_n)'\]
	\[\mathbb{X}_1 = (X_{1,1}, \dots, X_{1,n})'\]
with \(\mathbb{X}_2, \mathbb{Z}_1, \mathbb{Z}_2, \mathbb{P}_Z, \mathbb{P}_2, \mathbb{M}_Z\) defined equivalently and:
	\[\mathbb{P}_1 = \mathbb{Z}_1(\mathbb{Z}_1'\mathbb{Z}_1)^{-1}\mathbb{Z}_1\]
	\[\mathbb{M}_1 = \mathbb{I} - \mathbb{P}_1\]
and note that:
	\[\mathbb{Y} = \mathbb{X}_1\hat{\beta}_{1,n} + \mathbb{X}_2 \hat{\beta}_{2,n} + \hat{\mathbb{U}}\]
and:
	\[\mathbb{M}_2 \mathbb{Y} = \mathbb{M}_2\mathbb{X}_1 \hat{\beta}_{1,n} + \hat{\mathbb{U}}\]
and if exactly identified, multiply through by \(\mathbb{Z}_1\) to get:
	\[\hat{\beta}_{1,n} = (\mathbb{Z}_1'\mathbb{M}_2\mathbb{X}_1)^{-1}(\mathbb{Z}_1'\mathbb{M}_2 \mathbb{Y})\]
and if overidentified, multiply through by \((\mathbb{P}_1\mathbb{M}_2 \mathbb{X}_1)'\) to get:
	\[\mathbb{X}_1\mathbb{M}_2 \mathbb{P}_1 \mathbb{M}_2 \mathbb{Y} = \mathbb{X}_1'\mathbb{M}_2 \mathbb{P}_1 \mathbb{M}_2\mathbb{X}_1\hat{\beta}_{1,n} + \mathbb{X}_1'\mathbb{M}_2 \mathbb{P}_1 \hat{\mathbb{U}}\]
and \(\mathbb{X}_1'\mathbb{M}_2 \mathbb{P}_1 \hat{\mathbb{U}} = 0\) because:
	\[\mathbb{P}_1\mathbb{M}_2 \mathbb{X}_1 = \mathbb{P}_Z \mathbb{M}_2 \mathbb{X}_1\]
so enough to show that:
	\[\mathbb{X}_1'\mathbb{M}_2\mathbb{P}_Z\hat{\mathbb{U}} = 0\]
and \((\mathbb{P}_Z\mathbb{X})'\hat{\mathbb{U}} = 0\) because we can add and subtract the same number:\footnote{That is, \[\mathbb{P}_Z \mathbb{I}\mathbb{X} = \mathbb{P}_Z(\mathbb{M}_2 + \mathbb{P}_2)\mathbb{X}\text{.}\]}
	\[(\mathbb{P}_Z\mathbb{X})'\hat{\mathbb{U}} = (\mathbb{P}_Z \mathbb{M}_2 \mathbb{X} + \mathbb{P}_Z \mathbb{P}_2 \mathbb{X})'\hat{\mathbb{U}} = \mathbb{X}'\mathbb{M}_2\mathbb{P}_Z'\hat{\mathbb{U}} + \mathbb{X}'\mathbb{P}_2\mathbb{P}_Z\hat{\mathbb{U}} = 0\]
because:
	\[\mathbb{X}'\mathbb{P}_2 \mathbb{P}_Z \hat{\mathbb{U}} = \mathbb{X}'\mathbb{P}_2\hat{\mathbb{U}} = 0\]
because \(\mathbb{P}_2 \hat{\mathbb{U}} = 0\). 

\section{Properties of TSLS estimator}
\newthought{Same} setup as before.\marginnote{Excercise: IV is not unbiased.} 
\begin{lem}[Consistency] Under the assumptions of IV \(\hat{\beta}_n \overset{p}{\rightarrow} \beta\). 
	\begin{proof} For \(\beta = E[\pi'ZZ'\pi]^{-1}E[\pi'ZY]\) and:
	\[\hat{\beta}_n = \left(\frac{1}{n} \sum_{i=1}^n \hat{\pi}_n' Z_i Z_i' \hat{\pi}_n \right)^{-1}\left(\frac{1}{n} \sum_{i=1}^n \hat{\pi}_n'Z_i Y_i  \right)\]
	and from OLS we have that \(\hat{\pi}_n \overset{p}{\rightarrow} \pi\) and that the same averages are going to converge to their analogs and by CMT we get that \(\hat{\beta}_n \overset{p}{\rightarrow} \beta\).
	\end{proof}
\end{lem}
\begin{lem}[Limiting distribution] On top of the standard IV assumptions, if we assume further that:
	\[Var[ZU] = E[ZZ'U^2]\]
exists, then:
	\[\sqrt{n}(\hat{\beta}_n - \beta) = \underbrace{\left(\frac{1}{n} \sum_{i=1}^n \hat{\pi}_n' Z_i Z_i' \hat{\pi}_n \right)^{-1}}_\textrm{\(\overset{p}{\rightarrow} (\pi'E[ZZ']\pi)^{-1}\)} \underbrace{\left(\frac{1}{\sqrt{n}} \sum_{i=1}^n \hat{\pi}_n'Z_i U_i  \right)}_\textrm{\(= \hat{\pi}_n' \frac{1}{\sqrt{n}} \sum_{i=1}^n Z_i U_i \overset{d}{\rightarrow} \mathcal{N}(0, \pi' Var(ZU) \pi )\).}\]
so:
	\[\sqrt{n}(\hat{\beta}_n - \beta) \overset{d}{\rightarrow} \mathcal{N}(0, \Omega)\]
where:\marginnote{Excercise: Simplify if \(E[U|Z] = 0\) and \(Var(U|Z) = \sigma^2\). Homoskedasticity.}
	\[\Omega = (\pi'E[ZZ']\pi)^{-1}\pi'Var[ZU]\pi(\pi'E[ZZ']\pi)^{-1} \text{.}\]
\end{lem}
\begin{lem}[Consistent estimation of \(\Omega\)] \marginnote{Excercise: Write out estimator under homoskedasticity.} More generally, without homoskedasticity, need to estimate \(Var(ZU)\). Use:
	\[Var(ZU) = \frac{1}{n}\sum_{i=1}^n Z_iZ_i'\hat{U}_i^2\]
where:
	\[\hat{U}_i = Y_i - X_i'\hat{\beta}_n \text{.}\]
\end{lem}

\section{Weak insturments}
\newthought{Asymptotic} approximation above for \(\sqrt{n}(\hat{\beta}_n - \beta)\) may be poor in finite-sample when \(E[ZX']\) is ``close'' to having rank \(< k+1\).\footnote{Analogy: Confidence intervals for Bernoulli(\(p\)).} Let's consider a simple version for the regression case:
	\[Y_i = \beta X_i + U_i\]
and:
	\[X_i = \pi Z_i + V_i\]
with:
	\[(U_1, V_1), \dots, (U_n, V_n) \overset{iid}{\sim} \mathcal{N}(0, \Sigma)\]
where:
	\[\Sigma = \left(\begin{array}{cc} \sigma_1^2 & \sigma_{1,2} \\ \sigma_{1,2} & \sigma_2^2 \end{array} \right)\]
and \(Z_1, \dots, Z_n\) are non-random\footnote{Deterministic.} and \(\pi \ne 0\). Then in this case:
	\[\hat{\beta}_n = \frac{\frac{1}{n} \sum_i Z_i Y_i}{\frac{1}{n} \sum_i Z_i X_i} = \beta + \frac{\frac{1}{n} \sum_i Z_i U_i}{\left(\frac{1}{n}\sum_i Z_i^2\right) + \frac{1}{n} \sum_i Z_i V_i}\]
and define \(\overline{Z_n^2} := \frac{1}{n}\sum_i Z_i^2 \) so:
	\[\sqrt{n}(\hat{\beta}_n - \beta) = \frac{\frac{1}{\sqrt{n}}\sum_i Z_i U_i}{\overline{Z_n^2} \pi + \frac{1}{n} \sum_i Z_i V_i}\]
and the joint distribution of the numerator/denominator\footnote{Upstairs and downstairs.} is:
	\[\left(\begin{array}{c} \frac{1}{\sqrt{n}}\sum_i Z_i U_i \\ \overline{Z_n^2} \pi + \frac{1}{n} \sum_i Z_i V_i \end{array}\right) \sim \mathcal{N}\left(\begin{array}{c} 0 \\ \overline{Z_n^2} \pi \end{array}, \left[\begin{array}{cc} \overline{Z_n^2} \sigma_1^2 & \frac{1}{\sqrt{n}} \overline{Z_n^2} \sigma_{1,2} \\ \frac{1}{\sqrt{n}} \overline{Z_n^2} \sigma_{1,2} & \frac{1}{n} \overline{Z_n^2}\sigma_2^2\end{array}\right] \right)\]
and if \(\overline{Z_n^2} \rightarrow \overline{Z^2}\) then:
	\[\sqrt{n}(\hat{\beta}_n - \beta) \overset{d}{\rightarrow} \mathcal{N}\left(0, \frac{\sigma^2_1}{\pi^2 \overline{Z^2}}\right)\]
and we expect this to be a ``good'' approximation when:
	\[\overline{Z_n^2} \pi >> \frac{1}{\sqrt{n}} \sigma_2 \sqrt{\overline{Z_n^2}}  \text{.}\]
\marginnote{Something about how you can always pick a \(\pi\) s.t. this is bad? Probably related to the Bernoulli example.}

\newthought{Methods} that don't suffer from this problem have been developed, though.\footnote{Robust methods for weak instruments.} To see this, we'll return to our general framework and suppose we wanted to test:
	\[H_0: \beta = c\ \text{ vs. } H_A: \beta \ne c\]
and define:
	\[W_i(c) := Z_i(Y_i - X_i'c) \text{.}\]
Then under \(H_0\), \(W_i(c) = Z_i U_i\) and we know that in this case \(W_i(c)\) is an i.i.d. sequence of mean zero random vectors. Under \(H_A\): 
	\[W_i(c) = Z_i(Y_i - X_i'\beta + X_i'(\beta - c)) = Z_i U_i + Z_i X_i'(\beta - c)\]
which may not have mean zero. This suggests a test we've already developed:
	\[T_n = n \bar{W}_n(c)' \hat{\Sigma}_n^{-1}(c) \bar{W}_n(c)\]
where:
	\begin{align*}
		& W_n(c) = \frac{1}{n} \sum_{i=1}^n W_i(c) \\
		& \hat{\Sigma}_n(c) = \frac{1}{n} \sum_{i=1}^n (W_i(c) - \bar{W}_n(c))(W_i(c) - \bar{W}_n(c))'
	\end{align*}
and critical value \(c_{l+1, 1-\alpha}\) which is the \((1-\alpha)^{th}\) quantile of the \(\chi_{l+1}^2\).\footnote{This is legit, but probably very underpowered.} A closely related variant is the \emph{Anderson-Rubin test}. The idea is to regress:
	\[Y_i - X_i'c\ \text{ on } Z_i\]
and test whether the coefficients are all equal to zero. When the model is exactly identified\footnote{I.e., \(l+1 = k+1\).} some recent research suggests that these tests have ``good'' power. But if the model is overidentified then it's possible that you can do better. Also, if you're only interested in certain components of \(\beta\), e.g., \(\beta_1\), it's possible that you can do better.
	
\newthought{Another} approach is to think about a two-step procedure. For example, you might consider testing the null that:
	\[H_0: rank E[ZX'] < k +1\ \text{ vs. } H_A: rank E[ZX'] = k+1\]
and in some cases this is easy to do, like when there's only one endogenous regressor. To do this, you'd just test whether the coefficients in the first-stage are all zero. However, this doesn't solve the problem. Will still behave poorly in finite samples. 

\section{Efficiency} 
\newthought{We} could have solved for \(\beta\) using any \(l+1 \times k+1\) matrix \(\Gamma\) s.t. \(E[\Gamma' ZX']\) has rank \(k+1\). Then:
	\[\beta = E[\Gamma' Z X']^{-1}E[\Gamma'ZY]\]
which would have given us the following estimator:
	\[\tilde{\beta}_n = \left(\frac{1}{n}\sum_{i=1}^n \Gamma' Z_i X_i'\right)^{-1} \left(\frac{1}{n} \sum_{i=1}^n \Gamma' Z_i Y_i\right)\]
and you can work out the limiting distribution of this estimator to be:
	\[\sqrt{n}(\tilde{\beta}_n - \beta) \overset{d}{\rightarrow} \mathcal{N}(0, \tilde{\Omega})\]
where:
	\[\tilde{\Omega} = E[\Gamma'ZX']^{-1}\Gamma'Var(ZU)\Gamma E[\Gamma'ZX']^{-1'} \text{.}\]
Recall that for our original estimator the limiting distribution was:
	\[\sqrt{n}(\hat{\beta}_n - \beta) \overset{d}{\rightarrow} \mathcal{N}(0, \Omega)\]
with:
	\[\Omega = E[\pi' ZZ' \pi]^{-1}\pi' Var(ZU) \pi E[\pi' ZZ' \pi]^{-1}\]
and under certain assumptions \(\Gamma = \pi\) is the ``best.'' I.e.:
	\[\Omega \le \tilde{\Omega} \text{.}\]
The assumptions we need are:
	\[E[U|Z] = 0\ \&\ Var(U|Z) = \sigma^2\]
which gives us that \(Var(ZU) = \sigma^2E[ZZ']\). Then define \(W:=\Gamma' Z\) and \(W^* := \pi' Z\). Then our two variance-covariance terms reduce to:\footnote{The following trick helps us get the expression for \(\tilde{\Omega},\ E[\Gamma'ZX'] = E[\Gamma'ZZ'\pi]\) because \(X = \pi'Z + V\).}
	\begin{align*}
		& \Omega = \sigma^2 E[W^* W^{*'}]^{-1} \\
		& \tilde{\Omega} = \sigma^2E[WW^{*'}]^{-1}E[WW'] E[WW^{*'}]^{-1'}
	\end{align*}
Then we can use the following trick:
	\[\Omega \le \tilde{\Omega} \iff \Omega^{-1} \ge \tilde{\Omega}^{-1} \text{.}\]
Then:
	\[\Omega^{-1} \ge \tilde{\Omega}^{-1} \iff E[W^*W^{*'}] - E[WW^{*'}]' E[WW']^{-1} E[WW^{*'}] \ge 0 \iff E[W'] \ge 0\]
where \(V = W^* - BLP(W^*|W)\).\footnote{Azeems says: Check!}

\section{Heterogeneity} 
\newthought{Recall} our model is just: \(Y = X'\beta + U\). This implies that a change in \(X\) from \(X=x\) to \(X=\tilde{x}\) holding everything else constant is the \emph{same} for everybody. How do we relax this? One approach is to treat \(\beta\) as random.\footnote{Random coefficients model.} Then we can absorb the error term into \(\beta\) and our model becomes:
	\[Y = X'\beta \text{.}\]
This is a hot area of research right now. We'll just consider a simple case, though. Take \(k = 1\) and \(X_1 = D\) where \(D \in \{0, 1\}\). So:
	\[Y = \beta_0 + \beta_1 D\]
where we can think about \(D\) as being some treatment.\footnote{Job training program? Red-pill vs. blue-pill.} Neyman introduced the potential outcomes framework to think about this problem.\footnote{Suck it, Rubin!} The potential outcomes are \((Y_0, Y_1)\) where \(Y_0\) is the outcome if not treated and \(Y_1\) is outcome is treated. In this notation:
	\[Y = DY_1 + (1-D)Y_0 \text{.}\]
And the following words tend to be used to describe objects that emerge from this setup: \begin{enumerate}
	\item \(Y_1 - Y_0 = \) treatment effect.
	\item \(E[Y_1 - Y_0] =\) average treatment effect.
	\item \(E[Y_1 - Y_0 | D=1] =\) average treatment effect on the treated. \end{enumerate}
\footnote{In this equation:
	\[Y = \beta_0 + \beta_1 D\]
take \(\beta_0 = Y_0\) and \(\beta_1 = Y_1 - Y_0\).} Suppose that \((Y_1, Y_0)\) is independent of \(D\) and \(P\{D =1\} \in (0, 1)\).\footnote{E.g., a randomized trial.} In this case, OLS of \(Y\) on \((1, D)\) will consistently estimate the average treatment effect. Why?\footnote{Supposedly we showed this first-step on a problem set.}
	\begin{align*}
		\frac{Cov(Y, D)}{Var(D)} & = E[Y|D=1] - E[Y|D=0] \\
							& = E[Y_1 |D=1] - E[Y_0|D=0] \\
							& = E[Y_1 - Y_0] \text{.} \end{align*}
What if \((Y_1, Y_0)\) is not independent of \(D\)? Then suppose there is a binary instrument \(Z\) on \(D\). Then the TSLS regression of \(Y\) on \(D\) with \(Z\) as an instrument. Then the slope estimand in this regression is:
	\begin{align*}
		\frac{Cov(Y,Z)}{Cov(D, Z)} & = \frac{\frac{Cov(Y, Z)}{Var(Z)}}{\frac{Cov(D,Z)}{Var(Z)}} \\
							& = \frac{E[Y|Z=1] - E[Y|Z=0]}{E[D|Z=1] - E[D|Z=0]}
	\end{align*}
and it's useful to introduce potential treatments \(D_1, D_0\) where:
	\[D = D_1 Z + D_0 (1-Z)\]
and instrument relevance requires:
	\[P\{D_1 \ne D_0\} > 0\]
while instrument exogeneity requires:
	\[(Y_1, Y_0, D_1, D_0)\ \text{ independent } Z\text{.}\]
We also assume monotonicity,\footnote{Sometimes called uniformity, because it applies to every person.} which requires:
	\[P\{D_1 \ge D_0\} = 1 \text{.}\]
OK. Now that we've got all this notation, let's work to simplify our slope estimand. First, note:
	\begin{align*}
		E[Y|Z=1] - E[Y|Z=0] &= E[Y_1D + Y_0(1-D)|Z = 1] - E[Y|Z=0] \\
						& = E[Y_1D_1 + Y_0(1-D+1)|Z=1] - E[Y|Z=0] \\
						& = E[Y_1D_1 + Y_0(1-D+1)|Z=1] - E[Y_1D_0 + Y_0(1-D_0)] \\
						& = E[(Y_1 - Y_0)(D_1 - D_0)] \\
						& = E[Y_1 - Y_0 | D_1 - D_0 \ne 0]P\{D_1 - D_0 \ne 0\} \\
						& = E[Y_1 - Y_0 | D_1 > D_0]P\{D_1 > D_0\}\ \text{ by uniformity. }
	\end{align*}
Then by the same type of steps:
	\[E[D|Z = 1] - E[D|Z=0] = E[D_1 - D_0] = P\{D_1 > D_0\}\]
and we can express our estimand as:
	\[E[Y_1 - Y_0 | D_1 > D_0] =: LATE\text{.}\]

\newthought{That is,} we only observe the population where \(D_1 > D_0\) which is unobserved.\footnote{The compliers.} Is this interesting? Another important take-away is that different instruments estimate different treatment effects. This observation is unique to a world with random effects to treatment. If treatment effects are constant then this isn't an issue.\footnote{Can you include covariates? People do, but there's not a great justification.} Also, Heckman and Vytlacil have unified this stuff.\footnote{World congress paper in ECMA is the best.}

\chapter{Maximum Likelihood Estimation}
\newthought{Paradigm} shift time: Let's venture into the world of parameterized models. 

\section{Unconditional Maximum Likelihood Estimators}
\newthought{Suppose} \(X_1,\dots,X_n\) is an i.i.d. sequence of random vectors with distribution \(P\) and we'll assume \(P=P_{\theta_0}\) and \(\theta_0 \in \Theta \subseteq \mathbb{R}^d\). That is,
	\[\{P_\theta:\ \theta \in \Theta\}\]
is a parametric model for \(P\). Assume \(P_\theta\) has a density \(q_\theta\) with respect to a common measure \(\mu\). The likelihood of \(X_1,\dots,X_n\) is simply the joint density of \(X_1,\dots, X_n\) evaluated at \(X_1, \dots, X_n\). Mathematically we can define the likelihood function:
	\[l_n (\theta) := \prod_{1 \le i \le n} q_\theta(X_i)\]
and the ML estimator of \(\theta\), \(\hat{\theta}_n\) is simply any:\footnote{ML estimators need not exist. For that case we just take the ``near'' maximizer. Also, ML estimators need not be unique. Just take any one.}
	\[\hat{\theta}_n \in \arg \max_{\theta \in \Theta} l_n(\theta)\text{.}\]
Typically, though, we work with the follow transformation:
	\[L_n(\theta) = \frac{1}{n} \log(l_n(\theta)) = \frac{1}{n}\sum_{i=1}^n \log q_\theta(X_i)\]
which is convenient because it's now just a sample average and we've got a ton of tools at our disposal to work with that sort of object.\footnote{See: Everything above.}
\begin{ex}[ML Estimation with Bernoulli RVs] \(X_1, \dots, X_n\) i.i.d. \(P = P_{\theta_0} = \mathcal{B}(\theta_0),\ \theta_0 \in (0,1)\). Then we know our density is:
	\[ q_\theta (x) = \begin{cases} \theta,\   &x = 1 \\ 1-\theta,\ &x = 0 \\0,\ &o/w \end{cases} \]
which we can write as:
	\[q_\theta(x) = \theta^x (1-\theta)^{1-x}\]
so:
	\[l_n(\theta) = \prod_{i=1}^n \left(\theta^{X_i} (1-\theta)^{1-X_i}\right)\]
and then:
	\begin{align*}
		L_n(\theta) =& \frac{1}{n} \sum_{i=1}^n X_i \log \theta + (1-X_i)\log(1-\theta) \\
				=& \bar{X}_n \log \theta + (1-\bar{X}_n)\log(1-\theta)
	\end{align*}
and the first-order conditions imply that \(\hat{\theta}_n = \bar{X}_n\).
\end{ex}
\begin{ex}[Uniform] \(X_1, \dots, X_n\) i.i.d. \(P=P_{\theta_0} = \mathcal{U}(0, \theta_0)\) where \(0 \le \theta_0 < \infty\). Then:\footnote{Working with this MLE was on the 2013 Core.}
	\[q_\theta(x) = \begin{cases} \frac{1}{\theta},\ & \text{ if } 0 \le x \le \theta \\ 0,\ &o/w \end{cases} \]
giving us that:
	\[l_n(\theta) = \begin{cases} \frac{1}{\theta^n},\ & \text{ if } 0 \le X_i \le \theta\ \forall i \\ 0, & o/w \end{cases} \]
which implies that:
	\[\hat{\theta}_n = \max_{1 \le i \le n} X_i \text{.}\]
Why? Well because \(l_n(\theta)\) is decreasing in \(\theta\) for \(\theta\) s.t. \(0 \le X_i \le \theta,\ \forall i\) and \(X_j \le \max_i \{X_i\} \le \theta\).\footnote{Where \(X_j\) is all \(X_i\) that aren't the maximum of all the \(X_i\)s.}  
\end{ex}
\begin{ex}[Mixed distributions] Suppose \(X_1, \dots, X_n \overset{i.i.d.}{\sim} P=P_{\theta_0}\) where \(\theta_0 \in (-\infty, \infty)\) and \(P_\theta\) is distribution of \(X = \max\{Z-\theta, 0\}\) where \(Z \sim \mathcal{N}(0,1)\). Then:
	\[P\{X=0\} = \Phi(\theta)\]
so the density of \(Z- \theta\) is \(\phi(x+\theta)\), so:
	\[q_\theta = \begin{cases} \Phi(\theta)&,\ \text{if }\ x=0 \\ \phi(x + \theta)&,\ \text{if }\ x > 0 \end{cases}\]
then:
	\[l_n(\theta) = \prod_{1 \le i \le n: X_i = 0} \Phi(\theta) \prod_{1 \le i \le n: X_i > 0} \phi(X_i + \theta)\]
so in this case there's no easy closed form expression for \(\hat{\theta}_n\). But we can still study its properties using its implied characterizations as a (near) maximizer. 
\end{ex}

\section{Conditional ML Estimators}
\newthought{Suppose} now that we have \((Y_1, X_1), \dots, (Y_n, X_n)\) is an i.i.d. sequence of random vectors with distribution \(P\). Write \(P \rightarrow (P_{Y|X}, P_X)\). Assume that:
	\[P_{Y|X} = P_{\theta_0}\ \text{ where } \theta_0 \in \Theta\]
and assume \(P_\theta\) has a density (w/r/t some \(\mu\)) \(q_\theta\). The conditional likelihood of \((Y_1, X_1), \dots, (Y_n, X_n)\) is just the joint conditional density evaluated at \((Y_1, X_1), \dots, (Y_n, X_n)\). So,
	\[l_n(\theta) = \prod_{1 \le i \le n} q_\theta(Y_i | X_i)\]
and again, it's easier to work with:
	\[L_n(\theta) = \frac{1}{n} \log l_n(\theta) = \frac{1}{n} \sum_{i=1}^n \log q_\theta (Y_i | X_i)\]
and a conditional ML estimator \(\hat{\theta}_n\) is any (near) maximum of \(l_n(\theta)\) by taking \(X_i=\)constant, we get unconditional ML estimators. 

\begin{ex}[Probit model] \((Y_1, X_1), \dots, (Y_n, X_n)\) i.i.d. \(P\) and assume the marginal distribution \(P_{Y|X} = P_{\theta_0}\) with \(\theta_0 \in \mathbb{R}^{k+1}\), \(Y_i \in \{0,1\},\ X_i \in \mathbb{R}^{k+1}\). Then we'll further assume that:\footnote{Can think of \(Y_i = I\{X_i'\theta \ge \epsilon_i\}\) where \(\epsilon_i \sim \mathcal{N}(0,1)\).}
	\[q_\theta(y | x) = \begin{cases} \Phi(x'\theta)&,\ \text{ if } y = 1 \\ 1-\Phi(x'\theta)&,\ \text{ if } y = 0 \end{cases}\]
which is a lot easier to work with if we just write:
	\[q_\theta(y|x) = \Phi(x'\theta)^y (1-\Phi(x'\theta))^{1-y}\]
which gives us the following log-likelihood function:
	\[L_n(\theta) = \frac{1}{n} \sum_{i=1}^n Y_i \log \Phi(X_i'\theta) + (1-Y_i)\log(1-\Phi(X_i'\theta))\text{.}\]
\end{ex}

\begin{ex}[Mul] \end{ex}

\section{Properties of ML Estimators}
\newthought{Hopefully} this new approach to estimation has the sorts of features of an estimator that we like. 

\subsection{Consistency} 
\newthought{With} \(L_n(\theta) = \frac{1}{n} \sum_i q_\theta (Y_i|X_i)\) with \(\hat{\theta}_n\) a ``near'' maximizer of this function \(L_n(\theta)\) and there's no general closed form solution to \(\hat{\theta}_n\) so it'll take a little bit of work to see whether \(\hat{\theta}_n \overset{p}{\rightarrow} \theta_0\). First, define:
	\[L(\theta) := E[\log q_\theta (Y_i | X_i)]\]
where we're taking the expectation over \(P_{\theta_0}, P_X\), not \(P_\theta\). Second, here's a Lem.:
\begin{lem} If \(\forall\ \theta \ne \theta_0:\ P\{q_\theta (Y_i | X_i) \ne q_{\theta_0} (Y_i | X_i)\} > 0\) then \(L(\theta)\) is uniquely maximized at \(\theta = \theta_0\). 
\begin{proof} The proof is essentially an application of Jensen's inequality.\footnote{Full circle, bitches.} Define:
	\begin{align*}
		M(\theta):=&L(\theta) - L(\theta_0) \\
				=&E\left[\log \frac{q_\theta(Y_i | X_i)}{q_{\theta_0} (Y_i | X_i)} \right]
	\end{align*}
and then by Jensen's inequality we have that:
	\[M(\theta) \le \log E \left[ \frac{q_\theta(Y_i | X_i)}{q_{\theta_0} (Y_i | X_i)} \right] \]
and we'll express the expectation as an integral to simply:
	\[E\left[\frac{q_\theta(Y_i | X_i)}{q_{\theta_0} (Y_i | X_i)} \right] = \int \int \frac{q_\theta(y | x)}{q_{\theta_0} (y | x)}  q_{\theta_0}(y|x) d\mu(y) dP_X(x)\]
which we can write as:
	\[\int \int q_\theta(y | x) d\mu(y) dP_X(x) = 1\]
which is equal to 1 because we're just integrating over a density.\footnote{That shit better equal 1.} 

Because \(M(\theta_0) = 0\), we need to show that for \(\theta \ne \theta_0\), \(M(\theta) < 0\). And, since \(\log\) is strictly concave \(M(\theta) < 0\) unless for some \(c\):
	\[P\left\{\frac{q_\theta(Y_i | X_i)}{q_{\theta_0} (Y_i | X_i)} = c \right\} = 1\]
and there are three possible cases: \begin{enumerate}
	\item If \(c > 1\) then we've violated the result from J's Inequality above. 
	\item If \(c < 1\) then \(M(\theta) < 0\) and we're good.
	\item If \(c = 1\) then we're good because the original assumption of the Lemma ruled this out. 
\end{enumerate}

\end{proof}
\end{lem}
\begin{lem} Let \(\hat{\theta}_n\) for \(n \ge 1\) be such that:\footnote{Which is clearly satisifed if: \[L_n(\hat{\theta}_n) \ge \sup_{\theta \in \Theta} L_n(\theta) - o_p(1)\text{.}\]}
	\[L_n(\hat{\theta}_n) \ge L_n(\theta_0) - o_P(1)\]
and if:\footnote{We call this condition the uniform convergence condition.} 
	\[\sup_{\theta \in \Theta} |L_n(\theta) - L(\theta) | \overset{p}{\rightarrow} 0\]
and if:\footnote{We call this condition the well seperating condition.} 
	\[\forall\ \delta > 0,\ \sup_{\theta \in \Theta \setminus B_\delta(\theta_0)} L(\theta) < L(\theta_0)\]
then:
	\[\hat{\theta}_n \overset{p}{\rightarrow} \theta_0 \text{.}\]
\begin{proof} As is tradition, here we want to show that:
	\[\forall \epsilon > 0,\ P\{|\hat{\theta}_n - \theta_0| > \epsilon\} \rightarrow 0 \text{.}\]
When \(|\hat{\theta}_n - \theta_0| > \epsilon\) we can use the unique maximizer result from above to write,
	\begin{align*}
		L(\theta_0) - L(\hat{\theta}_n) & \ge L(\theta_0) - \sup_{\theta \in \Theta \setminus B_\delta(\theta_0)} L(\theta) \\
								& =: \eta \\
								& > 0\ \text{ by well-seperated condition.}
	\end{align*}
Thus far we've shown that:
	\[P\{|\hat{\theta}_n - \theta_0| > \epsilon\} \le P\{L(\theta_0) - L(\hat{\theta}_n) \ge \eta\}\]
and we want to show that that RHS is gonna go to \(0\). Well, one useful fact is that:
	\[L_n(\theta_0) \overset{p}{\rightarrow} L(\theta_0)\]
so:
	\[L_n(\hat{\theta}_n) \ge L(\theta_0) - o_P(1)\]
so:
	\begin{align*}
		L(\theta_0) - L(\hat{\theta}_n) \le & L_n(\hat{\theta}_n) - L(\hat{\theta}_n) + o_P(1)\ \text{ by def. of } \hat{\theta}_n \\
								\le & \sup_{\theta \in \Theta} |L_n(\theta) - L(\theta)| + o_P(1) \\
								\overset{p}{\rightarrow} & 0\ \text{ by uniform conv.}
	\end{align*}

\end{proof}
\end{lem}
\noindent Now we've got useful conditions for consistency,\footnote{Well-seperating maximum and uniform convergence.} but it'd be nice to have some sufficient conditions that are easier to work with to get the consistency result.
\begin{lem} Let \(\Theta\) be compact, \(L(\theta)\) is continuous, and \(L(\theta)\) is uniquely maximized at \(\theta = \theta_0\). Then the well-separating condition holds.
\begin{proof} Let \(\delta > 0\) be given. Then \(\Theta \setminus B_\delta(\theta_0)\) is compact, so there exists \(\theta^* \in \Theta \setminus B_\delta(\theta_0)\) s.t.:
	\[\sup_{\theta \in \Theta \setminus B_\delta(\theta_0)} L(\theta) = L(\theta^*) < L(\theta_0)\]
and that \(\theta^*\ne \theta_0\) where the last inequality holds by the unique maximum of continuous functions on compact sets. So \(\theta_0\) is the unique maximizer. 
\end{proof}
\end{lem}
\begin{lem} \(X_1, \dots, X_n\) i.i.d. with distribution P, \(\Theta \subseteq \mathbb{R}^k\) is compact. If \(f(x, \theta)\) is continuous in \(\theta\) for each value of \(x\) and there exists a function \(F(x)\) s.t.:\footnote{Dominated.}
	\[|f(x,\theta)| \le F(x)\ \forall x,\ \theta\]
and \(E[F(X_i)] < \infty\) then:
	\[\sup_{\theta \in \Theta} \left|\frac{1}{n} \sum_{i=1}^n f(X_i, \theta) - E[f(X_i, \theta)]\right| \overset{p}{\rightarrow} 0 \text{.}\]
\end{lem}
\noindent By applying this result with \(f(x, \theta)\) given by \(\log q_\theta (y|x)\) then we can get conditions for:
	\[\sup_{\theta \in \Theta} |L_n(\theta) - L(\theta)| \overset{p}{\rightarrow} 0 \text{.}\]
An example of applying these might be useful:
\begin{ex}[Consistency of Probit] Want to show that \(\hat{\theta}_n \overset{p}{\rightarrow} \theta_0\). Recall that
	\[\log q_\theta(y|x) = y \log \Phi(x'\theta) + (1-y) \log (1 - \Phi(x'\theta)) \text{.}\]
Assume that \(\Theta\) is compact, there's no perfect collinearity in \(X_i\) and \(X_i\) is bounded. 
\begin{enumerate}
\item \emph{Well-separating}: Then to verify that it's well-separating we need:
	\[L(\theta) = E[\log q_\theta (Y_i | X_i)]\]
is continuous in \(\theta\), \(\Theta\) is compact (which we've just assumed), and to check unique maximizer. That is, we need to show \(\theta \ne \theta_0\):
	\[P\{q_\theta (Y_i | X_i) \ne q_{\theta_0} (Y_i | X_i)\} > 0\]
and:
	\[P\{q_\theta (Y_i | X_i) \ne q_{\theta_0} (Y_i | X_i)\} = P\{\Phi(X_i'\theta) \ne \Phi(X_i' \theta_0)\} = P\{X_i'\theta \ne X_i'\theta_0\} = P\{X_i'(\theta - \theta_0) \ne 0\} > 0\]
which holds by the no perfect colinearity assumption. Then we have well-separating.
\item \emph{Uniform convergence}: We already have that \(\Theta\) is compact, \(\log q_\theta (y|x)\) is continuous in \(\theta\) for all \((y,x)\). Then all we have to check is the supremum condition:
	\[\sup_{y,x,\theta} | \log q_\theta(y|x)| = \max\left\{ \sup_{x,\theta} | \log \Phi(x'\theta)|, \sup_{x, \theta} | \log(1-\Phi(x'\theta))\right\} \le M < \infty\]
where we can bound the second term because of the compactness of \(\Theta\) and because \(X_i\) is bounded. Take \(F(x) = M\) and apply the uniform law of large numbers.\footnote{U.L.L.N.}
\end{enumerate}
\end{ex}

\newthought{Some} examples are hard to show the sufficient conditions for consistency but you can still show it more directly.
\begin{ex}[Uniform] \(X_1, \dots, X_n \overset{i.i.d.}{\sim} \mathcal{U}(0, \theta_0),\ \theta_0 \ge 0\) where:
	\[\hat{\theta}_n = \max_{1 \le i \le n} X_i\]
and we can show directly that \(\hat{\theta}_n \overset{p}{\rightarrow} \theta_0\).
	\begin{align*}
		P\{|\hat{\theta}_n - \theta_0| > \epsilon\} & = P\{\theta_0 - \theta_n > \epsilon \}\ \text{ because } \theta_0 \ge \theta_n \\
										& =P\{\max_i X_i < \theta_0 - \epsilon \} \\
										& =P\{X_i < \theta_0 - \epsilon\}^n \\
										& =\left(\frac{\theta_0 - \epsilon}{\theta_0}\right)^n \\
										& \rightarrow 0
	\end{align*}									
\end{ex}

\section{Mis-specification} 
\newthought{What} if the distribution of \(Y|X \ne P_\theta\) for any \(\theta\)?\footnote{There's a common idea that MLE is robust to misspecifications.} Still reasonable to expect \(\hat{\theta}_n \overset{p}{\rightarrow} \theta^*\) which maximizes \(L(\theta)\). Why? Well to see, let \(f(y|x)\) be the unknown density of \(Y|X\) w/r/t \(\mu\). Then the \(\theta^*\) that maximizes:
	\[L(\theta) = E[\log q_\theta(Y_i | X_i)] \]
is the same as the \(\theta^*\) that minimizes:
	\[E[\log f(Y_i | X_i)] - E[\log q_\theta (Y_i | X_i)] = E\left[\log \frac{f(Y_i | X_i)}{q_\theta(Y_i |X_i)}\right] = d(f,q_\theta)\]
where we're taking the expectation over the true distribution, \(f(y|x)\). The last term is always \(\ge 0\).\footnote{This comes off of Jensen's inequality. Take the negative of the \(\log(\cdot)\).} Also, it's \(= 0\) for \(f = q_\theta\), but it's not symmetric. Anyways, this conveys the sense in which you can estimate the parameters with the wrong model. It's similar to Interp. \#2 of OLS.

\section{Limiting distribution}
\newthought{Under} fairly general conditions,
	\[\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\rightarrow} \mathcal{N}(0, \Omega) \text{.}\]
\begin{lem} Suppose:
	\[\hat{\theta}_n \in \arg \max_{\theta \in \Theta} L_n(\theta)\]
that:
	\[\theta_0 \in \arg \max_{\theta \in \Theta} L(\theta)\]
that \(\hat{\theta}_n \overset{p}{\rightarrow} \theta_0\), and that \(\theta_0 \in Int(\Theta)\). Also assume that \(\log q_\theta(y|x)\) is twice continuously differentiable in \(\theta,\ \forall (y,x)\). Also, for some \(\delta > 0\):
	\[|D_{\theta_j} \log q_\theta (y|x) | \le M_1 (y, x)\ \forall \theta \in B_\delta(\theta_0),\ j,\ (y,x)\]
and \(E[M_1(Y_i, X_i)] < \infty\), where \(D_{\theta_j}\) is the derivative w/r/t \(\theta_j\), where \(\theta_j\) is the \(j^{th}\) component of \(\theta\). Also need a second-derivative equivalent. That is, suppose:
	\[|D^2_{\theta_j,\theta_l} \log q_\theta(y|x)| \le M_2 (y,x)\ \forall \theta \in B_\delta (\theta_0),\ (j,l),\ (y,x)\]
and \(E[M_2(Y_i, X_i)] < \infty\). Define:
	\[A := E[D_\theta \log q_{\theta_0} (Y_i| X_i) D_{\theta'} \log q_{\theta_0}(Y_i|X_i)]\]
and:
	\[B:= E[D^2_{\theta,\theta'} \log q_{\theta_0} (Y_i|X_i)]\]
and assume that \(A,B\) both exist and \(B\) is non-singular. Then:
	\[\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\rightarrow} \mathcal{N}(0, B^{-1}AB^{-1}) \text{.}\]
\begin{proof} We'll do this proof in steps that we'll use numbers to demarcate. \begin{enumerate}
	\item W.T.S. 
		\[E[D_\theta \log q_{\theta_0} (Y_i | X_i)] = 0\]
	which holds because \(\theta_0 \in Int(\Theta)\) and is in the \(\arg \max L(\theta)\) so \(D_\theta L(\theta_0) = 0\) and \(L(\theta_0) = \log q_{\theta_0} (y|x)\), which you can take expectations of and pass the \(D_\theta\) through to get the desired expression.
	\item W.T.S.
		\[D_\theta L_n(\hat{\theta}_n) = 0 \text{.}\]
	We have that \(L_n\) is twice differentiable because \(\log q_\theta\) is differentiable. Also, \(\hat{\theta}_n \overset{p}{\rightarrow} \theta_0 \in Int(\Theta)\) so with probability approaching 1 \(\hat{\theta}_n \in Int(\Theta)\) so because \(\hat{\theta}_n \in \arg \max L_n(\theta)\) we have \(D_\theta L_n(\hat{\theta}_n) = 0\). Now we can apply the Mean Value Theorem.\footnote{Are you fucking shitting me? Wow. Anyways, MVT says:
		\[ f(x + h) = f(x) + f'(\tilde{x})h\]
	with \(\tilde{x}\) on line segment between \(x, x+h\).} Then:
		\[0 = D_\theta L_n(\hat{\theta}_n) = D_\theta L_n(\theta_0) + H_n(\hat{\theta}_n - \theta_0)\]
	where \(H_n\) is the matrix whose \(j^{th}\) component equals the row (??) of:
		\[D^2_{\theta,\theta'} L_n(\tilde{\theta}_{n,j})\]
	where \(\tilde{\theta}_{n,j}\) lies between \(\theta_0\) and \(\hat{\theta}_n\). Then:
		\[-H_n \sqrt{n}(\hat{\theta}_n - \theta_0) = \sqrt{n} D_\theta L_n(\theta_0)\]
	and then:
		\[\sqrt{n}D_\theta L_n(\theta_0) = \frac{1}{\sqrt{n}} \sum_{i=1}^n D_\theta \log q_{\theta_0} (Y_i | X_i) \overset{d}{\rightarrow} \mathcal{N}(0, A) \text{.}\]
	Now to complete the proof, show that \(H_n \overset{p}{\rightarrow} B\) and it's enough to show that \(D_{\theta, \theta'}^2 L_n (\tilde{\theta}_{n,j}) \overset{p}{\rightarrow} B\) since \(\hat{\theta}_n \overset{p}{\rightarrow} \theta_0 \implies \tilde{\theta}_{n,j} \overset{p}{\rightarrow} \theta_0\). Then observe that:
		\begin{align*}
			D^2_{\theta,\theta'} L_n (\tilde{\theta}_{n,j}) =& \frac{1}{n} D^2_{\theta,\theta'} \log q_{\tilde{\theta}_{n,j}} (Y_i | X_i) \\
											=& E[D_{\theta,\theta'}^2 \log q_{\tilde{\theta}_{n,j}} (Y_i | X_i)] + o_p(1)\ \text{ by U.L.L.N. } \\
											\rightarrow & E[D_{\theta,\theta'}^2 \log q_{\theta_0} (Y_i | X_i)]\ \text{ because } \tilde{\theta}_{n,j} \overset{p}{\rightarrow} \theta_0
		\end{align*}
\end{enumerate}
\end{proof}
\end{lem}

\noindent This is all kind of a mess, but often \(\Omega\) simplifies. Suppose \(D_\theta L(\theta_0) = 0\). Then if we pass the derivative through the expectation we get:
	\begin{align*}
		D_\theta L(\theta_0) =& E[D_\theta \log q_{\theta_0} (Y_i | X_i)] \\
						=& \int \int D_\theta \log q_{\theta_0}(y|x) q_{\theta_0}(y|x) d\mu(y) dP_X(x) \\
						=&\int \int D_\theta \log q_{\theta}(y|x) q_{\theta}(y|x) d\mu(y) dP_X(x)\ \text{ because functional relationship (huh?) } \\
						=& 0 \text{ because of the original identity.}
	\end{align*}					
Then we can differentiate w.r.t. \(\theta\) to get:
	\[D_{\theta'} \int \int D_\theta \log q_\theta (y|x) q_\theta d\mu(y) dP_{X}(x) = 0\]
which turns into:
	\[\int \int D_{\theta, \theta'}^2 \log q_\theta(y|x) q_\theta(y|x) d\mu(y) dP_{X}(x) + \int \int D_\theta \log q_\theta(y|x) D_{\theta'} q_\theta(y|x) d\mu(y) dP_X (x) = 0\]
and note that:
	\[D_{\theta'} q_\theta(y|x) = \frac{D_{\theta'} q_\theta(y|x)}{q_\theta (y|x)} q_\theta (y|x) = D_{\theta'} \log q_\theta (y|x) q_\theta (y|x)\]
so evaluating at \(\theta = \theta_0\) we get:
	\[-E[D^2_{\theta, \theta'} \log q_{\theta_0} (Y_i|X_i)] = E[D_\theta \log q_{\theta_0}(Y_i|X_i) D_{\theta'} q_{\theta_0}(Y_i | X_i)] \]
which is just:
	\[-B = A\]
using our terminology from above. So plugging in:
	\[\Omega = -B^{-1} = A^{-1} \text{.}\]
Then quantity \(-B\) is called the \emph{Fisher information matrix} and:
	\[D_\theta \log q_\theta (Y_i | X_i) = i^{th}\ \text{ score. }\]

\newthought{The efficiency} of MLE is a much cited result. The Cramer-Rao Lower Bound argument is that:
	\[\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\rightarrow} \mathcal{N}(0, -B^{-1})\]
when this is true, the ML is often ``efficient.'' Azeem isn't a of the Cramer-Rao argument. He's a fan of the convolution argument, but this is hard to show. 
\begin{ex} Previous limiting distribution may not always hold. Suppose \(X_1, \dots X_n \overset{i.i.d.}{\sim} \mathcal{U}(0, \theta_0)\). Then we have that:
	\[\hat{\theta}_n = \max_{1 \le i \le n} X_i\]
and here:\footnote{The C.D.F. of an exponential is:
	\[F(t) = \begin{cases} 0,\ \text{ if } t < 0 \\ 1-\exp(-t/\theta_0),\ \text{ if } t \ge 0 \text{.} \end{cases} \]}
\footnote{\emph{Useful fact:}
	\[\lim_{n \rightarrow \infty} \left(1 - \frac{c}{n}\right)^n \rightarrow \exp(-c) \text{.}\]}
	\[-n (\hat{\theta}_n - \theta_0) \overset{d}{\rightarrow} \mathcal{E}(\theta_0) \text{.}\]
Then we need to show that for all \(t\):
	\[P\{-n(\hat{\theta}_n - \theta_0) \le t \} \rightarrow F(t) \text{.}\]
If \(t < 0\) then we're done because \(0 \rightarrow 0\). For \(t \ge 0\) we have that:
	\begin{align*}
		P\{-n(\hat{\theta}_n - \theta_0) \le t\} &= P\left\{\hat{\theta}_n \ge \theta_0 - \frac{t}{n}\right\} \\
									&= 1 - P\left\{\hat{\theta}_n < \theta_0 - \frac{t}{n} \right\} \\
									&= 1 - P\left\{X_i < \theta_0 - \frac{t}{n} \right\}^n \\
									&= 1 - \left(\frac{\theta_0 - \frac{t}{n}}{\theta_0}\right)^n \\
									&= 1 - \left(1 - \frac{\frac{t}{\theta_0}}{n}\right)^n \\
									& \rightarrow 1 - \exp\left(-\frac{t}{\theta_0}\right) \\
									&= F(t) \text{.}
 	\end{align*}
\end{ex}

\section{Inference}
\newthought{There} are three approaches to inference with MLE.\footnote{The trinity!} Here we'll consider tests of the form: 
	\[H_0: f(\theta_0) = 0\ \text{ vs. } H_A: f(\theta_0) \ne 0\]
for \(f: \mathbb{R}^k \rightarrow \mathbb{R}^p\) where \(D_\theta f(\theta_0),\ (p \times k),\) has rank \(p\). 

\subsection{Wald tests}
\newthought{By} simply applying the Delta Method we get that:
	\[\sqrt{n}(f(\hat{\theta}_n) - f(\theta_0)) \overset{d}{\rightarrow} \mathcal{N}(0, D_\theta f(\theta_0) \Omega D_\theta f(\theta_0)')\]
and we can construct a \(\chi_p^2\) test but need consistent estimate of variance. To do this we'll just estimate \(D_\theta f(\theta_0)\) with \(D_\theta f(\hat{\theta}_n)\) and to estimate \(\Omega = -B^{-1} = A^{-1}\) we could use:
	\[\hat{\Omega}_n = -\left(\frac{1}{n} \sum_{i=1}^n D_{\theta, \theta}^2 \log q_{\hat{\theta}_n} (Y_i| X_i) \right)^{-1}\]
or:
	\[\hat{\Omega}_n = \left(\frac{1}{n} \sum_{i=1}^n D_\theta \log q_{\hat{\theta}_n} (Y_i| X_i) D_{\theta'} \log q_{\hat{\theta}_n}(Y_i|X_i) \right)^{-1} \text{.}\]

\subsection{Score tests}
\newthought{Given} the same testing problem define:\footnote{Score!} \footnote{This is also called a Lagrange multiplier test, a characterization that Azeem eschews for some pedantic reason.} 
	\[\tilde{\theta}_n \in \arg\max_{\theta \in \Theta,\ f(\theta) = 0} L_n(\theta)\]
and the hope is that if the null is true then \(\tilde{\theta}_n \overset{p}{\rightarrow} \theta_0\). The idea is that under the null \(D_\theta L_n(\tilde{\theta}_n) \approx 0\). A sketch of formalization would be that by the MVT:\footnote{Mean value theorem!}
	\[D_\theta L_n (\tilde{\theta}_n) = D_\theta L_n(\theta_0) + H_n(\tilde{\theta}_n - \theta_0)\]
where \(H_n\) has \(j^{th}\) row given by the \(j^{th}\) row of \(D_{\theta, \theta'}^2 L_n(\theta)\) evaluated at \(\theta = \tilde{\theta}_{n,j}\) between \(\tilde{\theta}_n\) and \(\theta_0\). If \(\tilde{\theta}_n \overset{p}{\rightarrow} \theta_0\) then \(\tilde{\theta}_{n,j} \overset{p}{\rightarrow} \theta_0\), so by arguing as before that:
	\[H_n \overset{p}{\rightarrow} E[D_{\theta,\theta}^2 \log q_{\theta_0}(Y_i|X_i)] = -B\]
we can use the MVT again to get:
	\[f(\tilde{\theta}_n) = f(\theta_0) + F_n(\tilde{\theta}_n - \theta_0)\]
where \(F_n\) has \(j^{th}\) row \(= j^{th}\) row of \(D_\theta f(\theta)\) evaluated at \(\theta = \theta^{*}_{n,j}\) which is between \(\tilde{\theta}_n\) and \(\theta_0\). Then the CMT\footnote{Country Music Television? jk. nr.} gives us that:
	\[F_n \overset{p}{\rightarrow} D_\theta f(\theta_0) \text{.}\]
Then under the null:
	\[F_n(\tilde{\theta}_n - \theta_0) = 0\]
which gives us that:
	\[\underbrace{F_n H_n^{-1}}_\textrm{use \(D_\theta f(\tilde{\theta}_n) \hat{B}^{-1}\)} \sqrt{n} D_{\theta} L_n(\tilde{\theta}_n) = \underbrace{F_n H_n^{-1}}_\textrm{\(\overset{p}{\rightarrow} D_\theta f(\theta_0) B^{-1}\)} \underbrace{\sqrt{n} D_\theta L_n(\theta_0)}_\textrm{\(\overset{d}{\rightarrow} \mathcal{N}(0, A)\)} + o_P(1)\]
where \(\hat{B} = \frac{1}{n} \sum_{i=1}^n D_{\theta,\theta}' \log q_{\tilde{\theta}_n} (Y_i|X_i)\). Then combining the limiting properties of the RHS we get that:
	\[RHS \overset{d}{\rightarrow} \mathcal{N}(0, D_\theta f(\theta_0) B^{-1} A B^{-1} D_\theta f(\theta_0)')\]
where \(B^{-1}AB^{-1} = A^{-1}\) or \(-B^{-1}\). 

\subsection{Likelihood ratio test}
\newthought{Given} the same setup as above with \(\tilde{\theta}_n\) is as before. Then under the null:
	\[\frac{l_n(\hat{\theta}_n)}{l_n(\tilde{\theta}_n)} = \frac{\prod_{i=1}^n q_{\hat{\theta}_n} (Y_i|X_i)}{\prod_{i=1}^n q_{\tilde{\theta}_n}(Y_i | X_i)}\]
shouldn't be too big. Equivalently:
	\[L_n(\hat{\theta}_n) - L_n(\tilde{\theta}_n) = 0 \text{.}\]
Then using arguments similar to those shown above:
	\[2(L_n(\hat{\theta}_n) - L_n(\tilde{\theta}_n)) \overset{d}{\rightarrow} \chi_p^2 \text{.}\]

\newthought{Big picture} what to use when? \begin{enumerate}
\item Wald test is not invariant to parameterization. 
\item Score test's advantage is that it's
\item LRT suffers less from the invariance issue and in simple problems\footnote{E.g., testing \(\theta_0 = \theta^*\) vs. \(\theta_0 = \theta^{**}\).} it's the most powerful.\footnote{Neyman-Pearson Lemma.}
\end{enumerate}

\part{Empirical Analysis 31100---Harald Uhlig}

\chapter{Measure Theory}

\section{Measure Spaces}
\begin{quote} \newthought{Topology} is about \emph{open} sets. The characterizing property of a \emph{continuous} function \(f\) is that the inverse image \(f^{-1}(G)\) of an open set \(G\) is open. Measure theory is about \emph{measurable} sets. The characterizing property of a \emph{measurable} function \(f\) is that the inverse image \(f^{-1}(A)\) of any measurable set is measurable.

In topology, one axiomatizes the notion of `open set', insisting in particular that the union of \emph{any} collection of open sets is open, and that the intersection of a \emph{finite} collection of open sets is open.

In measure theory, on axiomatizes the notion of `measurable set', insisting that the union of a \emph{countable} collection of measurable sets is measurable, and that the intersection of a \emph{countable} collection of measurable sets is also measurable. Also, the complement of a measurable set must be measurable, and the whole space must be measurable. Thus the measurable sets form a \(\sigma\)-algebra, a structure stable (or `closed') under countably many set operations. Without the insistence that `only countable many operations are allowed', measure theory would be self-contradictory---a point lost on certain philosophers of probability.

\noindent ---Williams (1991)
\end{quote}

\newthought{A measure} space is a composed of a triple, \((\Omega, \mathcal{F}, \mu)\), which consists of:
	\begin{enumerate}
		\item \(\Omega\): A set of points which are different states of nature, \(\omega\).
		\item \(\mathcal{F}\): A set of subsets called that you can think of as events of \(\Omega\),\footnote{The idea is that \(\Omega\) has all the possible combination of states of nature and different combinations of those can occur. Those are called events and are just subsets of \(\Omega\).} that form a \(\sigma-\)algebra:
			\begin{enumerate}
				\item \(\Omega \in \mathcal{F}\)
				\item If \(A \in \mathcal{F}\), then so is its complement, \(A^c = \Omega \setminus A \in \mathcal{F}\).
				\item \(A_j \in \mathcal{F},\ j = 1,2,\dots\) implies \(\cup_{j=1}^\infty A_j \in \mathcal{F}\)
			\end{enumerate}
		\item A measure \(\mu\) that is a mapping, \(\mu:\mathcal{F} \rightarrow \mathbb{R}_{+} \cup \{\infty\}\) with:\footnote{Is every set measurable? \begin{ex}[Non-measurable set] Pick \(\Omega = [0,\ 1]\). Also, define equivalence, \(x\sim y\) for \(x, y\in[0,\ 1]\) if \(x-y \in \mathbb{Q}\). Then for any \(x \in [0,\ 1]\) there is \(A_x = \{y:\ x \sim y\}\), e.g., \(A_0 = \mathbb{Q} \cap [0,\ 1]\). Then each \(A_x\) has countably many numbers. Furthermore, pick the set \(B\) s.t. \([0,\ 1] = \cup_{x\in B} A_x\). Then \(B\) has uncountably many elements in it because we've used it to construct the reals from the rationals. Then \(B\) is not measurable. \end{ex}}
			\begin{enumerate}
				\item Positivity: \(\mu(A) \ge 0\).
				\item \(\sigma-\)additivity: If \(A_j \in \mathcal{F},\ j=1,2,\dots\) are disjoint, then:
					\[\mu\left(\cup_{j=1}^\infty A_j\right) = \sum_{j=1}^\infty \mu(A_j) \text{.}\]
				\item \(\mu(\emptyset) = 0\)
			\end{enumerate}
		\item Probability space or probability measure: \(\mu(\Omega) = 1\).\footnote{Not all measures have to be a probability measure, but a probability measure ought to satisfy this criteria.}
	\end{enumerate}
	\begin{marginfigure}
		\includegraphics{measure_space.png}
		\caption{An example of a measure space, \(\Omega\), with events, \(A_1\) and \(A_2\), and a measure, \(\mu\) that maps events to the real number line.}
	\end{marginfigure}
\noindent Geometrically, you can think of this triple as some amorphous space, \(\Omega\). Within \(\Omega\) there's points \(\omega\) that can be collected into subsets, \(\mathcal{F}\). The measure, \(\mu\), maps the events, \(\mathcal{F}\), into the real number line. Discrete examples also help to convey the setup. 
\begin{ex}[Flipping two coins] Suppose you're flipping two coins at random. Then the possible states of nature are:
	\[\Omega = \{(H, H), (H, T), (T, H), (T, T)\} \text{.}\]
Some examples of sets of the subsets of \(\Omega,\ \mathcal{F}\), that meet the criteria of a \(\sigma-\)algebra are:
	\[\mathcal{F}_0 = \{\{\emptyset\}, \{\Omega\}\} = \{\{\emptyset\}, \{(H, H), (H, T), (T, H), (T, T)\}\}\]
and:
	\[\mathcal{F}_1 = \{\{\emptyset\}, \{(H, H), (H, T)\}, \{(T, H), (T, T)\}, \{\Omega\}\}\]
and:
	\begin{align*}
		\mathcal{F}_2 = 2^\Omega = & \{\{\emptyset\}, \{(H, H), (H, T)\},  \{(H, H), (T,H)\},  \{(H, H), (T, T)\}, \{(H, H), (H, T), (T, H)\}, \dots, \{\Omega\}\} \\
							= & \{A_x \times \{H, T\}:\ A_x \subseteq \{H, T\}\}
	\end{align*}
Are we sure these are \(\sigma-\)algebras? Well \(\Omega\) is in each \(\mathcal{F}\), if there's something in \(\mathcal{F}\) then so is its complement, and the subsets implies that the union of subsets is in \(\mathcal{F}\). One measure we could use would be:
	\[\mu(A) = \sum_{\omega \in A} \frac{1}{2}\]
which is a probability measure. 
\end{ex}
\begin{ex}[Rolling two dice] Suppose you're rolling two dice. Then:
	\[\Omega = \{\omega = (x, y):\ x, y\ \in \{1, \dots, 6\}\}\]
and then three \(\sigma-\)algebras would be:
	\begin{enumerate}
		\item \(\mathcal{F}_0 = \{\emptyset, \Omega\}\)
		\item \(\mathcal{F}_1 = \{A_x \times \{1, \dots, 6\}:\ A_x \subseteq \{1, \dots, 6\}\}\)
		\item \(\mathcal{F}_2 = \{A\subseteq \Omega\}\)
	\end{enumerate}
where \(\mathcal{F}_1\) is just the set where the first element is first 1 and then that's crossed with every possible outcome for the second die, second the first element is 2 and then that's crossed with every possible outcome for the second die, etc., etc. The probability measure would be:
	\[\mu(A) = \sum_{\omega \in A} \frac{1}{36} \text{.}\]
Notice that here each subsequent \(\sigma-\)algebra is contained in one another. That is:
	\[\mathcal{F}_0 \subseteq \mathcal{F}_1 \subseteq \mathcal{F}_2\]
which we call a \emph{filtration}. You can think of each \(\sigma-\)algebra being indexed by time, \(t\), \(\mathcal{F}_t\), where \(\mathcal{F}_t\) is the set of events ``known'' or the information at \(t\). The idea here is that \(\mathcal{F}_0\) is the information you have regarding states of the world when you've rolled neither dice, etc., etc.\footnote{Recall Lars's notation for information available at time \(t\) in a conditional expectation statement. \#martingale}
\end{ex}
\begin{ex}[Infinite sequence] A more mathematical example would be an the natural numbers. Here we'd define the triple according to:
	\begin{enumerate}
		\item \(\Omega = \mathbb{N}\)
		\item \(\mathcal{F} = \{A \subseteq \Omega\}\)
		\item Let \(\alpha_j = \mu(\{j\})\). Then \(\mu(A) = \sum_{j \in A} \alpha_j\).
	\end{enumerate}
\end{ex}

\begin{ex}[Lebesgue measure] The Lebesgue measure, which is just a generalization of the Uniform distribution (\(\mathcal{U}\)) is defined by the following triple:
	\begin{enumerate}
		\item \(\Omega = \mathbb{R}^m\)
		\item \(\mathcal{F} = \mathcal{B}(\Omega)\): The Borel-\(\sigma\) algebra, i.e., the smallest \(\sigma-\)algebra, which contains all open subsets of \(\Omega\).
		\item Let \(I_j = [a_j, b_j],\ a_j \le b_j \in \mathbb{R}\) be intervals. Define the box:\begin{marginfigure}
	\includegraphics{lebesgue.jpg}
	\caption{Henri Lebesgue wearing an early prototype of Morpheus's sunglasses in the Matrix.}
\end{marginfigure}
			\[B = I_1 \times\dots\times I_n \text{.}\]
		and define:
			\[\mu(B) = (b_1 - a_1)(b_2 - a_2)\dots(b_m - a_m) \text{.}\]
		\item Can define Lebesgue-measurable sets. See lecture notes if interested.
	\end{enumerate}
\end{ex}

\section{Integration}
\newthought{Once} you have measures, you can define the sense in which you integrate over a measure. Let \((\Omega, \mathcal{F}, \mu)\) be a measure space. Then:
	\begin{enumerate}
		\item A function \(f: \Omega \rightarrow \mathbb{R}^k\) is called \(\mathcal{F}-\)measurable if \(f^{-1}(B) \in \mathcal{F}\) for ever Borel set \(B \in \mathcal{B}(\mathbb{R}^k)\).
		\item Suppose \(f = I\{\omega \in A\}\). Then define the integral:
			\[\int f d\mu = \int f(\omega)\mu(d\omega) = \mu(A) \text{.}\]
		\item Suppose \(f\) is a linear combination of indicator functions:
			\[f(\omega) = \sum_{j=1}^n \psi_j I\{\omega \in A_j\},\ A_j \in \mathcal{F} \text{.}\]
		Then define the integral per line extension:\footnote{An example could be: \(\psi_1 = 1,\ \psi_2 = 3.5\). Then: \[f = \sum_{j=1}^2 \psi_j I\{\omega \in A_1\}\] and:
			\[\int f d \mu = \mu(A_1) + 3.5\mu(A_1) = 4.5\mu(A_1)\]
		which is a result we get for both approaches to integrating.}
			\[\int f d\mu = \sum_{j=1}^n \psi_j \int I\{\omega \in A_j\} d\mu = \sum_{j=1}^n \psi_j \mu(A_j) \text{.}\]
	\end{enumerate}
Additionally we can extend this to all positive measurable functions and can then extend to all measurable functions, \(f\), with:
	\[\int f d\mu = \int \max\{f, 0\} d\mu - \int \max\{-f, 0\}d\mu\]
provided at least one of the integrals is finite. For \(A \in \mathcal{F}\) define:
	\[\int_A f d\mu = \int I\{\omega \in A\}f(\omega)\mu(d\omega) \text{.}\]
If \(\mu\) is a probability measure then we can define the expectation of \(f\) as:
	\[E[f] = \int f d\mu \text{.}\]

\begin{thm}[Radon-Nikodym Theorem] Let \(\mathcal{F}\) be a \(\sigma-\)algebra on \(\Omega\). Let \(\mu\) and \(\nu\) be two measures on \(\mathcal{F}\). Suppose that \(\mu(\Omega) < \infty\) and \(\nu(\Omega) < \infty\). Suppose that \(\nu\) is absolutely continuous with respect to \(\mu\).\footnote{I.e., \(\mu(A) = 0\) implies \(\nu(A) = 0\) for \(A \in \mathcal{F}\), which we write as \(\nu << \mu\).}\footnote{Here's a theorem on this topic: \begin{thm}Suppose there is a function \(g\) so that:
	\[\nu(A) = \int_A g d\mu\]
for all \(A\in \mathcal{F}\). Then \(\nu\) is absolutely continuous w/r/t \(\mu\), i.e., \(\nu << \mu\). \end{thm}} I.e., \(\nu << \mu\). Then there exists a positive measurable function \(g\), called the Radon-Nikodym derivative:\footnote{If you have an expression like: 
	\[\int_A d\nu = \int_A \frac{d \nu}{d\mu} d\mu\]
you should note that you can't cancel out the \(d\mu\)s because the \(\frac{d\nu}{d\mu}\) is a function, not a ratio and the \(d\mu\) is the thing you're measuring over, not some small change. The point is just that you can't use typical operations on this stuff and that seems to be because it's pretty bad notation.}
	\[g:\ \Omega \rightarrow \mathbb{R}_{+}\ \text{ or } g=\frac{d\nu}{d\mu}\ \text{ with } \nu(A) = \int_A g d\mu = \int_A \frac{d\nu}{d\mu}d\mu\]
for all \(A \in \mathcal{F}\).\footnote[][.3cm]{The theorem requires \(g\) to be positive, but this can be extended to signed measures, \(\nu:\ \Omega \rightarrow \mathbb{R}\). This allows you to drop the positiveness requirement but impose \(-\infty < \nu(\Omega) < \infty\).}
\end{thm}

\noindent To see the value of the Radon-Nikodym Theorem we'll work through three examples:
\begin{ex}[Conditional expectations with rolling two dice] Pick \(\Omega = \{\omega = (x, y):\ x, y \in \{1, \dots, 6\}\},\ \mathcal{F} = \mathcal{F}_j\) for \(j = 0, 1, 2,\ \mu(A) = \sum_{\omega \in A} \frac{1}{36}\). Also, let \(f:\ \Omega \rightarrow \mathbb{R}\) be measurable:
	\[\int f d\mu = \sum_{\omega \in \Omega} \frac{f(\omega)}{36} \text{.}\]
Furthermore, let \(f\) be \(\mathcal{F}_2\)-measurable.\footnote[][.3cm]{\(\mathcal{F}_2\) is the power set, so it includes every open set in \(\Omega\) and the inverse of any open set is an open set, so all functions, \(f\), are \(\mathcal{F}_2\)-measurable.} To get a sense of conditional expectations in this framework we want to find a \(\mathcal{F}_1\)-measurable function \(g:\Omega \rightarrow \mathbb{R}\), \(g E[g|\mathcal{F}_1] = E[f|\mathcal{F}_1] = E_1[f]\) because we construct \(g\) s.t.:
	\[\int_A g d\mu = \int_A f d\mu,\ \text{ for all }\ A \in \mathcal{F}_1\]
and we're certain that \(g\) exists due to the Rad.-Nik. Thm. for signed measures:\footnote[][.3cm]{Why are we certain? Define \(\nu(A)\) for \(A \in \mathcal{F}_1\) per:
	\[\nu(A) = \int_A f(x, y)d\mu\]
and define \(\tilde{\mu}\) on \(\mathcal{F}_1\) per: \(\tilde{\mu}(A) = \mu(A),\ \forall A \in \mathcal{F}_1\). Then \(\nu << \tilde{\mu}\) and then we can apply Rad.-Nik. which implies that there is a \(\mathcal{F}_1\)-measurable function \(g\) so that:
	\[\nu(A) = \int g d\tilde{\mu} = \int g d\mu \text{.}\]}
	\[g(x, y) = E_1[f(x, y)] = E[f(x, y)|\mathcal{F}_1] = E[f(x, y)| x] = \sum_{j = 1}^6 \frac{1}{6} f(x, j) \text{.}\]
\end{ex}
\begin{ex}[Integration as summation] If we revisit the infinite sequence example above, which had:
	\[(\Omega, \mathcal{F}, \mu) = \left(\mathbb{N}, \{A \subseteq \Omega\}, \mu(A) = \sum_{j \in A} \alpha_j\right)\]
for \(\alpha_j \ge 0\). Then for \(f: \Omega \rightarrow \mathbb{R}\):
	\[\int f d\mu = \sum_{j=1}^\infty \alpha_j f(j)\]
where \(\alpha_j = \mu(\{j\})\). 
\end{ex}
\begin{ex}[Lebesgue measure revisited] For \((\Omega, \mathcal{F}, \mu) = (\text{An open subset of some } \mathbb{R}^n, \mathcal{B}(\Omega), \text{ Lebesgue measure })\) then for a measurable function, \(f: \Omega \rightarrow \mathbb{R}\), the integral over \(f\) with respect to \(\mu\) is what we expect. E.g., for:
	\[f(\omega) = \kappa I\{\omega \in B\}\]
then:
	\[\int f d\mu = \kappa (b_1 - a_1)(b_2 - a_2)\dots(b_n - a_n) \text{.}\]
\end{ex}

\chapter{Maximum Likelihood Estimation}
\section{Framework}
\newthought{Idea} is that there's some unknown parameter, \(\theta \in \Theta\), with measure \(\mu(d\theta)\).\footnote{Often \(\Theta \subseteq \mathbb{R}^m\).} We observe \(y \in Y\) with measure \(\nu(dy)\) and probability density \(f(y|\theta)\) with respect to the measure \(\nu\). That is:
	\[\int f(y|\theta)\nu(dy) = 1\ \forall\ \theta \in \Theta\text{.}\]
Conceptually we think about conducting an experiment on \(\theta\) which leads to an observation \(y \sim f(y|\theta)\) for an assumed density, \(f(\cdot)\).\footnote{Sometimes the distinction between conditional and unconditional likelihoods is drawn. We typically think about conditional likelihoods when there is some set of covariates \(x\) that do not depend on \(\theta\), giving us:
	\[f(x, y | \theta) = f(y| \theta, x)f(x)\]
which is propoertional to \(f(y|theta, x)\). }
\begin{mydef}[Likelihood function] The likelihood function, \(L(\cdot)\) is \(L(\theta | y) := f(y|\theta)\). \end{mydef}
\begin{mydef}[Log-likelihood function] The log-likelihood function, \(\ell(\cdot)\) is \(\ell(\theta | y) := \log L(\theta| y)\). \end{mydef}
\begin{ex}[Linear Regression] Suppose \(y \in \mathbb{R}^n,\ X \in \mathbb{R}^{n\times k},\ \beta \in \mathbb{R}^k,\ \Sigma \in \mathbb{R}^{n \times n}\) where \(\Sigma\) is positive definite with:
	\[y = X\beta + \epsilon\]
with \(\epsilon \sim \mathcal{N}(0, \Sigma)\). Solving for \(\epsilon\) yields:
	\[\epsilon = y-X\beta \sim \mathcal{N}(0, \Sigma) \text{.}\]
The vector of parameters to identify (\(\theta\)) could be just \(\beta\) or might be both \(\beta, \Sigma\), etc., etc. The key assumption is that \(X\) does not depend on \(\Theta\), which gives us the following conditional likelihood:
	\[L(\theta| y, X) = (2\pi)^{-n/2}|\Sigma|^{-1/2} \exp\left\{ -\frac{1}{2}(y - X\beta)'\Sigma^{-1}(y-X\beta) \right\}\]
and we get a nice result if you assume \(\Sigma = \sigma^2 I_n\):\footnote{Which you get if the errors are i.i.d. and homoskedastic.} 
	\[L(\theta|y, X) = (2\pi)^{-n/2}\sigma^{-n} \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - X_i \beta)^2 \right\} \text{.}\]
\end{ex}
\begin{ex}[Logit and Probit] Suppose \(y \in \{0, 1\}\) according to:
	\[y = \begin{cases} 1,\ \text{ if } \epsilon \le X\beta \\ 0,\ \text{ if } \epsilon > X\beta \end{cases} \]
then the strategy that both Logit and Probits will take to writing down a likelihood is to partition the two states of the world (when the outcome equals 1 or 0). Pick \(\theta = \beta\). Then if \(y = 1\) we have the following likelihood: \begin{marginfigure} \includegraphics{logit_probit.png} \caption{The CDF of a logit and probit CDF are plotted. Additionally a Normal CDF with a slightly higher variance is plotted to show how to transform a Probit CDF to a Logit CDF.} \end{marginfigure} 
	\[L(\theta| y = 1, X) = G(X\beta)\]
where \(G\) is the CDF of the error term, \(\epsilon\). For \(y = 1\) the likelihood is just \(1 - G\). Then the Probit picks \(\epsilon \sim \mathcal{N}(0, 1)\) and the Logit picks:
	\[G(v) = \frac{e^v}{1 + e^v} \text{.}\]
\end{ex}

\section{MLE, Score, and Information Matrix}
\newthought{The} maximum likelihood estimator (MLE) is:
	\[\hat{\theta} := \arg \max_{\theta} L(\theta| y)\]
and to analytically solve for this estimator we probably want to think about first-order conditions. This sort of approach turns out to be so important that the vector it yields has a name: The Score.
\begin{mydef}[Score] For \(\theta \in \Theta \subseteq \mathbb{R}^m\), an open set, we define the score as the first derivative of the log-likelihood function:
	\[s(\theta) = s(\theta | y) = \frac{\partial \ell (\theta | y) }{\partial \theta}\]
which is a column vector that's as long as the number of parameters in \(\theta\).
\end{mydef}
\begin{thm} \(E_{\theta_0}[s(\theta_0 | y)] = 0\) where \(\theta_0\) is the true value of the parameter.\footnote{The sub-\(\theta_0\) on the expectation is to indicate that you're integrating with respect to the density at \(\theta_0\).} 
	\begin{proof} Densities must integrate to 1, so we can write:
		\[\forall \theta,\ \int f(y | \theta) \nu(dy) = 1\]
	differentiating both sides w/r/t \(\theta\) yields:
		\[\int \frac{\partial f(y | \theta) }{\partial \theta} \nu(dy) = 0\]
	which holds for all \(\theta\). Then we can multiply and divide within the integral by \(f(y | \theta_0)\) to get:\footnote{Because: \[\frac{\partial \ell}{\partial \theta} = \frac{\partial L/\partial \theta}{L} = \frac{\partial f/\partial \theta}{f} \text{.}\]}
		\[0 = \int \frac{\partial \ell(\theta| y)}{\partial \theta} f(y | \theta_0) \nu(dy) = \int s(\theta_0 | y) f(y| \theta_0) \nu(dy) = E_{\theta_0} [s(\theta_0 | y)]\]
	which gives us the desired identity. 
	\end{proof}
\end{thm}

\newthought{The} score function helps us identify \(\hat{\theta}\) but to go further we'll need a sense of the variance of our parameter. Taking the second derivative proves useful here.
\begin{mydef}[Information Matrix] The Information matrix is defined by : 
	\[\mathcal{I}(\theta) := E[s(\theta| y) s(\theta | y)'] \text{.}\]
\end{mydef}
\begin{thm} Assuming certain regularity conditions:
	\[\mathcal{I}(\theta_0) = E_{\theta_0} [s(\theta_0 | y) s(\theta_0 | y)'] = - E_{\theta_0} \left[ \frac{ \partial^2 \ell(\theta_0 | y) }{\partial \theta \partial \theta'} \right] \text{.}\]
\begin{proof}
	Starting from:
		\[E_{\theta_0} [s(\theta | y) ] = 0\]
	write out the integral form of the expectation and then differentiate both sides w/r/t \(\theta'\). Then we get:
		\[0 = \int \frac{\partial^2 \ell (\theta| y)}{\partial \theta \partial \theta'} f(y | \theta) \nu(dy) + \int \frac{\partial \ell(\theta | y)}{\partial \theta} \frac{\partial f(y | \theta)/\partial \theta'}{f(y | \theta)} f(y| \theta) \nu(dy)\]
	which we can re-write as:
		\[0 = E\left[\frac{\partial^2 \ell(\theta | y)}{\partial \theta \partial \theta'} \right] + \mathcal{I}(\theta) \text{.}\]
\end{proof}
\end{thm}
\newthought{One other} useful tool for working with likelihoods is taking first and second order expansions around some value of \(\theta\). Taylor's Theorem gives us:
	\[\ell (\tilde{\theta}) \approx \underbrace{\ell(\theta) + s(\theta)'(\tilde{\theta} - \theta)}_\textrm{Some constant, \(A\)} - \frac{1}{2}(\tilde{\theta} - \theta)' \underbrace{\frac{\partial^2 l(\theta)}{\partial \theta \partial \theta'}}_\textrm{\(\approx -\ell(\theta)\)}(\tilde{\theta} - \theta)\]
so then the likelihood is:
	\[L(\theta) \approx A \exp\left\{ -\frac{1}{2}(\tilde{\theta} - \theta)' \ell(\theta)(\tilde{\theta} - \theta) \right\}\]
which is approximately proportional to the normal density. Pretty awesome. Similarly, we can take a first-order expansion around \(s(\tilde{\theta})\):
	\[s(\tilde{\theta}) \approx s(\theta) + \frac{\partial^2 \ell(\theta)}{\partial \theta \partial \theta' }(\tilde{\theta} - \theta) \text{.}\]
Pick \(\theta = \theta_0\) for the value to perform the expansion around. Then:
	\[E_{\theta_0}[s(\tilde{\theta})] \approx E_{\theta_0}[s(\theta_0)] - \mathcal{I}(\theta_0) (\tilde{\theta} - \theta_0) = -\mathcal{I}(\theta_0)\]
and the score function similarly drops out when taking expectations of the likelihood. 

\section{Asymptotics}
\newthought{To develop} the asymptotic properties of likelihood estimation we'll first have to adjust our notation a bit. Define the following:
	\begin{align*}
		\ell_n(\theta)& := \frac{1}{n} \sum_{i=1}^n \ell(\theta | y_i) \\
		s_n(\theta)& := \frac{1}{n} \sum_{i=1}^n \frac{\partial \ell(\theta | y_i)}{\partial \theta} \\
		H_n(\theta)& := \frac{1}{n} \sum_{i=1}^n \frac{\partial^2 \ell(\theta | y_i)}{\partial \theta \partial \theta'} \text{.}
	\end{align*}
Then noting that the CLT gives us:
	\[\sqrt{n}s_n(\theta_0) \overset{d}{\rightarrow} \mathcal{N}(0, E[s(\theta_0)s(\theta_0)']) = \mathcal{N}(0, \mathcal{I}(\theta_0))\]
and that the MLE, \(\hat{\theta}_n\) solves \(s_n(\hat{\theta}_n) = 0\) we can take a first-order expansion around \(\theta_0\):
	\[s_n(\hat{\theta}_n) = 0 \approx s_n(\hat{\theta}_n) \approx s_n(\theta_0) + H_n(\theta_0)(\hat{\theta}_n - \theta_0)\]
which we can rearrange and pre-multiply by \(\sqrt{n}\) and \(-\mathcal{I}(\theta_0)^{-1}\) to get:
	\[\sqrt{n}\mathcal{I}(\theta_0)^{-1}H_n(\theta_0)(\hat{\theta}_n - \theta_0)\rightarrow \sqrt{n}(\hat{\theta}_n - \theta_0) \approx \sqrt{n}\mathcal{I}(\theta_0)^{-1}s_n(\theta_0) \]
so using the CLT we invoked above we get:
	\[\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\rightarrow} \mathcal{N}(0, \mathcal{I}(\theta_0)^{-1}\mathcal{I}(\theta_0)\mathcal{I}(\theta_0)^{-1}) = \mathcal{N}(0, \mathcal{I}(\theta_0)^{-1} \text{.}\]
\begin{thm} If \(\mathcal{I}(\theta)\) is invertible at the true \(\theta\) then:
	\[\sqrt{n}(\hat{\theta}_n - \theta) \overset{d}{\rightarrow} \mathcal{N}(0, \mathcal{I}(\theta)^{-1}) \text{.}\]
	\begin{proof} See immediately above.
	\end{proof}
\end{thm}

\newthought{A classic} result for MLEs is the following theorem.
\begin{thm}[Cramer-Rao Lower Bound] For a statistic, \(T\), that's a function, \(T: y\in Y \rightarrow \mathbb{R}^k\), if \(Var_\theta (T(y)) < \infty\) and \(\mathcal{I}(\theta)\) is invertible then:
	\[Var_\theta (T(y)) \ge \left(\frac{\partial \psi(\theta)}{\partial \theta} \right) \mathcal{I}(\theta)^{-1} \left(\frac{\partial \psi(\theta)}{\partial \theta}\right)^{'}\]
for \(\psi(\theta) = E_\theta[T(y)]\).\footnote{In words, this is just saying that for an unbiased estimator of \(\theta\), \(T(y)\), then the variance of that estimator is bounded below by the asymptotic variance of the MLE.} 
\end{thm}

\newthought{As we've done} many times, now that we have a limiting distribution we have to figure out the sample analog of the variance-covariance matrix. In this context, first recall the following two equations:
	\[\mathcal{I}(\theta) = E[s(\theta | y) s(\theta | y)'] = -E\left[\frac{\partial^2 \ell(\theta | y)}{\partial \theta \partial \theta'} \right]\]
which gives us two ways to construct a sample analog:
	\[\hat{\mathcal{I}}_n(\hat{\theta}_n) = \frac{1}{n}\sum_{i=1}^n s(\hat{\theta}_n | y_i)s(\hat{\theta}_n| y_i)'\]
and:
	\[\hat{\mathcal{I}}_n(\hat{\theta}_n) = \frac{1}{n}\sum_{i=1}^n \frac{\partial^2 \ell(\hat{\theta}_n | y_i)}{\partial \hat{\theta}_n \partial \hat{\theta}_n'} \]
then if \(\hat{\theta}_n \overset{p}{\rightarrow} \theta\) we have two consistent estimators for the information matrix. 

\section{Three hypothesis tests}
\dots

\chapter{Extremum Estimators}
\newthought{One way} to generalize the types of estimators we've looked at so far is to classify them as extremum estimators. We call an estimator, \(\hat{\theta}_n\), an extremum estimator if it solves:
	\[\hat{\theta}_n \in \arg \max_{\theta \in \Theta \subseteq \mathbb{R}^m} Q_n (\theta)\]
for some objective function \(Q_n(\theta)\). Examples of this type of estimator include:
	\begin{enumerate}
		\item MLE: \(Q_n(\theta) = \frac{1}{n} \sum_{i=1}^n l(\theta | Y_i)\).
		\item NLLS:\footnote{Non-linear least squares.} \(Q_n = -\frac{1}{n} \sum_{i=1}^n (Y_i - h(X_i; \theta))^2 \).
		\item M:\footnote{Moment.} \(Q_n(\theta) = \frac{1}{n} \sum_{i=1}^n m(Y_i; \theta)\).
		\item GMM: \(Q_n(\theta) = -\frac{1}{2}g_n(\theta)'\hat{W}_n g_n(\theta),\) for \(g_n(\theta) = \frac{1}{n}\sum_{i=1}^n g(Y_i; \theta)\).
	\end{enumerate}
\begin{thm}[Consistency of Extremum Estimators] Suppose \(Q_n,\ n=1,2, \dots\) are continuous functions of \(\theta \in \Theta\) where \(\Theta\) is a compact set and also suppose that we have: (identification) \(\theta_0 = \arg \max_{\theta \in \Theta} Q_0(\theta)\) is unique, (uniform convergence) \(Q_n(\cdot)\) converges uniformly in probability to \(Q_0(\cdot)\), i.e.:
	\[\sup_{\theta \in \Theta} |Q_n(\theta) - Q_0(\theta)| \overset{p}{\rightarrow} 0 \text{.}\]
Then \(\hat{\theta}_n \overset{p}{\rightarrow} \theta\). 
\end{thm}

\subsection{M-Estimators}
\newthought{Working} with M-Estimators is nearly identical to working with NLLS and we know how to do anything we want with MLEs, so in this section we just work through the conditions of M-Estimators.\footnote{We'll dive into GMM with different tools in a bit.} For the M-Estimators we assume twice-differentiability, where if the truth is \(\theta = \theta_0\), then we define the score:
	\[s(Y_i; \theta) = \frac{\partial m(Y_i; \theta)}{\partial \theta}\]
and the Hessian as:
	\[H(Y_i; \theta) = \frac{\partial^2 m(Y_i; \theta)}{\partial \theta \partial \theta'}\] 
then similar to MLE we get the following nice results:
	\begin{align*}
		Q_n(\theta) = \frac{1}{n} \sum_i m(Y_i; \theta) & \overset{p}{\rightarrow} E_{\theta_0}[ m(Y_i; \theta)] =: Q_0 (\theta) \\
		s_n (\theta) = \frac{\partial Q_n(\theta)}{\partial \theta} = \frac{1}{n} \sum_i s(Y_i; \theta) & \overset{p}{\rightarrow} E_{\theta_0}[s(Y_i; \theta)] = 0 \\
		H_n(\theta) = \frac{\partial^2 Q_n(\theta)}{\partial \theta \partial \theta'} = \frac{1}{n} \sum_i H(Y_i; \theta) & \overset{p}{\rightarrow} E_{\theta_0} [H(Y_i; \theta)] =: \Psi \text{ at } \theta = \theta_0
	\end{align*}
And we can also assume that \(Y_i\) are correlated across ``nearby'' \(i\), which is related to ergodicity.\footnote{The concept of ergodicity that Uhlig is using here is that the past correlations eventually are overwhelmed by the asymptotics.} Then:
	\[\sqrt{n} s_n(\theta_0) \overset{d}{\rightarrow} \mathcal{N}(0, \Sigma)\]
where the long-run variance of \(\Sigma\) of \(s(Y_i; \theta_0)\) is:\footnote{So if we have a scalar score function where each observation is \(s_i(Y_i; \theta)\):
	\[E\left[ \left(\frac{1}{n} \sum_i s_i\right)^2 \right] = \frac{1}{n^2} \sum_i E[s_i(Y_i; \theta)] = \frac{1}{n}E[s_i(Y_i; \theta)^2]\]
but if not i.i.d. then it's not as easy:
	\[E\left[ \left(\frac{1}{n} \sum_i s_i\right)^2 \right] = \frac{1}{n^2} \sum_i \sum_j E[s_i(Y_i; \theta) s_j(Y_j; \theta)'] \text{.}\]
}
	\[\Sigma = \sum_{k=-\infty}^\infty \Gamma_k\]
where \(\Gamma_k = E[s(Y_i; \theta_0) s(Y_{j+k}; \theta_0)']\).\footnote{Just as we did in Lars's class last year.} 

\begin{ex} Suppose \(E[s_i s_{i+k}] = 0,\ \forall\ k\ge 3\). Then:
	\[\Sigma = \frac{1}{n}\sum_{k=-2}^2 \left(1-\frac{|k|}{n}\right) \Gamma_k \approx \frac{1}{n} \sum_{k=-2}^2 \Gamma_k \text{.}\]
\end{ex}

\begin{ex} Suppose \(\epsilon_i \overset{i.i.d.}{\sim} \mathcal{N}(0, 1)\) and:
	\[s_i = (\epsilon_i,\ \epsilon_{i-1},\ \epsilon_{i-2})'\]
then if we take:
	\[E[s_i s_i']= \left[\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array} \right] = \Gamma_0\]
because the expectation of a squared \(\mathcal{N}(0, 1)\) is 1. Next we take one more iteration forward:
	\[E[s_i s_{i+1}'] = E\left[ \left[\begin{array}{c} \epsilon_i \\ \epsilon_{i-1} \\ \epsilon_{i-2} \end{array}\right] \left[ \begin{array}{ccc} \epsilon_i & \epsilon_{i-1} & \epsilon_{i-2} \end{array}\right] \right] = \left[\begin{array}{ccc} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array} \right] = \Gamma_1\]
and by the same logic:
	\[\Gamma_2 = \left[\begin{array}{ccc} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{array} \right]\]
and:
	\[\Gamma_3 = 0 \text{.}\]
If we then did \(\Gamma_{-1}, \Gamma_{-2}\) we'd get that:
	\[\Sigma = \left[\begin{array}{ccc} 1& 1&1 \\ 1& 1& 1 \\ 1& 1&1 \end{array} \right]\]
\end{ex}


\newthought{But what about} the estimator part of M-Estimators? Well, the estimator, \(\hat{\theta}_n\) solves:
	\[\frac{\partial Q_n(\hat{\theta}_n)}{\partial \theta} = s_n(\hat{\theta}_n) = 0\]
and if we take a first-order expansion around \(\theta_0\), we get:
	\[0 = s_n(\hat{\theta}_n) \approx s_n(\theta_0) + H_n(\theta_0)(\hat{\theta}_n - \theta_0)\]
and if we assume that \(\psi\) is invertible\footnote{Which implies positive definiteness.} then:
	\[\sqrt{n}(\hat{\theta}_n - \theta_0) \approx -\sqrt{n}\Psi^{-1} H_n(\theta_0) (\hat{\theta}_n - \theta_0) \approx \sqrt{n} \Psi^{-1}s_n(\theta_0)\]
and if we take the limit we get the familiar looking result:
	\[\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\rightarrow} \mathcal{N}(0, \Psi^{-1} \Sigma \Psi^{-1}) \text{.}\]
If \(\Gamma = \Psi\) then this simplifies to:
	\[\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\rightarrow} \mathcal{N}(0, \Psi^{-1})\]
where \(\Gamma = \Gamma(\theta_0)\) and \(\Psi = \Psi(\theta_0)\). 

\subsection{GMM}
\newthought{Further} generalizing brings us to GMM. First we assume twice differentiability of \(g(\cdot)\) and ergodicity as well. Furthermore, we assume that:
	\[\hat{W}_n \overset{p}{\rightarrow} \mathcal{W}\]
and define:
	\[\mathcal{S} := \sum_{k=-\infty}^\infty \Gamma_k\]
which is the long-run variance of \(g(Y_i; \theta_0)\), where \(\Gamma_k\) is as we defined before:
	\[\Gamma_k = E[g(Y_i; \theta_0) g(Y_{i+k}; \theta_0)']\]
and we lastly define:
	\[G:= E\left[\frac{\partial g(Y_i; \theta_0)}{\partial \theta'}\right] \text{.}\]
Then for our sample average of \(g(\cdot)\):
	\[g_n(\theta) := \frac{1}{n} \sum_{i=1}^n g(Y_i; \theta)\]
gives us:
	\[Q_n(\theta) := -\frac{1}{2} g_n(\theta)'\hat{W}_n g_n(\theta)\]
and:
	\[G_n(\theta) := \frac{\partial g_n(\theta)}{\partial \theta'} = \frac{1}{n} \sum_i \frac{\partial g(Y_i; \theta)}{\partial \theta'} \overset{p}{\rightarrow} G\ \text{ at } \theta = \theta_0\]
and:
	\[s_n(\theta) := \frac{\partial Q_n(\theta)}{\partial \theta} = -G_n(\theta)'\hat{W}_n g_n(\theta)\]
which gives us the asymptotics:
	\[\sqrt{n}s_n(\theta_0) \overset{d}{\rightarrow} \mathcal{N}(0, \Sigma)\]
where \(\Sigma := G'\mathcal{W}\mathcal{S}\mathcal{W}G\).\footnote{For \(E[g(Y_i; \theta_0)] = 0\) we can also write:
	\[\sqrt{n}g_n(Y_i; \theta_0) \overset{d}{\rightarrow} \mathcal{N}(0, \mathcal{S}) \text{.}\]}\footnote{The sample analog of \(\Sigma\) is:
		\[G_n' \hat{W}_n g_n g_n' \hat{W}_n G_n \text{.}\]}
Then we can do the same expansion procedure we've done in the past. First note that \(\hat{\theta}_n\) solves:
	\[s_n(\hat{\theta}_n) = 0\]
so we can do a first-order expansion of \(g_n(\theta)\) around \(\theta_0\):
	\[0 = s_n(\hat{\theta}_n) = -G_n(\hat{\theta}_n)'\hat{W}_n g_n(\hat{\theta}_n) \approx -G_n(\hat{\theta}_n)'\hat{W}_n G_n(\hat{\theta}_n - \theta_0)\]
Note that:
	\[G_n(\hat{\theta}_n)'\hat{W}_nG_n(\hat{\theta}_n) \overset{p}{\rightarrow} G'\mathcal{W}G =: \Psi\]
and if we assume that \(\Psi\) is invertible then:\footnote{Uhlig mentions that something's missing from this expansion that we dealt with with M-Estimators. Perhaps worth dwelling on.} 
	\[\sqrt{n}(\hat{\theta}_n - \theta_0) \approx \sqrt{n}\Psi^{-1}s_n(\theta_0)\]
and taking the limit gives us the asymptotics of the GMM estimator:
	\[\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\rightarrow} \mathcal{N}(0,\ \Psi^{-1}\Sigma \Psi^{-1})\]
and a good choice of \(\mathcal{W}\)? How about \(\mathcal{W} = \mathcal{S}^{-1}\) because then the asymptotics reduces to:
	\[\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\rightarrow} \mathcal{N}(0,\ \Psi^{-1}) \text{.}\]

\newthought{Hypothesis} testing with GMM isn't particularly unique. To see this, suppose we want to do constrained estimation as we did with MLEs. For:
	\[\hat{\theta}_{c, n} = \arg \max_{\theta \in \Theta} Q_n\ \ s.t.\ a(\theta) = 0\]
where \(\partial a(\theta_0)/\partial \theta\) has rank \(k\). Then if \(\Psi = \Sigma\) and \(\hat{\Psi}_n \overset{p}{\rightarrow} \Psi\) we get the following tests: \begin{enumerate}
	\item ``Likelihood'' Ratio Test: \(LR = 2n(Q_n(\hat{\theta}_n) - Q_n(\hat{\theta}_{c, n})) \overset{d}{\rightarrow} \chi^2_k\)
	\item Lagrange Multiplier Test: \(LM=n s_n(\hat{\theta}_{c,n})' \hat{\Psi}_n^{-1} s_n(\hat{\theta}_{c,n}))\overset{d}{\rightarrow} \chi^2_k\)
	\item Wald: Define the object \(A_n := \partial a(\hat{\theta}_n)/\partial \theta \overset{p}{\rightarrow} A = \partial a(\theta_0)/\partial\) then:
		\[W = n a(\hat{\theta}_n)'(A_n \hat{\Psi}_n^{-1}A_n')^{-1}a(\hat{\theta}_n) \overset{d}{\rightarrow} \chi^2_k \]
\end{enumerate}

\begin{ex}[IV as GMM] Suppose \(Z_t,\ t=1, \dots, T\) are uncorrelated with \(\epsilon_t\). Then:
	\[E[Z_t \epsilon_t] = E[Z_t(Y_t - X_t\beta)] = 0\]
is just our moment condition. We can generalize this using:
	\[g([X_t, Z_t]; \theta) = E[Z_t'f(X_t; \theta)] = 0\]
and find \(\hat{\theta}\) per GMM for some suitable weighting matrix, \(\hat{W}_t\). \end{ex}

\chapter{Bayesian Inference} 

\section{Framework}
\newthought{The setup} for Bayesian Inference goes as follows: There is an unknown parameter \(\theta \in \Theta\) with measure \(\mu(d\theta)\). We observe \(X\) with measure \(\nu(dX)\) and \(X\) has the density \(f(X|\theta)\) w/r/t \(\nu\) which allows us to write down the likelihood function \(L(\theta | X) = f(X|\theta)\). Conceptually we think about draws (experiment) on \(\theta\) which leads to an observation \(X \sim f(X|\theta)\) for some known \(f(\cdot)\) if it is carried out. 

\begin{mydef}[Sufficient Statistic] A function or statistic \(T\) of \(X\) is sufficient if the distribution of \(X\) condition on \(T(X)\) does not depend on \(\theta\). \end{mydef}
\begin{ex} Suppose \(X_i \overset{i.i.d.}{\sim} \mathcal{N}(\mu, \sigma^2)\). Then a sufficient statistic for \(X\) is \(T(X) = (\bar{X}_n, \hat{\sigma}_n^2)\). \end{ex}
\begin{mydef}[Sufficiency Principle] Two observations \(X, Y\) which lead to the same value of a sufficient statistic, \(T\), \(T(X) = T(Y)\), lead to the same inference regarding \(\theta\). \end{mydef}
\begin{mydef}[Conditionality Principle] If two experiments can be carried out on \(\theta\) and if exactly one of these experiments is actually carried out with some probability \(p\), then the resulting inference on \(\theta\) should only depend on the selected experiment and the resulting observation.\footnote{Huh? The idea seems to just be that data you don't observe shouldn't impact your inference on \(\theta\) which seems like a terrible assumption.}\end{mydef}
\begin{mydef}[Likelihood Principle] The information brought about by an observation \(X_i\) about \(\theta\) is entirely contained in the likelihood function, \(L(\theta|X_i)\) and if two observations \(X_1, X_2\) lead to proportional likelihood functions:
	\[L(\theta|X_1) = \alpha L(\theta| X_2),\ \alpha > 0\]
then they shall lead to the same inference regarding \(\theta\). \end{mydef}

Inference under these principles can be done by maximizing the likelihood by choice of \(\theta\) and then using an estimate of the information matrix to test hypotheses. A Bayesian would go a slightly different route. They'd first define a prior, \(\pi(\theta)\), which is a density w/r/t \(\mu\). Then the posterior would be:
	\[\pi(\theta|X) = \frac{L(\theta| X)\pi(\theta)}{\int_\Theta L(\tilde{\theta}|X)\pi(\tilde{\theta})\mu(d\tilde{\theta})} \text{.}\]
How does this allow for us to do inference on \(\theta\)? Well the idea is that \(X \sim f(X|\theta_0)\) is given when the experiment is conducted and the true parameter, \(\theta_0\), is distributed according to \(\pi(\theta_0|X)\), so the true parameter is a random variable. This is in stark contrast to the frequentist approach where \(\theta_0\) is unknown and the observation \(X \sim f(X|\theta_0)\) is random. 

\begin{mydef}[Stopping Rule Principle] If a sequence of experiments is directed by a stopping rule, \(\tau\), that indicates when the experiments stop, then inference about \(\theta\) shall depend on \(\tau\) only through the resulting sample. \end{mydef}
\begin{ex} Experimenter has 100 observations, \(X_i \overset{i.i.d.}{\sim} \mathcal{N}(\theta, 1)\) with sample mean \(\bar{X}_n = 0.2\). The frequentist wants to conduct the following test: \(H_0: \theta = 0\) vs. \(H_A: \theta \ne 0\). Then there are the following stopping rules: \begin{enumerate}
	\item Stop always: If \(\sqrt{100}\ \bar{X}_{100} > 1.96\) then reject.
	\item If \(\sqrt{100}\bar{X}_{100} \ge c\) then stop and reject. If not take another 100 draws and reject if: \(\sqrt{200}\bar{X}_{200} \ge c\). 
	\item Now suppose that an RA is generating the data and shows up with probability \(p\). As a classical econometrician you work out all the possibilities and make inference accordingly. Yikes.
\end{enumerate} 
Stopping rule principle avoids these issues. \end{ex}
\begin{ex} Suppose we have data, \(X_1, \dots, X_n\ \overset{i.i.d.}{\sim} \mathcal{B}(\theta)\). To fix ideas suppose each observation is a realization of some casino game where \(\theta\) is the probability that the gambler wins. Then define:
	\[X^{(n)} := \sum_{i=1}^n X_i\]
which would give us the following likelihood:
	\[L(\theta| X^{(n)}) = f(x^{(n)}|\theta, n)\]
where \(f(\cdot)\) is the Binomial distribution for \(n\) draws. Now consider the following stopping rules: \begin{enumerate}
	\item Take 100 draws. None more.
	\item Take draws until \(X^{(n)} = \frac{n}{2}\) or \(n = 1,000,000\) where the idea is that \(1,000,000\) is the maximum number of draws you could observe.
	\item Suppose \(n = 100\) and \(X^{(100)} = 50\). Then the stopping rule principle says that inference about \(\theta\) does not depend on the stopping rule. 
\end{enumerate}
Clearly the second stopping rule is going to bias your estimate of \(\theta\) and overweight the possibility that the game is fair.\footnote{That is, that \(\theta_0 =\) 0.5.} 
\end{ex}

\begin{ex} Suppose \(X_i \overset{i.i.d.}{\sim} \mathcal{N}(\theta, 1)\). Then a stopping rule like:
	\[|\bar{X}_n| = \left | \frac{1}{n} \sum_i X_i \right | > \frac{1.96}{\sqrt{n}}\]
then classical inference that's done carelessly would always reject \(H_0: \theta = 0\) at the 5\% level. The Bayesian approach doesn't suffer from these issues, though. Hmmmmmmmm. 
\end{ex}

\newthought{To complete} the Bayesian framework, we need two more ingredients: A decision \(\delta(X) \in \mathcal{D}\) and a loss function, \(\mathcal{L}(\theta, \delta(X))\).\footnote{E.g., Quadratic Loss: \(\mathcal{L}(\theta, \delta(X)) = ||\theta - \delta(X)||^2\).} Then risk from the frequentist's perspective is had with:
	\[\mathcal{R}(\theta, \delta) := E_\theta [\mathcal{L}(\theta, \delta(X))] = \int_X \mathcal{L}(\theta, \delta(X)) f(X|\theta)dX\]
but from the Bayesian perspective, the posterior expected loss is:
	\[\rho(\pi, \delta(X)) := E_\pi [\mathcal{L}(\theta, \delta(X))| X] = \int_\Theta \mathcal{L}(\theta, \delta(X)) \pi(\theta|X)d\theta\]
so the integrated risk is:
	\[r(\theta, \delta) := E_\pi [\mathcal{R}(\theta, \delta)] = \int_\Theta \int_X \mathcal{L}(\theta, \delta(X)) f(X| \theta) \pi(\theta) dX d\theta = \int_X \rho(\theta, \delta(X))m(X)dX\]
where \(m(X) = \int_\Theta f(X|\theta) \pi(\theta) d\theta\). 
\begin{mydef}[Admissibility] An estimator \(\delta_0\) is \emph{admissibile} if there is no estimator \(\delta_1\), which dominates \(\delta_0\). That is, there's no \(\delta_1\) that satisfies:
	\[\mathcal{R}(\theta, \delta_0) \ge \mathcal{R}(\theta, \delta_1)\]
which holds with strictly for at least one value of \(\theta_0\). 
\end{mydef}
\begin{mydef}[Bayes Estimator] A Bayes estimator associated with a prior distribution \(\pi\) and a loss function \(\mathcal{L}\) is any estimator \(\delta^\pi\) which minimizes \(r(\theta, \delta)\):
	\[\delta^\pi \in \arg \min_{d \in \mathcal{D}} \rho (\pi, d | X) \text{.}\]
\end{mydef}
\begin{mydef}[Bayes Risk] The value \(r(\pi) := r(\pi, \delta^\pi)\) is called the Bayes risk. 
\end{mydef}
\begin{thm} Bayes estimator \(\delta^\pi\) is admissible given certain condition.
\end{thm}
\begin{thm} All admissible estimators are limits of sequences of Bayes estimators.
\end{thm}
\begin{thm} The MLE estimator is inadmissible and is dominated by the James-Stein estimator:
	\[\delta_{JS}(\hat{\theta}) = \left(1 - \frac{k-2}{||\hat{\theta}||^2} \right)\hat{\theta}\]
\end{thm}

\section{Conjugacy and Priors}
\newthought{A few words on this} 
\begin{mydef} If the prior \(\pi\) is a member of a parametric family of distributions, so that the posterior, \(\pi(\theta|X)\), also belongs to that family, then the family is called conjugate to \(\{f(\cdot | \theta):\ \theta \in \Theta\}\). 
\end{mydef}

\begin{ex}[Flat Prior] Not invariant to reparamaterization. 
\end{ex}


\begin{ex}[Jeffrey's Prior] Suppose \(\theta \in \mathbb{R}\) and \(\tilde{\theta} \in \mathbb{R}\) and the two parameters are related according to \(h(\cdot)\): \(\tilde{\theta} = h(\theta)\) and the function \(h(\cdot)\) is one-to-one. Then we get the following two priors: \(\tilde{\pi}(\tilde{\theta})\) and \(\pi(\theta)\) and a desirable property for these priors is that they're invariant to reparameterization:
	\[\int_{\tilde{A}} \tilde{\pi}(\tilde{\theta})d\tilde{\theta} = \int_A \pi(\theta)d\theta\]
where \(A, h(A)=\tilde{A} \subseteq \mathbb{R}\). To get this invariance we need the following property to hold:
	\[\tilde{\pi}(\tilde{\theta})d\tilde{\theta} = \pi(\theta)d\theta \text{.}\]
Suppose you chose \(h(\cdot)\) so that:
	\[X | \tilde{\theta} \sim \mathcal{N}(\tilde{\theta}, \sigma^2)\]
	\dots
\end{ex}


\section{Numerical Methods for Bayesian Inference}



































\end{document}